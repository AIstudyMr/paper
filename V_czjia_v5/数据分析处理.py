import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
from datetime import datetime, timedelta
import warnings
from scipy import signal
from scipy.stats import pearsonr
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def read_csv_with_encoding(file_path, encodings=['utf-8', 'gbk', 'gb2312', 'utf-8-sig']):
    """å°è¯•ä¸åŒç¼–ç è¯»å–CSVæ–‡ä»¶"""
    for encoding in encodings:
        try:
            df = pd.read_csv(file_path, encoding=encoding)
            print(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶")
            return df
        except UnicodeDecodeError:
            continue
        except Exception as e:
            print(f"ä½¿ç”¨ {encoding} ç¼–ç æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue
    raise ValueError("æ— æ³•è¯»å–æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ç¼–ç ")

def process_data_for_time_period(summary_df, start_time, end_time, time_interval_seconds=60):
    """
    ä¸ºæŒ‡å®šæ—¶é—´æ®µå¤„ç†æ•°æ®
    
    å‚æ•°:
    summary_df: æ±‡æ€»æ•°æ®DataFrame
    start_time: å¼€å§‹æ—¶é—´
    end_time: ç»“æŸæ—¶é—´
    time_interval_seconds: æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤60ç§’ï¼ˆ1åˆ†é’Ÿï¼‰
                          æ”¯æŒä»»æ„ç§’æ•°è®¾ç½®ï¼Œä¾‹å¦‚ï¼š60, 30, 20, 10, 5, 3, 1ç­‰
    """
    # è½¬æ¢æ—¶é—´åˆ—
    summary_df['æ—¶é—´'] = pd.to_datetime(summary_df['æ—¶é—´'])
    
    # ç­›é€‰æ—¶é—´æ®µå†…çš„æ•°æ®
    mask = (summary_df['æ—¶é—´'] >= start_time) & (summary_df['æ—¶é—´'] <= end_time)
    period_data = summary_df.loc[mask].copy()
    
    if period_data.empty:
        print(f"è­¦å‘Šï¼šæ—¶é—´æ®µ {start_time} åˆ° {end_time} æ²¡æœ‰æ•°æ®")
        return None
    
    # æŒ‰æŒ‡å®šæ—¶é—´é—´éš”é‡é‡‡æ ·
    period_data.set_index('æ—¶é—´', inplace=True)
    

    # å®šä¹‰éœ€è¦çš„åˆ—
    required_columns = [
        'æŠ˜å æœºå®é™…é€Ÿåº¦', 'æŠ˜å æœºå…¥åŒ…æ•°', 'æŠ˜å æœºå‡ºåŒ…æ•°', 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'å­˜çº¸ç‡',
        'è£åˆ‡æœºå®é™…é€Ÿåº¦', 'æœ‰æ•ˆæ€»åˆ‡æ•°', '1#æœ‰æ•ˆåˆ‡æ•°', '2#æœ‰æ•ˆåˆ‡æ•°', '3#æœ‰æ•ˆåˆ‡æ•°', '4#æœ‰æ•ˆåˆ‡æ•°',
        'è¿›ç¬¬ä¸€è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬äºŒè£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬ä¸‰è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬å››è£åˆ‡é€šé“çº¸æ¡è®¡æ•°',
        '1#å°åŒ…æœºå…¥åŒ…æ•°', '1#å°åŒ…æœºå®é™…é€Ÿåº¦', '2#å°åŒ…æœºå…¥åŒ…æ•°', '2#å°åŒ…æœºå®é™…é€Ÿåº¦',
        '3#å°åŒ…æœºå…¥åŒ…æ•°', '3#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦', '4#å°åŒ…æœºå…¥åŒ…æ•°', '4#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦'
    ]
    
    # æ£€æŸ¥ç¼ºå¤±çš„åˆ—
    missing_cols = [col for col in required_columns if col not in period_data.columns]
    if missing_cols:
        print(f"è­¦å‘Šï¼šç¼ºå¤±ä»¥ä¸‹åˆ—: {missing_cols}")
        # ä½¿ç”¨å¯ç”¨çš„åˆ—
        available_cols = [col for col in required_columns if col in period_data.columns]
        if not available_cols:
            print("é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•éœ€è¦çš„åˆ—")
            return None
        required_columns = available_cols
    
    # åˆ›å»ºç»“æœå­—å…¸
    result_data = {}
    
    # ç´¯ç§¯é‡åˆ—ï¼ˆè®¡ç®—æ¯åˆ†é’Ÿå·®å€¼ï¼‰
    cumulative_cols = [
        'æŠ˜å æœºå…¥åŒ…æ•°', 'æŠ˜å æœºå‡ºåŒ…æ•°', 'æœ‰æ•ˆæ€»åˆ‡æ•°', '1#æœ‰æ•ˆåˆ‡æ•°', '2#æœ‰æ•ˆåˆ‡æ•°', 
        '3#æœ‰æ•ˆåˆ‡æ•°', '4#æœ‰æ•ˆåˆ‡æ•°', '1#å°åŒ…æœºå…¥åŒ…æ•°', '2#å°åŒ…æœºå…¥åŒ…æ•°', 
        '3#å°åŒ…æœºå…¥åŒ…æ•°', '4#å°åŒ…æœºå…¥åŒ…æ•°', 'å­˜çº¸ç‡'
    ]
    
    # ç¬æ—¶é‡åˆ—
    instantaneous_cols = [
        'æŠ˜å æœºå®é™…é€Ÿåº¦', 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'è£åˆ‡æœºå®é™…é€Ÿåº¦',
        'è¿›ç¬¬ä¸€è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬äºŒè£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬ä¸‰è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬å››è£åˆ‡é€šé“çº¸æ¡è®¡æ•°',
        '1#å°åŒ…æœºå®é™…é€Ÿåº¦', '2#å°åŒ…æœºå®é™…é€Ÿåº¦', '3#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦', '4#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦'
    ]
    
    # æŒ‰æŒ‡å®šæ—¶é—´é—´éš”é‡é‡‡æ ·å¤„ç†
    resample_freq = f'{time_interval_seconds}S'  # ç”Ÿæˆé‡é‡‡æ ·é¢‘ç‡å­—ç¬¦ä¸²ï¼Œå¦‚'30S', '20S', '10S'
    interval_data = period_data.resample(resample_freq)
    
    # å¤„ç†ç´¯ç§¯é‡
    for col in cumulative_cols:
        if col in period_data.columns:
            # è®¡ç®—æ¯ä¸ªæ—¶é—´é—´éš”çš„å·®å€¼
            interval_diff = interval_data[col].last() - interval_data[col].first()
            # æŠ˜å æœºå‡ºåŒ…æ•°å’ŒæŠ˜å æœºå…¥åŒ…æ•°ç­‰ä½¿ç”¨å›ºå®šç³»æ•°25
            if col in ['æŠ˜å æœºå‡ºåŒ…æ•°', 'æŠ˜å æœºå…¥åŒ…æ•°', 'æœ‰æ•ˆæ€»åˆ‡æ•°', '1#æœ‰æ•ˆåˆ‡æ•°', 
                       '2#æœ‰æ•ˆåˆ‡æ•°', '3#æœ‰æ•ˆåˆ‡æ•°', '4#æœ‰æ•ˆåˆ‡æ•°', '1#å°åŒ…æœºå…¥åŒ…æ•°', 
                       '2#å°åŒ…æœºå…¥åŒ…æ•°', '3#å°åŒ…æœºå…¥åŒ…æ•°', '4#å°åŒ…æœºå…¥åŒ…æ•°']:
                # ç»Ÿä¸€ä½¿ç”¨å›ºå®šç³»æ•°25ï¼Œä¸éšæ—¶é—´é—´éš”è°ƒæ•´
                result_data[col] = (interval_diff / 25).values
            elif col == 'å­˜çº¸ç‡':
                # å­˜çº¸ç‡è®¡ç®—å·®å€¼ï¼Œä¸é™¤ä»¥ç³»æ•°
                result_data[col] = interval_diff.values
            else:
                result_data[col] = interval_diff.values
    
    # å¤„ç†ç¬æ—¶é‡
    if 'æŠ˜å æœºå®é™…é€Ÿåº¦' in period_data.columns:
        # è®¡ç®—æ¯ä¸ªæ—¶é—´é—´éš”å¹³å‡å€¼å†ä½¿ç”¨å›ºå®šç³»æ•°
        avg_speed = interval_data['æŠ˜å æœºå®é™…é€Ÿåº¦'].mean()
        # ä½¿ç”¨å›ºå®šç³»æ•°9.75ï¼Œä¸éšæ—¶é—´é—´éš”è°ƒæ•´
        result_data['æŠ˜å æœºå®é™…é€Ÿåº¦'] = (avg_speed / 25).round(2).values
    
    if 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡' in period_data.columns:
        # è®¡ç®—æ¯ä¸ªæ—¶é—´é—´éš”çš„å’Œ
        result_data['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡'] = interval_data['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡'].sum().values
    
    if 'è£åˆ‡æœºå®é™…é€Ÿåº¦' in period_data.columns:
        # è®¡ç®—æ¯ä¸ªæ—¶é—´é—´éš”å¹³å‡å€¼ä½¿ç”¨å›ºå®šç³»æ•°
        avg_speed = interval_data['è£åˆ‡æœºå®é™…é€Ÿåº¦'].mean()
        # ä½¿ç”¨å›ºå®šç³»æ•°9.75ï¼Œä¸éšæ—¶é—´é—´éš”è°ƒæ•´
        result_data['è£åˆ‡æœºå®é™…é€Ÿåº¦'] = (avg_speed / 25).round(2).values
    
    # å¤„ç†è£åˆ‡é€šé“çº¸æ¡è®¡æ•°
    cut_channel_cols = ['è¿›ç¬¬ä¸€è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬äºŒè£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬ä¸‰è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬å››è£åˆ‡é€šé“çº¸æ¡è®¡æ•°']
    for col in cut_channel_cols:
        if col in period_data.columns:
            result_data[col] = interval_data[col].sum().values
    
    # å¤„ç†å°åŒ…æœºé€Ÿåº¦
    packer_speed_cols = ['1#å°åŒ…æœºå®é™…é€Ÿåº¦', '2#å°åŒ…æœºå®é™…é€Ÿåº¦', '3#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦', '4#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦']
    packer_speeds = []
    for col in packer_speed_cols:
        if col in period_data.columns:
            avg_speed = interval_data[col].mean()
            # ä½¿ç”¨å›ºå®šç³»æ•°25ï¼Œä¸éšæ—¶é—´é—´éš”è°ƒæ•´
            speed_processed = (avg_speed / 25).round(2)
            result_data[col] = speed_processed.values
            packer_speeds.append(speed_processed.values)
    
    # è®¡ç®—å°åŒ…æœºé€Ÿåº¦æ€»å’Œ
    if packer_speeds:
        packer_speed_sum = np.sum(packer_speeds, axis=0)
        result_data['å°åŒ…æœºé€Ÿåº¦æ€»å’Œ'] = packer_speed_sum
    
    # è®¡ç®—å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ
    packer_input_cols = ['1#å°åŒ…æœºå…¥åŒ…æ•°', '2#å°åŒ…æœºå…¥åŒ…æ•°', '3#å°åŒ…æœºå…¥åŒ…æ•°', '4#å°åŒ…æœºå…¥åŒ…æ•°']
    packer_inputs = []
    for col in packer_input_cols:
        if col in result_data:
            packer_inputs.append(result_data[col])
    
    if packer_inputs:
        packer_input_sum = np.sum(packer_inputs, axis=0)
        result_data['å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ'] = packer_input_sum
    
    # å­˜çº¸ç‡å·²åœ¨ç´¯ç§¯é‡å¤„ç†ä¸­è®¡ç®—å·®å€¼
    
    # åˆ›å»ºæ—¶é—´ç´¢å¼•
    time_index = interval_data.groups.keys()
    
    return result_data, list(time_index)

def plot_data(data_dict, time_index, start_time, end_time, output_dir, time_interval_seconds=60):
    """ç»˜åˆ¶æ•°æ®å›¾è¡¨
    
    å‚æ•°:
    data_dict: æ•°æ®å­—å…¸
    time_index: æ—¶é—´ç´¢å¼•
    start_time: å¼€å§‹æ—¶é—´
    end_time: ç»“æŸæ—¶é—´
    output_dir: è¾“å‡ºç›®å½•
    time_interval_seconds: æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰ï¼Œæ”¯æŒä»»æ„ç§’æ•°è®¾ç½®
    """
    if not data_dict:
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è®¡ç®—æŒç»­æ—¶é—´
    duration = end_time - start_time
    duration_str = str(duration).split('.')[0]  # å»æ‰å¾®ç§’éƒ¨åˆ†
    
    # æ ¼å¼åŒ–æ ‡é¢˜ï¼ŒåŒ…å«æ—¶é—´é—´éš”ä¿¡æ¯
    interval_desc = f"{time_interval_seconds}ç§’é—´éš”"
    title = f"æ—¶é—´æ®µ: {start_time.strftime('%Y-%m-%d %H:%M:%S')} è‡³ {end_time.strftime('%Y-%m-%d %H:%M:%S')}\næŒç»­æ—¶é—´: {duration_str} | æ•°æ®é—´éš”: {interval_desc}"
    
    # å®šä¹‰ç»˜å›¾é¡ºåº
    plot_order = [
        'æŠ˜å æœºå®é™…é€Ÿåº¦', 'æŠ˜å æœºå…¥åŒ…æ•°', 'æŠ˜å æœºå‡ºåŒ…æ•°', 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'å­˜çº¸ç‡',
        'è£åˆ‡æœºå®é™…é€Ÿåº¦', 'æœ‰æ•ˆæ€»åˆ‡æ•°', '1#æœ‰æ•ˆåˆ‡æ•°', '2#æœ‰æ•ˆåˆ‡æ•°', '3#æœ‰æ•ˆåˆ‡æ•°', '4#æœ‰æ•ˆåˆ‡æ•°',
        'è¿›ç¬¬ä¸€è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬äºŒè£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬ä¸‰è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬å››è£åˆ‡é€šé“çº¸æ¡è®¡æ•°',
        '1#å°åŒ…æœºå…¥åŒ…æ•°', '1#å°åŒ…æœºå®é™…é€Ÿåº¦', '2#å°åŒ…æœºå…¥åŒ…æ•°', '2#å°åŒ…æœºå®é™…é€Ÿåº¦','å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ',
        '3#å°åŒ…æœºå…¥åŒ…æ•°', '3#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦', '4#å°åŒ…æœºå…¥åŒ…æ•°', '4#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦',
        'å°åŒ…æœºé€Ÿåº¦æ€»å’Œ'
    ]
    
    # è¿‡æ»¤å­˜åœ¨çš„åˆ—
    available_cols = [col for col in plot_order if col in data_dict]
    
    if not available_cols:
        print("æ²¡æœ‰å¯ç»˜åˆ¶çš„æ•°æ®")
        return
    
    # è®¡ç®—å­å›¾æ•°é‡å’Œå¸ƒå±€
    n_cols = len(available_cols)
    n_rows = (n_cols + 2) // 3  # æ¯è¡Œ3ä¸ªå­å›¾
    
    # åˆ›å»ºå›¾å½¢ï¼Œä¸ºæ ‡é¢˜ç•™å‡ºæ›´å¤šç©ºé—´
    fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows + 1))
    if n_rows == 1:
        axes = axes.reshape(1, -1)
    
    # æ·»åŠ æ€»æ ‡é¢˜
    fig.suptitle(title, fontsize=16, fontweight='bold', y=0.98)

    # æ‰å¹³åŒ–axesæ•°ç»„
    axes_flat = axes.flatten()
    
    # ç»˜åˆ¶æ¯ä¸ªæŒ‡æ ‡
    for i, col in enumerate(available_cols):
        ax = axes_flat[i]
        
        if len(data_dict[col]) > 0:
            # åˆ›å»ºæ—¶é—´åºåˆ—
            time_series = pd.Series(data_dict[col], index=time_index[:len(data_dict[col])])
            
            # ç»˜åˆ¶æ›²çº¿
            ax.plot(time_series.index, time_series.values, marker='o', markersize=3)
            ax.set_title(col, fontsize=12)
            ax.set_xlabel('æ—¶é—´')
            ax.set_ylabel('æ•°å€¼')
            ax.grid(True, alpha=0.3)
            
            # æ—‹è½¬xè½´æ ‡ç­¾
            ax.tick_params(axis='x', rotation=45)
    
    # éšè—å¤šä½™çš„å­å›¾
    for i in range(len(available_cols), len(axes_flat)):
        axes_flat[i].set_visible(False)
    
    # è°ƒæ•´å¸ƒå±€ï¼Œä¸ºé¡¶éƒ¨æ ‡é¢˜ç•™å‡ºç©ºé—´
    plt.tight_layout(rect=[0, 0, 1, 0.94])
    
    # ä¿å­˜å›¾ç‰‡
    filename = f"{start_time.strftime('%Y%m%d_%H%M%S')}_{end_time.strftime('%Y%m%d_%H%M%S')}.png"
    filepath = os.path.join(output_dir, filename)
    plt.savefig(filepath, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"å›¾ç‰‡å·²ä¿å­˜: {filepath}")

def plot_combined_data(data_dict, time_index, start_time, end_time, output_dir, time_interval_seconds=60):
    """ç»˜åˆ¶ç»„åˆæ•°æ®å›¾è¡¨ - å°†æŒ‡å®šçš„å¤šåˆ—æ•°æ®ç»˜åˆ¶åœ¨åŒä¸€ä¸ªå›¾ä¸­ï¼Œå¹¶è¿›è¡Œæ—¶é—´åç§»ç›¸å…³æ€§åˆ†æ
    
    å‚æ•°:
    data_dict: æ•°æ®å­—å…¸
    time_index: æ—¶é—´ç´¢å¼•
    start_time: å¼€å§‹æ—¶é—´
    end_time: ç»“æŸæ—¶é—´
    output_dir: è¾“å‡ºç›®å½•
    time_interval_seconds: æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰ï¼Œæ”¯æŒä»»æ„ç§’æ•°è®¾ç½®
    """
    if not data_dict:
        return []
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è®¡ç®—æŒç»­æ—¶é—´
    duration = end_time - start_time
    duration_str = str(duration).split('.')[0]  # å»æ‰å¾®ç§’éƒ¨åˆ†
    
    # æ ¼å¼åŒ–æ ‡é¢˜ï¼ŒåŒ…å«æ—¶é—´é—´éš”ä¿¡æ¯
    interval_desc = f"{time_interval_seconds}ç§’é—´éš”"
    title = f"æ—¶é—´æ®µ: {start_time.strftime('%Y-%m-%d %H:%M:%S')} è‡³ {end_time.strftime('%Y-%m-%d %H:%M:%S')}\næŒç»­æ—¶é—´: {duration_str} | æ•°æ®é—´éš”: {interval_desc}"
    
    # ==============================================
    # åœ¨è¿™é‡Œå®šä¹‰è¦ç»„åˆç»˜åˆ¶çš„åˆ—æ•°æ®ç»„åˆ
    # ==============================================
    plot_combinations = [
        {
            'title': 'å…±åŒå‰ç½®æµç¨‹1',
            'columns': ['æŠ˜å æœºå…¥åŒ…æ•°','æŠ˜å æœºå‡ºåŒ…æ•°'],
            'colors': ['purple', 'orange', 'brown', 'pink', 'gray', 'olive']
        },
        {
            'title': 'å…±åŒå‰ç½®æµç¨‹2',
            'columns': ['æŠ˜å æœºå‡ºåŒ…æ•°','å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡'],
            'colors': ['red', 'blue', 'green', 'orange', 'purple']
        },
        {
            'title': 'å¤–å¾ªç¯åˆ†æµè¿æ¥1',
            'columns': ['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡','è¿›ç¬¬ä¸€è£åˆ‡é€šé“çº¸æ¡è®¡æ•°'],
            'colors': ['red', 'blue', 'green', 'orange']
        },
        {
            'title': 'å¤–å¾ªç¯åˆ†æµè¿æ¥2',
            'columns': ['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡','è¿›ç¬¬äºŒè£åˆ‡é€šé“çº¸æ¡è®¡æ•°'],
            'colors': ['red', 'blue', 'green','orange','purple']
        },
        {
            'title': 'å¤–å¾ªç¯åˆ†æµè¿æ¥3',
            'columns': ['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡','è¿›ç¬¬ä¸‰è£åˆ‡é€šé“çº¸æ¡è®¡æ•°'],
            'colors': ['red', 'blue']
        },
        {
            'title': 'å¤–å¾ªç¯åˆ†æµè¿æ¥4',
            'columns': ['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡','è¿›ç¬¬å››è£åˆ‡é€šé“çº¸æ¡è®¡æ•°'],
            'colors': ['red', 'blue']
        },
        {
            'title': 'ç”Ÿäº§çº¿1æµç¨‹1',
            'columns': ['è¿›ç¬¬ä¸€è£åˆ‡é€šé“çº¸æ¡è®¡æ•°','1#æœ‰æ•ˆåˆ‡æ•°'],
            'colors': ['red', 'blue']
        },
        {
            'title': 'ç”Ÿäº§çº¿1æµç¨‹2',
            'columns': ['1#æœ‰æ•ˆåˆ‡æ•°','1#å°åŒ…æœºå…¥åŒ…æ•°'],
            'colors': ['red', 'blue']
        },
        {
            'title': 'ç”Ÿäº§çº¿2æµç¨‹1',
            'columns': ['è¿›ç¬¬äºŒè£åˆ‡é€šé“çº¸æ¡è®¡æ•°','2#æœ‰æ•ˆåˆ‡æ•°'],
            'colors': ['red', 'blue']
        },
        {
            'title': 'ç”Ÿäº§çº¿2æµç¨‹2',
            'columns': ['2#æœ‰æ•ˆåˆ‡æ•°','2#å°åŒ…æœºå…¥åŒ…æ•°'],
            'colors': ['red', 'blue']
        },
        {
            'title': 'ç”Ÿäº§çº¿3æµç¨‹1',
            'columns': ['è¿›ç¬¬ä¸‰è£åˆ‡é€šé“çº¸æ¡è®¡æ•°','3#æœ‰æ•ˆåˆ‡æ•°'],
            'colors': ['red', 'blue']
        },
        {
            'title': 'ç”Ÿäº§çº¿3æµç¨‹2',
            'columns': ['3#æœ‰æ•ˆåˆ‡æ•°','3#å°åŒ…æœºå…¥åŒ…æ•°'],
            'colors': ['red', 'blue']
        },
        {
            'title': 'ç”Ÿäº§çº¿4æµç¨‹1',
            'columns': ['è¿›ç¬¬å››è£åˆ‡é€šé“çº¸æ¡è®¡æ•°','4#æœ‰æ•ˆåˆ‡æ•°'],
            'colors': ['red', 'blue']
        },
        {
            'title': 'ç”Ÿäº§çº¿4æµç¨‹3',
            'columns': ['4#æœ‰æ•ˆåˆ‡æ•°','4#å°åŒ…æœºå…¥åŒ…æ•°'],
            'colors': ['red', 'blue']
        }
    ]
    # ==============================================
    
    # å­˜å‚¨æ—¶é—´åç§»åˆ†æç»“æœ
    shift_analysis_results = []
    
    # ä¸ºæ¯ä¸ªç»„åˆåˆ›å»ºä¸€ä¸ªå›¾è¡¨
    for combo_idx, combo in enumerate(plot_combinations):
        # è¿‡æ»¤å­˜åœ¨çš„åˆ—
        available_cols = [col for col in combo['columns'] if col in data_dict and len(data_dict[col]) > 0]
        
        if not available_cols:
            print(f"è·³è¿‡ç»„åˆ '{combo['title']}'ï¼šæ²¡æœ‰å¯ç”¨æ•°æ®")
            continue
        
        # åˆ›å»ºå›¾å½¢
        plt.figure(figsize=(12, 8))
        
        # æ·»åŠ æ ‡é¢˜
        plt.suptitle(f"{combo['title']}\n{title}", fontsize=14, fontweight='bold')
        
        # ç»˜åˆ¶æ¯åˆ—æ•°æ®
        for i, col in enumerate(available_cols):
            if len(data_dict[col]) > 0:
                # åˆ›å»ºæ—¶é—´åºåˆ—
                time_series = pd.Series(data_dict[col], index=time_index[:len(data_dict[col])])
                
                # é€‰æ‹©é¢œè‰²
                color = combo['colors'][i % len(combo['colors'])]
                
                # ç»˜åˆ¶æ›²çº¿
                plt.plot(time_series.index, time_series.values, 
                        marker='o', markersize=4, label=col, color=color, linewidth=2)
        
        # è®¾ç½®å›¾è¡¨å±æ€§
        plt.xlabel('æ—¶é—´', fontsize=12)
        plt.ylabel('æ•°å€¼', fontsize=12)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        
        # ä¸ºæ¯ä¸ªç»„åˆåˆ›å»ºç‹¬ç«‹çš„å­æ–‡ä»¶å¤¹
        safe_title = combo['title'].replace('/', '_').replace('\\', '_').replace(':', '_')
        combo_dir = os.path.join(output_dir, safe_title)
        os.makedirs(combo_dir, exist_ok=True)
        
        # ä¿å­˜å›¾ç‰‡
        filename = f"{start_time.strftime('%Y%m%d_%H%M%S')}_{end_time.strftime('%Y%m%d_%H%M%S')}.png"
        filepath = os.path.join(combo_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ç»„åˆå›¾è¡¨å·²ä¿å­˜: {filepath}")
        
        # å¯¹äºæ°å¥½æœ‰ä¸¤ä¸ªæŒ‡æ ‡çš„ç»„åˆï¼Œè¿›è¡Œæ—¶é—´åç§»ç›¸å…³æ€§åˆ†æ
        if len(available_cols) == 2:
            col1, col2 = available_cols
            shift_result = calculate_time_shift_correlation(
                data_dict[col1], 
                data_dict[col2], 
                col1, 
                col2, 
                time_interval_seconds
            )
            
            if shift_result:
                # æ·»åŠ é¢å¤–ä¿¡æ¯
                period_name = f"{start_time.strftime('%Y%m%d_%H%M%S')}_{end_time.strftime('%Y%m%d_%H%M%S')}"
                shift_result.update({
                    'time_period': period_name,
                    'chart_title': combo['title'],
                    'start_time': start_time.strftime('%Y-%m-%d %H:%M:%S'),
                    'end_time': end_time.strftime('%Y-%m-%d %H:%M:%S'),
                    'time_interval_seconds': time_interval_seconds
                })
                shift_analysis_results.append(shift_result)
                print(f"  â†’ æ—¶é—´åç§»åˆ†æ: {shift_result['shift_description']}, ç›¸å…³ç³»æ•°: {shift_result['best_correlation']:.3f}")
    
    return shift_analysis_results

def calculate_time_shift_correlation(data1, data2, col1_name, col2_name, time_interval_seconds=60, max_shift_seconds=300):
    """
    è®¡ç®—ä¸¤ä¸ªæ—¶é—´åºåˆ—åœ¨ä¸åŒæ—¶é—´åç§»ä¸‹çš„ç›¸å…³æ€§
    
    å‚æ•°:
    data1, data2: ä¸¤ä¸ªæ—¶é—´åºåˆ—æ•°æ®
    col1_name, col2_name: åˆ—å
    time_interval_seconds: æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰ï¼Œæ”¯æŒä»»æ„ç§’æ•°è®¾ç½®
    max_shift_seconds: æœ€å¤§åç§»æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5åˆ†é’Ÿ
    
    è¿”å›:
    åŒ…å«æœ€ä½³åç§»å’Œç›¸å…³æ€§ä¿¡æ¯çš„å­—å…¸
    """
    try:
        # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
        min_length = min(len(data1), len(data2))
        if min_length < 10:  # æ•°æ®ç‚¹å¤ªå°‘
            return None
        
        data1 = np.array(data1[:min_length])
        data2 = np.array(data2[:min_length])
        
        # æ ‡å‡†åŒ–æ•°æ®ï¼ˆå»é™¤å‡å€¼å¹¶å½’ä¸€åŒ–ï¼‰
        data1_norm = (data1 - np.mean(data1)) / (np.std(data1) + 1e-8)
        data2_norm = (data2 - np.mean(data2)) / (np.std(data2) + 1e-8)
        
        # è®¡ç®—æœ€å¤§åç§»æ­¥æ•°
        max_shift_steps = min(max_shift_seconds // time_interval_seconds, min_length // 2)
        max_shift_steps = max(1, int(max_shift_steps))
        
        correlations = []
        shifts = []
        
        # è®¡ç®—ä¸åŒåç§»ä¸‹çš„ç›¸å…³æ€§ï¼ˆåªè€ƒè™‘data2æ»åäºdata1çš„æƒ…å†µï¼‰
        for shift in range(0, max_shift_steps + 1):
            if shift == 0:
                # æ— åç§»
                corr, _ = pearsonr(data1_norm, data2_norm)
            else:
                # data2 å‘ååç§»ï¼ˆdata2æ»åäºdata1ï¼‰
                if len(data1_norm[:-shift]) > 5 and len(data2_norm[shift:]) > 5:
                    corr, _ = pearsonr(data1_norm[:-shift], data2_norm[shift:])
                else:
                    corr = 0
            
            if not np.isnan(corr):
                correlations.append(corr)
                shifts.append(shift * time_interval_seconds)  # è½¬æ¢ä¸ºç§’
            
        if not correlations:
            return None
            
        # æ‰¾åˆ°æœ€é«˜ç›¸å…³æ€§
        max_corr_idx = np.argmax(np.abs(correlations))
        best_shift = shifts[max_corr_idx]
        best_correlation = correlations[max_corr_idx]
        
        # è§£é‡Šåç§»æ–¹å‘ï¼ˆåªè€ƒè™‘data2æ»åäºdata1çš„æƒ…å†µï¼‰
        if best_shift > 0:
            shift_description = f"{col2_name} æ»å {col1_name} {abs(best_shift)} ç§’"
        else:
            shift_description = "æ— æ˜æ˜¾æ—¶é—´åç§»"
        
        return {
            'col1_name': col1_name,
            'col2_name': col2_name,
            'best_shift_seconds': best_shift,
            'best_correlation': best_correlation,
            'shift_description': shift_description,
            'all_shifts': shifts,
            'all_correlations': correlations,
            'data_points': min_length
        }
        
    except Exception as e:
        print(f"è®¡ç®—æ—¶é—´åç§»ç›¸å…³æ€§æ—¶å‡ºé”™: {e}")
        return None

def calculate_column_difference(data_dict, time_index, col1, col2, period_name):
    """è®¡ç®—ä¸¤åˆ—æ•°æ®çš„å·®å€¼å¹¶ç»Ÿè®¡æ­£è´Ÿæ¯”ä¾‹"""
    if col1 not in data_dict or col2 not in data_dict:
        missing_cols = []
        if col1 not in data_dict:
            missing_cols.append(col1)
        if col2 not in data_dict:
            missing_cols.append(col2)
        print(f"è­¦å‘Šï¼šæ—¶é—´æ®µ {period_name} ç¼ºå¤±åˆ—: {missing_cols}")
        return None
    
    # ç¡®ä¿ä¸¤åˆ—æ•°æ®é•¿åº¦ä¸€è‡´
    min_length = min(len(data_dict[col1]), len(data_dict[col2]))
    if min_length == 0:
        print(f"è­¦å‘Šï¼šæ—¶é—´æ®µ {period_name} æ•°æ®ä¸ºç©º")
        return None
    
    col1_data = np.array(data_dict[col1][:min_length])
    col2_data = np.array(data_dict[col2][:min_length])
    
    # è®¡ç®—å·®å€¼ (col1 - col2)
    difference = col1_data - col2_data
    
    # ç»Ÿè®¡æ­£è´Ÿå·®å€¼
    positive_count = np.sum(difference > 0)
    negative_count = np.sum(difference < 0)
    zero_count = np.sum(difference == 0)
    total_count = len(difference)
    
    # è®¡ç®—æ¯”ä¾‹
    positive_ratio = positive_count / total_count * 100 if total_count > 0 else 0
    negative_ratio = negative_count / total_count * 100 if total_count > 0 else 0
    zero_ratio = zero_count / total_count * 100 if total_count > 0 else 0
    
    # åˆ›å»ºç»“æœå­—å…¸
    result = {
        'period_name': period_name,
        'col1_name': col1,
        'col2_name': col2,
        'time_index': time_index[:min_length],
        'col1_data': col1_data,
        'col2_data': col2_data,
        'difference': difference,
        'total_count': total_count,
        'positive_count': positive_count,
        'negative_count': negative_count,
        'zero_count': zero_count,
        'positive_ratio': positive_ratio,
        'negative_ratio': negative_ratio,
        'zero_ratio': zero_ratio,
        'mean_difference': np.mean(difference),
        'std_difference': np.std(difference),
        'max_difference': np.max(difference),
        'min_difference': np.min(difference)
    }
    
    return result

def calculate_compound_difference(data_dict, time_index, period_name):
    """è®¡ç®—å¤åˆå·®å€¼ï¼š(æŠ˜å æœºå‡ºåŒ…æ•° - å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡) Ã— 1.37 - å­˜çº¸ç‡"""
    required_cols = ['æŠ˜å æœºå‡ºåŒ…æ•°', 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'å­˜çº¸ç‡']
    missing_cols = [col for col in required_cols if col not in data_dict]
    
    if missing_cols:
        print(f"è­¦å‘Šï¼šæ—¶é—´æ®µ {period_name} ç¼ºå¤±åˆ—: {missing_cols}")
        return None
    
    # ç¡®ä¿æ‰€æœ‰æ•°æ®é•¿åº¦ä¸€è‡´
    min_length = min(len(data_dict[col]) for col in required_cols)
    if min_length == 0:
        print(f"è­¦å‘Šï¼šæ—¶é—´æ®µ {period_name} æ•°æ®ä¸ºç©º")
        return None
    
    # è·å–æ•°æ®
    folding_out = np.array(data_dict['æŠ˜å æœºå‡ºåŒ…æ•°'][:min_length])
    inner_loop = np.array(data_dict['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡'][:min_length])
    storage_rate = np.array(data_dict['å­˜çº¸ç‡'][:min_length])
    
    # ç¬¬ä¸€æ­¥ï¼šè®¡ç®— (æŠ˜å æœºå‡ºåŒ…æ•° - å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡) Ã— 1.37
    step1_diff = (folding_out - inner_loop) * 1.37
    
    # ç¬¬äºŒæ­¥ï¼šè®¡ç®— step1_diff - å­˜çº¸ç‡
    final_diff = step1_diff - storage_rate
    
    # ç»Ÿè®¡æ­£è´Ÿå·®å€¼
    positive_count = np.sum(final_diff > 0)
    negative_count = np.sum(final_diff < 0)
    zero_count = np.sum(final_diff == 0)
    total_count = len(final_diff)
    
    # è®¡ç®—æ¯”ä¾‹
    positive_ratio = positive_count / total_count * 100 if total_count > 0 else 0
    negative_ratio = negative_count / total_count * 100 if total_count > 0 else 0
    zero_ratio = zero_count / total_count * 100 if total_count > 0 else 0
    
    # åˆ›å»ºç»“æœå­—å…¸
    result = {
        'period_name': period_name,
        'calculation_formula': '(æŠ˜å æœºå‡ºåŒ…æ•° - å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡) Ã— 1.37 - å­˜çº¸ç‡',
        'time_index': time_index[:min_length],
        'folding_out_data': folding_out,
        'inner_loop_data': inner_loop,
        'storage_rate_data': storage_rate,
        'step1_diff': step1_diff,
        'final_difference': final_diff,
        'total_count': total_count,
        'positive_count': positive_count,
        'negative_count': negative_count,
        'zero_count': zero_count,
        'positive_ratio': positive_ratio,
        'negative_ratio': negative_ratio,
        'zero_ratio': zero_ratio,
        'mean_difference': np.mean(final_diff),
        'std_difference': np.std(final_diff),
        'max_difference': np.max(final_diff),
        'min_difference': np.min(final_diff),
        'step1_mean': np.mean(step1_diff),
        'step1_std': np.std(step1_diff)
    }
    
    return result

def save_compound_difference_analysis(all_results, output_dir):
    """ä¿å­˜å¤åˆå·®å€¼åˆ†æç»“æœåˆ°CSVæ–‡ä»¶"""
    if not all_results:
        print("æ²¡æœ‰å¤åˆå·®å€¼åˆ†æç»“æœå¯ä¿å­˜")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜è¯¦ç»†æ•°æ®
    detailed_data = []
    for result in all_results:
        for i, (time_point, folding_val, inner_val, storage_val, step1_val, final_val) in enumerate(zip(
            result['time_index'], result['folding_out_data'], result['inner_loop_data'], 
            result['storage_rate_data'], result['step1_diff'], result['final_difference'])):
            detailed_data.append({
                'æ—¶é—´æ®µ': result['period_name'],
                'æ—¶é—´ç‚¹': time_point,
                'æŠ˜å æœºå‡ºåŒ…æ•°': folding_val,
                'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡': inner_val,
                'å­˜çº¸ç‡': storage_val,
                'ç¬¬ä¸€æ­¥è®¡ç®—((æŠ˜å æœºå‡ºåŒ…æ•°-å¤–å¾ªç¯)Ã—1.37)': step1_val,
                'æœ€ç»ˆå·®å€¼(ç¬¬ä¸€æ­¥-å­˜çº¸ç‡)': final_val,
                'å·®å€¼ç¬¦å·': 'æ­£' if final_val > 0 else ('è´Ÿ' if final_val < 0 else 'é›¶')
            })
    
    detailed_df = pd.DataFrame(detailed_data)
    detailed_filename = "å¤åˆå·®å€¼åˆ†æ_è¯¦ç»†æ•°æ®.csv"
    detailed_path = os.path.join(output_dir, detailed_filename)
    detailed_df.to_csv(detailed_path, index=False, encoding='utf-8-sig')
    print(f"å¤åˆå·®å€¼è¯¦ç»†æ•°æ®å·²ä¿å­˜: {detailed_path}")
    
    # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
    summary_data = []
    for result in all_results:
        summary_data.append({
            'æ—¶é—´æ®µ': result['period_name'],
            'è®¡ç®—å…¬å¼': result['calculation_formula'],
            'æ€»æ•°æ®ç‚¹': result['total_count'],
            'æ­£å·®å€¼æ•°é‡': result['positive_count'],
            'è´Ÿå·®å€¼æ•°é‡': result['negative_count'],
            'é›¶å·®å€¼æ•°é‡': result['zero_count'],
            'æ­£å·®å€¼æ¯”ä¾‹(%)': round(result['positive_ratio'], 2),
            'è´Ÿå·®å€¼æ¯”ä¾‹(%)': round(result['negative_ratio'], 2),
            'é›¶å·®å€¼æ¯”ä¾‹(%)': round(result['zero_ratio'], 2),
            'æœ€ç»ˆå·®å€¼å¹³å‡å€¼': round(result['mean_difference'], 4),
            'æœ€ç»ˆå·®å€¼æ ‡å‡†å·®': round(result['std_difference'], 4),
            'æœ€ç»ˆå·®å€¼æœ€å¤§å€¼': round(result['max_difference'], 4),
            'æœ€ç»ˆå·®å€¼æœ€å°å€¼': round(result['min_difference'], 4),
            'ç¬¬ä¸€æ­¥è®¡ç®—å¹³å‡å€¼': round(result['step1_mean'], 4),
            'ç¬¬ä¸€æ­¥è®¡ç®—æ ‡å‡†å·®': round(result['step1_std'], 4)
        })
    
    summary_df = pd.DataFrame(summary_data)
    summary_filename = "å¤åˆå·®å€¼åˆ†æ_ç»Ÿè®¡æ±‡æ€».csv"
    summary_path = os.path.join(output_dir, summary_filename)
    summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
    print(f"å¤åˆå·®å€¼ç»Ÿè®¡æ±‡æ€»å·²ä¿å­˜: {summary_path}")
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    total_positive = sum(r['positive_count'] for r in all_results)
    total_negative = sum(r['negative_count'] for r in all_results)
    total_zero = sum(r['zero_count'] for r in all_results)
    total_all = sum(r['total_count'] for r in all_results)
    
    # ä¿å­˜æ€»ä½“ç»Ÿè®¡ç»“æœ
    if total_all > 0:
        total_positive_ratio = (total_positive / total_all) * 100
        total_negative_ratio = (total_negative / total_all) * 100
        total_zero_ratio = (total_zero / total_all) * 100
        
        # è®¡ç®—åŠ æƒå¹³å‡å·®å€¼
        total_weighted_sum = sum(result['mean_difference'] * result['total_count'] for result in all_results)
        average_difference = total_weighted_sum / total_all
        
        # åˆ¤æ–­æ•´ä½“è¶‹åŠ¿
        if total_positive_ratio > total_negative_ratio:
            trend = "å¤åˆè®¡ç®—ç»“æœ > 0 (æ­£å·®å€¼å ä¸»å¯¼)"
        elif total_negative_ratio > total_positive_ratio:
            trend = "å¤åˆè®¡ç®—ç»“æœ < 0 (è´Ÿå·®å€¼å ä¸»å¯¼)"
        else:
            trend = "å¤åˆè®¡ç®—ç»“æœ â‰ˆ 0 (æ­£è´Ÿå·®å€¼åŸºæœ¬ç›¸ç­‰)"
        
        # åˆ›å»ºæ€»ä½“ç»Ÿè®¡æ•°æ®
        overall_stats = {
            'åˆ†æé¡¹ç›®': ['(æŠ˜å æœºå‡ºåŒ…æ•° - å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡) Ã— 1.37 - å­˜çº¸ç‡'],
            'æ€»æ•°æ®ç‚¹æ•°': [total_all],
            'æ­£å·®å€¼æ•°é‡': [total_positive],
            'æ­£å·®å€¼æ¯”ä¾‹(%)': [round(total_positive_ratio, 2)],
            'è´Ÿå·®å€¼æ•°é‡': [total_negative],
            'è´Ÿå·®å€¼æ¯”ä¾‹(%)': [round(total_negative_ratio, 2)],
            'é›¶å·®å€¼æ•°é‡': [total_zero],
            'é›¶å·®å€¼æ¯”ä¾‹(%)': [round(total_zero_ratio, 2)],
            'å¹³å‡å·®å€¼': [round(average_difference, 4)],
            'æ€»ä½“è¶‹åŠ¿': [trend]
        }
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        overall_df = pd.DataFrame(overall_stats)
        overall_filename = "å¤åˆå·®å€¼åˆ†æ_æ€»ä½“ç»Ÿè®¡.csv"
        overall_path = os.path.join(output_dir, overall_filename)
        overall_df.to_csv(overall_path, index=False, encoding='utf-8-sig')
        print(f"å¤åˆå·®å€¼æ€»ä½“ç»Ÿè®¡å·²ä¿å­˜: {overall_path}")
        
        # è¾“å‡ºæ€»ä½“ç»Ÿè®¡ç»“æœ
        print("\n" + "="*80)
        print(f"ğŸ¯ å¤åˆå·®å€¼åˆ†ææ€»ä½“ç»Ÿè®¡ç»“æœ")
        print("="*80)
        print(f"è®¡ç®—å…¬å¼: (æŠ˜å æœºå‡ºåŒ…æ•° - å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡) Ã— 1.37 - å­˜çº¸ç‡")
        print(f"æ€»æ•°æ®ç‚¹æ•°: {total_all:,}")
        print(f"æ­£å·®å€¼: {total_positive:,} ä¸ª ({total_positive_ratio:.2f}%)")
        print(f"è´Ÿå·®å€¼: {total_negative:,} ä¸ª ({total_negative_ratio:.2f}%)")
        print(f"é›¶å·®å€¼: {total_zero:,} ä¸ª ({total_zero_ratio:.2f}%)")
        print(f"å¹³å‡å·®å€¼: {average_difference:.4f}")
        print("="*80)
        print(f"ğŸ“Š {trend}")
        print("="*80)
    
    print(f"\n=== å¤åˆå·®å€¼åˆ†ææ€»ç»“ ===")
    print(f"æ€»æ•°æ®ç‚¹: {total_all}")
    print(f"æ­£å·®å€¼: {total_positive} ä¸ª ({total_positive/total_all*100:.2f}%)")
    print(f"è´Ÿå·®å€¼: {total_negative} ä¸ª ({total_negative/total_all*100:.2f}%)")
    print(f"é›¶å·®å€¼: {total_zero} ä¸ª ({total_zero/total_all*100:.2f}%)")

def save_difference_analysis(all_results, col1, col2, output_dir):
    """ä¿å­˜å·®å€¼åˆ†æç»“æœåˆ°CSVæ–‡ä»¶"""
    if not all_results:
        print("æ²¡æœ‰å·®å€¼åˆ†æç»“æœå¯ä¿å­˜")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜è¯¦ç»†æ•°æ®
    detailed_data = []
    for result in all_results:
        for i, (time_point, diff_val, col1_val, col2_val) in enumerate(zip(
            result['time_index'], result['difference'], result['col1_data'], result['col2_data'])):
            detailed_data.append({
                'æ—¶é—´æ®µ': result['period_name'],
                'æ—¶é—´ç‚¹': time_point,
                f'{col1}': col1_val,
                f'{col2}': col2_val,
                'å·®å€¼': diff_val,
                'å·®å€¼ç¬¦å·': 'æ­£' if diff_val > 0 else ('è´Ÿ' if diff_val < 0 else 'é›¶')
            })
    
    detailed_df = pd.DataFrame(detailed_data)
    detailed_filename = f"{col1}_å‡_{col2}_è¯¦ç»†æ•°æ®.csv"
    detailed_path = os.path.join(output_dir, detailed_filename)
    detailed_df.to_csv(detailed_path, index=False, encoding='utf-8-sig')
    print(f"è¯¦ç»†å·®å€¼æ•°æ®å·²ä¿å­˜: {detailed_path}")
    
    # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
    summary_data = []
    for result in all_results:
        summary_data.append({
            'æ—¶é—´æ®µ': result['period_name'],
            'æ€»æ•°æ®ç‚¹': result['total_count'],
            'æ­£å·®å€¼æ•°é‡': result['positive_count'],
            'è´Ÿå·®å€¼æ•°é‡': result['negative_count'],
            'é›¶å·®å€¼æ•°é‡': result['zero_count'],
            'æ­£å·®å€¼æ¯”ä¾‹(%)': round(result['positive_ratio'], 2),
            'è´Ÿå·®å€¼æ¯”ä¾‹(%)': round(result['negative_ratio'], 2),
            'é›¶å·®å€¼æ¯”ä¾‹(%)': round(result['zero_ratio'], 2),
            'å¹³å‡å·®å€¼': round(result['mean_difference'], 4),
            'å·®å€¼æ ‡å‡†å·®': round(result['std_difference'], 4),
            'æœ€å¤§å·®å€¼': round(result['max_difference'], 4),
            'æœ€å°å·®å€¼': round(result['min_difference'], 4)
        })
    
    summary_df = pd.DataFrame(summary_data)
    summary_filename = f"{col1}_å‡_{col2}_ç»Ÿè®¡æ±‡æ€».csv"
    summary_path = os.path.join(output_dir, summary_filename)
    summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
    print(f"å·®å€¼ç»Ÿè®¡æ±‡æ€»å·²ä¿å­˜: {summary_path}")
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    total_positive = sum(r['positive_count'] for r in all_results)
    total_negative = sum(r['negative_count'] for r in all_results)
    total_zero = sum(r['zero_count'] for r in all_results)
    total_all = sum(r['total_count'] for r in all_results)
    
    print(f"\n=== {col1} - {col2} å·®å€¼åˆ†ææ€»ç»“ ===")
    print(f"æ€»æ•°æ®ç‚¹: {total_all}")
    print(f"æ­£å·®å€¼: {total_positive} ä¸ª ({total_positive/total_all*100:.2f}%)")
    print(f"è´Ÿå·®å€¼: {total_negative} ä¸ª ({total_negative/total_all*100:.2f}%)")
    print(f"é›¶å·®å€¼: {total_zero} ä¸ª ({total_zero/total_all*100:.2f}%)")
    print(f"å¹³å‡å·®å€¼: {np.mean([r['mean_difference'] for r in all_results]):.4f}")

def save_overall_statistics(col1, col2, total_data_points, total_positive_count, 
                           total_negative_count, total_zero_count, 
                           total_positive_ratio, total_negative_ratio, total_zero_ratio,
                           average_difference, trend, output_dir):
    """ä¿å­˜æ€»ä½“ç»Ÿè®¡ç»“æœåˆ°CSVæ–‡ä»¶"""
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆ›å»ºæ€»ä½“ç»Ÿè®¡æ•°æ®
        overall_stats = {
            'åˆ†æé¡¹ç›®': [f'{col1} - {col2}'],
            'æ€»æ•°æ®ç‚¹æ•°': [total_data_points],
            'æ­£å·®å€¼æ•°é‡': [total_positive_count],
            'æ­£å·®å€¼æ¯”ä¾‹(%)': [round(total_positive_ratio, 2)],
            'è´Ÿå·®å€¼æ•°é‡': [total_negative_count],
            'è´Ÿå·®å€¼æ¯”ä¾‹(%)': [round(total_negative_ratio, 2)],
            'é›¶å·®å€¼æ•°é‡': [total_zero_count],
            'é›¶å·®å€¼æ¯”ä¾‹(%)': [round(total_zero_ratio, 2)],
            'å¹³å‡å·®å€¼': [round(average_difference, 4)],
            'æ€»ä½“è¶‹åŠ¿': [trend]
        }
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        overall_df = pd.DataFrame(overall_stats)
        filename = f"{col1}_å‡_{col2}_æ€»ä½“ç»Ÿè®¡.csv"
        filepath = os.path.join(output_dir, filename)
        overall_df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        print(f"æ€»ä½“ç»Ÿè®¡ç»“æœå·²ä¿å­˜: {filepath}")
        
    except Exception as e:
        print(f"ä¿å­˜æ€»ä½“ç»Ÿè®¡ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")

def get_user_column_selection(available_columns):
    """è·å–ç”¨æˆ·é€‰æ‹©çš„ä¸¤åˆ—æ•°æ®"""
    print("\n=== å·®å€¼åˆ†æåŠŸèƒ½ ===")
    print("å¯ç”¨çš„åˆ—å:")
    for i, col in enumerate(available_columns, 1):
        print(f"{i:2d}. {col}")
    
    print(f"\nè¯·é€‰æ‹©ä¸¤åˆ—æ•°æ®è¿›è¡Œå·®å€¼åˆ†æï¼ˆæ ¼å¼ï¼šåˆ—å1,åˆ—å2ï¼‰")
    print("æˆ–è¾“å…¥ 'skip' è·³è¿‡å·®å€¼åˆ†æ")
    
    user_input = input("è¯·è¾“å…¥: ").strip()
    
    if user_input.lower() == 'skip':
        return None, None
    
    try:
        col1, col2 = [col.strip() for col in user_input.split(',')]
        if col1 in available_columns and col2 in available_columns:
            return col1, col2
        else:
            print("é”™è¯¯ï¼šè¾“å…¥çš„åˆ—åä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­")
            return None, None
    except ValueError:
        print("é”™è¯¯ï¼šè¯·æŒ‰æ ¼å¼è¾“å…¥ï¼Œä¾‹å¦‚ï¼šæŠ˜å æœºå…¥åŒ…æ•°,æŠ˜å æœºå‡ºåŒ…æ•°")
        return None, None

def main():
    """ä¸»å‡½æ•°"""
    
    # ================================================================
    # ğŸ”§ é…ç½®å‚æ•°åŒºåŸŸ - æ‚¨å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹æ—¶é—´é—´éš”
    # ================================================================
    TIME_INTERVAL_SECONDS = 1  # æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰
    """
    å¯è®¾ç½®ä¸ºä»»æ„ç§’æ•°ï¼Œä¾‹å¦‚ï¼š
    - 60  : 1åˆ†é’Ÿé—´éš”
    - 30  : 30ç§’é—´éš”
    - 20  : 20ç§’é—´éš”  
    - 10  : 10ç§’é—´éš”
    - 5   : 5ç§’é—´éš”
    - 3   : 3ç§’é—´éš”
    - 1   : 1ç§’é—´éš”ï¼ˆæœ€å°ç²’åº¦ï¼‰
    """
    # ================================================================
    
    print(f"ğŸ“Š å½“å‰è®¾ç½®ï¼šæ¯ {TIME_INTERVAL_SECONDS} ç§’ç»Ÿè®¡ä¸€æ¬¡äº§é‡æ•°æ®")
    
    # è¯»å–æ—¶é—´æ®µæ–‡ä»¶
    time_periods_file = "æŠ˜å æœºæ­£å¸¸è¿è¡Œä¸”é«˜å­˜çº¸ç‡æ—¶é—´æ®µ_æœ€ç»ˆç»“æœ.csv"
    summary_file = "å­˜çº¸æ¶æ•°æ®æ±‡æ€».csv"
    
    try:
        # è¯»å–æ—¶é—´æ®µæ•°æ®
        time_periods_df = pd.read_csv(time_periods_file)
        print(f"æˆåŠŸè¯»å–æ—¶é—´æ®µæ–‡ä»¶ï¼Œå…± {len(time_periods_df)} ä¸ªæ—¶é—´æ®µ")
        
        # è¯»å–æ±‡æ€»æ•°æ®
        summary_df = read_csv_with_encoding(summary_file)
        print(f"æˆåŠŸè¯»å–æ±‡æ€»æ–‡ä»¶ï¼Œå…± {len(summary_df)} è¡Œæ•°æ®")
        print(f"æ±‡æ€»æ–‡ä»¶åˆ—å: {list(summary_df.columns)}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = f"æ—¶é—´æ®µåˆ†æå›¾è¡¨_{TIME_INTERVAL_SECONDS}ç§’"
        combined_output_dir = f"ç»„åˆå›¾è¡¨åˆ†æ_{TIME_INTERVAL_SECONDS}ç§’"
        difference_output_dir = f"å·®å€¼åˆ†æç»“æœ_{TIME_INTERVAL_SECONDS}ç§’"
        
        # å­˜å‚¨æ‰€æœ‰æ—¶é—´æ®µçš„æ•°æ®ç”¨äºå·®å€¼åˆ†æ
        all_period_data = []
        available_columns = set()
        
        # å­˜å‚¨æ‰€æœ‰æ—¶é—´åç§»åˆ†æç»“æœ
        all_shift_analysis_results = []
        
        # å¤„ç†æ¯ä¸ªæ—¶é—´æ®µ
        for idx, row in time_periods_df.iterrows():
            start_time = pd.to_datetime(row['å¼€å§‹æ—¶é—´'])
            end_time = pd.to_datetime(row['ç»“æŸæ—¶é—´'])
            
            print(f"\nå¤„ç†æ—¶é—´æ®µ {idx+1}/{len(time_periods_df)}: {start_time} åˆ° {end_time}")
            
            # å¤„ç†æ•°æ®
            result = process_data_for_time_period(summary_df, start_time, end_time, TIME_INTERVAL_SECONDS)
            
            if result is not None:
                data_dict, time_index = result
                
                # æ”¶é›†å¯ç”¨åˆ—å
                available_columns.update(data_dict.keys())
                
                # å­˜å‚¨æ•°æ®ç”¨äºå·®å€¼åˆ†æ
                period_name = f"{start_time.strftime('%Y%m%d_%H%M%S')}_{end_time.strftime('%Y%m%d_%H%M%S')}"
                all_period_data.append({
                    'period_name': period_name,
                    'data_dict': data_dict,
                    'time_index': time_index
                })
                
                # ç»˜åˆ¶å›¾è¡¨
                plot_data(data_dict, time_index, start_time, end_time, output_dir, TIME_INTERVAL_SECONDS)
                
                # ç»˜åˆ¶ç»„åˆå›¾è¡¨å¹¶è·å–æ—¶é—´åç§»åˆ†æç»“æœ
                shift_results = plot_combined_data(data_dict, time_index, start_time, end_time, combined_output_dir, TIME_INTERVAL_SECONDS)
                if shift_results:
                    all_shift_analysis_results.extend(shift_results)
            else:
                print(f"è·³è¿‡æ—¶é—´æ®µ {idx+1}ï¼Œæ— æ•°æ®")
        
        print(f"\næ‰€æœ‰å•é¡¹å›¾è¡¨å·²ä¿å­˜åˆ°ç›®å½•: {output_dir}")
        print(f"æ‰€æœ‰ç»„åˆå›¾è¡¨å·²ä¿å­˜åˆ°ç›®å½•: {combined_output_dir}")
        
        # ä¿å­˜æ—¶é—´åç§»ç›¸å…³æ€§åˆ†æç»“æœ
        if all_shift_analysis_results:
            # åˆ›å»ºæ—¶é—´åç§»åˆ†æç»“æœç›®å½•
            shift_analysis_output_dir = f"æ—¶é—´åç§»ç›¸å…³æ€§åˆ†æç»“æœ_{TIME_INTERVAL_SECONDS}ç§’"
            os.makedirs(shift_analysis_output_dir, exist_ok=True)
            
            # è½¬æ¢ä¸ºDataFrame
            df_results = pd.DataFrame(all_shift_analysis_results)
            
            # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåºï¼Œä¾¿äºé˜…è¯»ï¼Œå¹¶è½¬æ¢ä¸ºä¸­æ–‡åˆ—å
            columns_mapping = {
                'time_period': 'æ—¶é—´æ®µæ ‡è¯†',
                'chart_title': 'å›¾è¡¨æ ‡é¢˜', 
                'start_time': 'å¼€å§‹æ—¶é—´',
                'end_time': 'ç»“æŸæ—¶é—´',
                'col1_name': 'ç¬¬ä¸€ä¸ªæŒ‡æ ‡',
                'col2_name': 'ç¬¬äºŒä¸ªæŒ‡æ ‡',
                'best_shift_seconds': 'æœ€ä½³åç§»æ—¶é—´(ç§’)',
                'best_correlation': 'æœ€ä½³ç›¸å…³ç³»æ•°',
                'shift_description': 'æ—¶é—´åç§»æè¿°',
                'data_points': 'æ•°æ®ç‚¹æ•°é‡',
                'time_interval_seconds': 'æ—¶é—´é—´éš”è®¾ç½®(ç§’)'
            }
            
            # é‡å‘½ååˆ—
            df_results = df_results.rename(columns=columns_mapping)
            
            # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåº
            columns_order = [
                'æ—¶é—´æ®µæ ‡è¯†', 'å›¾è¡¨æ ‡é¢˜', 'å¼€å§‹æ—¶é—´', 'ç»“æŸæ—¶é—´',
                'ç¬¬ä¸€ä¸ªæŒ‡æ ‡', 'ç¬¬äºŒä¸ªæŒ‡æ ‡', 'æœ€ä½³åç§»æ—¶é—´(ç§’)', 'æœ€ä½³ç›¸å…³ç³»æ•°',
                'æ—¶é—´åç§»æè¿°', 'æ•°æ®ç‚¹æ•°é‡', 'æ—¶é—´é—´éš”è®¾ç½®(ç§’)'
            ]
            df_results = df_results[columns_order]
            
            # ä¿å­˜æ±‡æ€»ç»“æœï¼ˆæŒ‰ç…§æœ€ä½³ç›¸å…³ç³»æ•°é™åºæ’åºï¼‰
            summary_filename = f"æ—¶é—´åç§»ç›¸å…³æ€§åˆ†æ_æ±‡æ€»ç»“æœ_{TIME_INTERVAL_SECONDS}ç§’.csv"
            summary_filepath = os.path.join(shift_analysis_output_dir, summary_filename)
            # æŒ‰ç…§æœ€ä½³ç›¸å…³ç³»æ•°é™åºæ’åº
            df_results_sorted = df_results.sort_values(by='æœ€ä½³ç›¸å…³ç³»æ•°', ascending=False)
            df_results_sorted.to_csv(summary_filepath, index=False, encoding='utf-8-sig',float_format='%.3f')
            print(f"\næ—¶é—´åç§»ç›¸å…³æ€§åˆ†ææ±‡æ€»ç»“æœå·²ä¿å­˜(å·²æŒ‰ç›¸å…³ç³»æ•°æ’åº): {summary_filepath}")
            
            # æŒ‰å›¾è¡¨ç±»å‹åˆ†ç»„ä¿å­˜ï¼ˆæ¯ç»„å†…æŒ‰ç…§æœ€ä½³ç›¸å…³ç³»æ•°é™åºæ’åºï¼‰
            grouped = df_results.groupby('å›¾è¡¨æ ‡é¢˜')
            for chart_title, group in grouped:
                safe_title = chart_title.replace('/', '_').replace('\\', '_').replace(':', '_')
                group_filename = f"æ—¶é—´åç§»åˆ†æ_{safe_title}_{TIME_INTERVAL_SECONDS}ç§’.csv"
                group_filepath = os.path.join(shift_analysis_output_dir, group_filename)
                # æ¯ç»„å†…æŒ‰æœ€ä½³ç›¸å…³ç³»æ•°é™åºæ’åº
                sorted_group = group.sort_values(by='æœ€ä½³ç›¸å…³ç³»æ•°', ascending=False)
                sorted_group.to_csv(group_filepath, index=False, encoding='utf-8-sig',float_format='%.3f')
                print(f"  â†’ {chart_title} åˆ†æç»“æœå·²ä¿å­˜(å·²æŒ‰ç›¸å…³ç³»æ•°æ’åº): {group_filepath}")
            
            # ç”Ÿæˆåˆ†ææ€»ç»“å¹¶ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶
            summary_lines = []
            summary_lines.append(f"=== æ—¶é—´åç§»ç›¸å…³æ€§åˆ†ææ€»ç»“ ===")
            summary_lines.append(f"æ€»åˆ†æç»„åˆæ•°: {len(all_shift_analysis_results)}")
            summary_lines.append(f"æ¶‰åŠå›¾è¡¨ç±»å‹: {len(grouped)} ç§")
            summary_lines.append(f"æ—¶é—´é—´éš”è®¾ç½®: {TIME_INTERVAL_SECONDS} ç§’")
            
            # æ˜¾ç¤ºç›¸å…³æ€§æœ€é«˜çš„å‰5ä¸ªç»“æœ
            top_correlations = df_results_sorted.head(5)
            summary_lines.append(f"\nç›¸å…³æ€§æœ€é«˜çš„å‰5ä¸ªç»“æœ:")
            for idx, row in top_correlations.iterrows():
                summary_line = f"  {row['å›¾è¡¨æ ‡é¢˜']}: {row['æ—¶é—´åç§»æè¿°']}, ç›¸å…³ç³»æ•°: {row['æœ€ä½³ç›¸å…³ç³»æ•°']:.3f}"
                summary_lines.append(summary_line)
            
            # æ˜¾ç¤ºå»¶è¿Ÿæ—¶é—´ç»Ÿè®¡
            delay_stats = df_results['æœ€ä½³åç§»æ—¶é—´(ç§’)'].describe()
            summary_lines.append(f"\næ—¶é—´åç§»ç»Ÿè®¡ (ç§’):")
            summary_lines.append(f"  å¹³å‡åç§»: {delay_stats['mean']:.1f} ç§’")
            summary_lines.append(f"  åç§»èŒƒå›´: {delay_stats['min']:.0f} è‡³ {delay_stats['max']:.0f} ç§’")
            summary_lines.append(f"  æ ‡å‡†å·®: {delay_stats['std']:.1f} ç§’")
            
            # ä¿å­˜åˆ†ææ€»ç»“åˆ°æ–‡æœ¬æ–‡ä»¶
            summary_text_filename = f"æ—¶é—´åç§»åˆ†ææ€»ç»“_{TIME_INTERVAL_SECONDS}ç§’.txt"
            summary_text_filepath = os.path.join(shift_analysis_output_dir, summary_text_filename)
            
            # æ·»åŠ æ¯ä¸ªç»„åˆçš„è¯¦ç»†åˆ†æç»“æœï¼ˆæŒ‰ç›¸å…³ç³»æ•°æ’åºï¼‰
            detailed_summary_lines = []
            detailed_summary_lines.append("\n" + "=" * 80)
            detailed_summary_lines.append("ã€è¯¦ç»†æ—¶é—´åç§»åˆ†æç»“æœã€‘")
            detailed_summary_lines.append("=" * 80)
            detailed_summary_lines.append("")
            
            # æŒ‰å›¾è¡¨åç§°åˆ†ç»„æ˜¾ç¤ºè¯¦ç»†ç»“æœ
            for chart_title, group_data in grouped:
                detailed_summary_lines.append(f"ã€{chart_title}ã€‘:")
                # æŒ‰æœ€ä½³ç›¸å…³ç³»æ•°æ’åº
                sorted_group = group_data.sort_values(by=['æœ€ä½³ç›¸å…³ç³»æ•°'], ascending=False)
                for i, (_, row) in enumerate(sorted_group.iterrows()):
                    detailed_summary_lines.append(f"  {i+1}. æ—¶é—´æ®µ: {row['å¼€å§‹æ—¶é—´']} è‡³ {row['ç»“æŸæ—¶é—´']}")
                    detailed_summary_lines.append(f"     {row['æ—¶é—´åç§»æè¿°']}")
                    detailed_summary_lines.append(f"     ç›¸å…³ç³»æ•°: {row['æœ€ä½³ç›¸å…³ç³»æ•°']:.4f}")
                    detailed_summary_lines.append(f"     æ•°æ®ç‚¹æ•°: {row['æ•°æ®ç‚¹æ•°é‡']}")
                    detailed_summary_lines.append("")
                detailed_summary_lines.append("-" * 50)
                detailed_summary_lines.append("")
            
            # æ·»åŠ æµç¨‹é“¾åˆ†æ - æŒ‰ç”Ÿäº§æµç¨‹é¡ºåºåˆ†ææ—¶é—´ä¼ é€’
            flow_analysis_lines = []
            flow_analysis_lines.append("=" * 80)
            flow_analysis_lines.append("ã€ç”Ÿäº§æµç¨‹æ—¶é—´ä¼ é€’åˆ†æã€‘")
            flow_analysis_lines.append("=" * 80)
            flow_analysis_lines.append("")
            
            # å®šä¹‰å…³é”®æµç¨‹é“¾
            process_chains = [
                # å…±åŒå‰ç½®æµç¨‹é“¾
                ["å…±åŒå‰ç½®æµç¨‹1", "å…±åŒå‰ç½®æµç¨‹2"],
                # ç”Ÿäº§çº¿1æµç¨‹é“¾
                ["å¤–å¾ªç¯åˆ†æµè¿æ¥1", "ç”Ÿäº§çº¿1æµç¨‹1", "ç”Ÿäº§çº¿1æµç¨‹2"],
                # ç”Ÿäº§çº¿2æµç¨‹é“¾
                ["å¤–å¾ªç¯åˆ†æµè¿æ¥2", "ç”Ÿäº§çº¿2æµç¨‹1", "ç”Ÿäº§çº¿2æµç¨‹2"],
                # ç”Ÿäº§çº¿3æµç¨‹é“¾
                ["å¤–å¾ªç¯åˆ†æµè¿æ¥3", "ç”Ÿäº§çº¿3æµç¨‹1", "ç”Ÿäº§çº¿3æµç¨‹2"],
                # ç”Ÿäº§çº¿4æµç¨‹é“¾
                ["å¤–å¾ªç¯åˆ†æµè¿æ¥4", "ç”Ÿäº§çº¿4æµç¨‹1", "ç”Ÿäº§çº¿4æµç¨‹2"]
            ]
            
            # è®¡ç®—æ¯æ¡æµç¨‹é“¾çš„å¹³å‡æ—¶é—´åç§»
            for i, chain in enumerate(process_chains):
                chain_name = f"æµç¨‹é“¾ {i+1}" if i > 0 else "å…±åŒå‰ç½®æµç¨‹é“¾"
                flow_analysis_lines.append(f"ã€{chain_name}ã€‘")
                
                # æ”¶é›†é“¾ä¸­å›¾è¡¨çš„å¹³å‡æ—¶é—´åç§»
                chain_shifts = []
                for chart_title in chain:
                    if chart_title in grouped.groups:
                        # è·å–è¯¥å›¾è¡¨çš„æ‰€æœ‰æ—¶é—´åç§»æ•°æ®
                        chart_data = grouped.get_group(chart_title)
                        avg_shift = chart_data['æœ€ä½³åç§»æ—¶é—´(ç§’)'].mean()
                        avg_corr = chart_data['æœ€ä½³ç›¸å…³ç³»æ•°'].mean()
                        chain_shifts.append((chart_title, avg_shift, avg_corr))
                        flow_analysis_lines.append(f"  {chart_title}: å¹³å‡åç§» {avg_shift:.1f} ç§’, å¹³å‡ç›¸å…³ç³»æ•° {avg_corr:.4f}")
                
                # è®¡ç®—æ€»åç§»
                if chain_shifts:
                    total_shift = sum(shift for _, shift, _ in chain_shifts)
                    flow_analysis_lines.append(f"  >>> æ€»ä½“å»¶è¿Ÿ: {total_shift:.1f} ç§’")
                    
                    # è®¡ç®—å¹³å‡ç›¸å…³ç³»æ•°
                    avg_chain_corr = sum(corr for _, _, corr in chain_shifts) / len(chain_shifts)
                    flow_analysis_lines.append(f"  >>> é“¾å¹³å‡ç›¸å…³ç³»æ•°: {avg_chain_corr:.4f}")
                flow_analysis_lines.append("")
            
            # åˆå¹¶æ‰€æœ‰å†…å®¹
            all_summary_lines = summary_lines + detailed_summary_lines + flow_analysis_lines
            with open(summary_text_filepath, 'w', encoding='utf-8') as f:
                f.write('\n'.join(all_summary_lines))
            
            # åœ¨ç»ˆç«¯æ˜¾ç¤ºåˆ†ææ€»ç»“
            print(f"\n=== æ—¶é—´åç§»ç›¸å…³æ€§åˆ†ææ€»ç»“ ===")
            print(f"æ€»åˆ†æç»„åˆæ•°: {len(all_shift_analysis_results)}")
            print(f"æ¶‰åŠå›¾è¡¨ç±»å‹: {len(grouped)} ç§")
            print(f"æ—¶é—´é—´éš”è®¾ç½®: {TIME_INTERVAL_SECONDS} ç§’")
            
            print(f"\nç›¸å…³æ€§æœ€é«˜çš„å‰5ä¸ªç»“æœ:")
            for idx, row in top_correlations.iterrows():
                print(f"  {row['å›¾è¡¨æ ‡é¢˜']}: {row['æ—¶é—´åç§»æè¿°']}, ç›¸å…³ç³»æ•°: {row['æœ€ä½³ç›¸å…³ç³»æ•°']:.3f}")
            
            print(f"\næ—¶é—´åç§»ç»Ÿè®¡ (ç§’):")
            print(f"  å¹³å‡åç§»: {delay_stats['mean']:.1f} ç§’")
            print(f"  åç§»èŒƒå›´: {delay_stats['min']:.0f} è‡³ {delay_stats['max']:.0f} ç§’")
            print(f"  æ ‡å‡†å·®: {delay_stats['std']:.1f} ç§’")
            print(f"\nåˆ†ææ€»ç»“å·²ä¿å­˜åˆ°: {summary_text_filepath}")
        else:
            print(f"\næœªç”Ÿæˆæ—¶é—´åç§»ç›¸å…³æ€§åˆ†æç»“æœ")
        
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 