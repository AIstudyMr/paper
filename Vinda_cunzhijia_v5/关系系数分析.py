import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def read_csv_with_encoding(file_path, encodings=['utf-8', 'gbk', 'gb2312', 'utf-8-sig']):
    """å°è¯•ä¸åŒç¼–ç è¯»å–CSVæ–‡ä»¶"""
    for encoding in encodings:
        try:
            df = pd.read_csv(file_path, encoding=encoding)
            print(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶")
            return df
        except UnicodeDecodeError:
            continue
        except Exception as e:
            print(f"ä½¿ç”¨ {encoding} ç¼–ç æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue
    raise ValueError("æ— æ³•è¯»å–æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ç¼–ç ")

def process_data_for_time_period(summary_df, start_time, end_time):
    """ä¸ºæŒ‡å®šæ—¶é—´æ®µå¤„ç†æ•°æ®ï¼ˆå¤åˆ¶è‡ªæ•°æ®åˆ†æå¤„ç†.pyï¼‰"""
    # è½¬æ¢æ—¶é—´åˆ—
    summary_df['æ—¶é—´'] = pd.to_datetime(summary_df['æ—¶é—´'])
    
    # ç­›é€‰æ—¶é—´æ®µå†…çš„æ•°æ®
    mask = (summary_df['æ—¶é—´'] >= start_time) & (summary_df['æ—¶é—´'] <= end_time)
    period_data = summary_df.loc[mask].copy()
    
    if period_data.empty:
        print(f"è­¦å‘Šï¼šæ—¶é—´æ®µ {start_time} åˆ° {end_time} æ²¡æœ‰æ•°æ®")
        return None
    
    # æŒ‰åˆ†é’Ÿé‡é‡‡æ ·
    period_data.set_index('æ—¶é—´', inplace=True)
    
    # å®šä¹‰éœ€è¦çš„åˆ—
    required_columns = [
        'æŠ˜å æœºå®é™…é€Ÿåº¦', 'æŠ˜å æœºå…¥åŒ…æ•°', 'æŠ˜å æœºå‡ºåŒ…æ•°', 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'å­˜çº¸ç‡',
        'è£åˆ‡æœºå®é™…é€Ÿåº¦', 'æœ‰æ•ˆæ€»åˆ‡æ•°', '1#æœ‰æ•ˆåˆ‡æ•°', '2#æœ‰æ•ˆåˆ‡æ•°', '3#æœ‰æ•ˆåˆ‡æ•°', '4#æœ‰æ•ˆåˆ‡æ•°',
        'è¿›ç¬¬ä¸€è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬äºŒè£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬ä¸‰è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬å››è£åˆ‡é€šé“çº¸æ¡è®¡æ•°',
        '1#å°åŒ…æœºå…¥åŒ…æ•°', '1#å°åŒ…æœºå®é™…é€Ÿåº¦', '2#å°åŒ…æœºå…¥åŒ…æ•°', '2#å°åŒ…æœºå®é™…é€Ÿåº¦',
        '3#å°åŒ…æœºå…¥åŒ…æ•°', '3#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦', '4#å°åŒ…æœºå…¥åŒ…æ•°', '4#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦'
    ]
    
    # æ£€æŸ¥ç¼ºå¤±çš„åˆ—
    missing_cols = [col for col in required_columns if col not in period_data.columns]
    if missing_cols:
        print(f"è­¦å‘Šï¼šç¼ºå¤±ä»¥ä¸‹åˆ—: {missing_cols}")
        # ä½¿ç”¨å¯ç”¨çš„åˆ—
        available_cols = [col for col in required_columns if col in period_data.columns]
        if not available_cols:
            print("é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•éœ€è¦çš„åˆ—")
            return None
        required_columns = available_cols
    
    # åˆ›å»ºç»“æœå­—å…¸
    result_data = {}
    
    # ç´¯ç§¯é‡åˆ—ï¼ˆè®¡ç®—æ¯åˆ†é’Ÿå·®å€¼ï¼‰
    cumulative_cols = [
        'æŠ˜å æœºå…¥åŒ…æ•°', 'æŠ˜å æœºå‡ºåŒ…æ•°', 'æœ‰æ•ˆæ€»åˆ‡æ•°', '1#æœ‰æ•ˆåˆ‡æ•°', '2#æœ‰æ•ˆåˆ‡æ•°', 
        '3#æœ‰æ•ˆåˆ‡æ•°', '4#æœ‰æ•ˆåˆ‡æ•°', '1#å°åŒ…æœºå…¥åŒ…æ•°', '2#å°åŒ…æœºå…¥åŒ…æ•°', 
        '3#å°åŒ…æœºå…¥åŒ…æ•°', '4#å°åŒ…æœºå…¥åŒ…æ•°', 'å­˜çº¸ç‡'
    ]
    
    # ç¬æ—¶é‡åˆ—
    instantaneous_cols = [
        'æŠ˜å æœºå®é™…é€Ÿåº¦', 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'è£åˆ‡æœºå®é™…é€Ÿåº¦',
        'è¿›ç¬¬ä¸€è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬äºŒè£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬ä¸‰è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬å››è£åˆ‡é€šé“çº¸æ¡è®¡æ•°',
        '1#å°åŒ…æœºå®é™…é€Ÿåº¦', '2#å°åŒ…æœºå®é™…é€Ÿåº¦', '3#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦', '4#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦'
    ]
    
    # æŒ‰åˆ†é’Ÿé‡é‡‡æ ·å¤„ç†
    minute_data = period_data.resample('1T')
    
    # å¤„ç†ç´¯ç§¯é‡
    for col in cumulative_cols:
        if col in period_data.columns:
            # è®¡ç®—æ¯åˆ†é’Ÿçš„å·®å€¼
            minute_diff = minute_data[col].last().diff().fillna(0)
            # æŠ˜å æœºå‡ºåŒ…æ•°å’ŒæŠ˜å æœºå…¥åŒ…æ•°éœ€è¦é™¤ä»¥25
            if col in ['æŠ˜å æœºå‡ºåŒ…æ•°', 'æŠ˜å æœºå…¥åŒ…æ•°', 'æœ‰æ•ˆæ€»åˆ‡æ•°', '1#æœ‰æ•ˆåˆ‡æ•°', 
                       '2#æœ‰æ•ˆåˆ‡æ•°', '3#æœ‰æ•ˆåˆ‡æ•°', '4#æœ‰æ•ˆåˆ‡æ•°', '1#å°åŒ…æœºå…¥åŒ…æ•°', 
                       '2#å°åŒ…æœºå…¥åŒ…æ•°', '3#å°åŒ…æœºå…¥åŒ…æ•°', '4#å°åŒ…æœºå…¥åŒ…æ•°']:
                result_data[col] = (minute_diff / 25).values
            elif col == 'å­˜çº¸ç‡':
                # å­˜çº¸ç‡è®¡ç®—æ¯åˆ†é’Ÿå·®å€¼ï¼Œä¸é™¤ä»¥25
                result_data[col] = minute_diff.values
            else:
                result_data[col] = minute_diff.values
    
    # å¤„ç†ç¬æ—¶é‡
    if 'æŠ˜å æœºå®é™…é€Ÿåº¦' in period_data.columns:
        # è®¡ç®—æ¯åˆ†é’Ÿå¹³å‡å€¼å†é™¤ä»¥9.75
        avg_speed = minute_data['æŠ˜å æœºå®é™…é€Ÿåº¦'].mean()
        result_data['æŠ˜å æœºå®é™…é€Ÿåº¦'] = (avg_speed / 9.75).round(2).values
    
    if 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡' in period_data.columns:
        # è®¡ç®—æ¯åˆ†é’Ÿçš„å’Œ
        result_data['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡'] = minute_data['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡'].sum().values
    
    if 'è£åˆ‡æœºå®é™…é€Ÿåº¦' in period_data.columns:
        # è®¡ç®—æ¯åˆ†é’Ÿå¹³å‡å€¼å†é™¤ä»¥9.75
        avg_speed = minute_data['è£åˆ‡æœºå®é™…é€Ÿåº¦'].mean()
        result_data['è£åˆ‡æœºå®é™…é€Ÿåº¦'] = (avg_speed / 9.75).round(2).values
    
    # å¤„ç†è£åˆ‡é€šé“çº¸æ¡è®¡æ•°
    cut_channel_cols = ['è¿›ç¬¬ä¸€è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬äºŒè£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬ä¸‰è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'è¿›ç¬¬å››è£åˆ‡é€šé“çº¸æ¡è®¡æ•°']
    for col in cut_channel_cols:
        if col in period_data.columns:
            result_data[col] = minute_data[col].sum().values
    
    # å¤„ç†å°åŒ…æœºé€Ÿåº¦
    packer_speed_cols = ['1#å°åŒ…æœºå®é™…é€Ÿåº¦', '2#å°åŒ…æœºå®é™…é€Ÿåº¦', '3#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦', '4#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦']
    packer_speeds = []
    for col in packer_speed_cols:
        if col in period_data.columns:
            avg_speed = minute_data[col].mean()
            speed_processed = (avg_speed / 25).round(2)
            result_data[col] = speed_processed.values
            packer_speeds.append(speed_processed.values)
    
    # è®¡ç®—å°åŒ…æœºé€Ÿåº¦æ€»å’Œ
    if packer_speeds:
        packer_speed_sum = np.sum(packer_speeds, axis=0)
        result_data['å°åŒ…æœºé€Ÿåº¦æ€»å’Œ'] = packer_speed_sum
    
    # è®¡ç®—å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ
    packer_input_cols = ['1#å°åŒ…æœºå…¥åŒ…æ•°', '2#å°åŒ…æœºå…¥åŒ…æ•°', '3#å°åŒ…æœºå…¥åŒ…æ•°', '4#å°åŒ…æœºå…¥åŒ…æ•°']
    packer_inputs = []
    for col in packer_input_cols:
        if col in result_data:
            packer_inputs.append(result_data[col])
    
    if packer_inputs:
        packer_input_sum = np.sum(packer_inputs, axis=0)
        result_data['å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ'] = packer_input_sum
    
    # åˆ›å»ºæ—¶é—´ç´¢å¼•
    time_index = minute_data.groups.keys()
    
    return result_data, list(time_index)

def analyze_correlation(data_dict, period_name):
    """åˆ†æä¸‰ä¸ªå˜é‡ä¹‹é—´çš„ç›¸å…³æ€§"""
    target_columns = ['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ', 'æœ‰æ•ˆæ€»åˆ‡æ•°']
    
    # æ£€æŸ¥æ‰€éœ€åˆ—æ˜¯å¦å­˜åœ¨
    missing_cols = [col for col in target_columns if col not in data_dict]
    if missing_cols:
        print(f"æ—¶é—´æ®µ {period_name} ç¼ºå¤±åˆ—: {missing_cols}")
        return None
    
    # è·å–æ•°æ®å¹¶ç¡®ä¿é•¿åº¦ä¸€è‡´
    min_length = min(len(data_dict[col]) for col in target_columns)
    if min_length == 0:
        print(f"æ—¶é—´æ®µ {period_name} æ•°æ®ä¸ºç©º")
        return None
    
    # æ„å»ºæ•°æ®çŸ©é˜µ
    data_matrix = np.array([data_dict[col][:min_length] for col in target_columns]).T
    
    # åˆ›å»ºDataFrameä¾¿äºåˆ†æ
    df = pd.DataFrame(data_matrix, columns=target_columns)
    
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    correlation_matrix = df.corr()
    
    # è®¡ç®—å„ç§ç»Ÿè®¡æŒ‡æ ‡
    result = {
        'period_name': period_name,
        'data_count': min_length,
        'correlation_matrix': correlation_matrix,
        'data_frame': df,
        'statistics': df.describe()
    }
    
    return result

def perform_regression_analysis(combined_df):
    """æ‰§è¡Œå›å½’åˆ†æ"""
    print("\n" + "="*80)
    print("å¤šå…ƒçº¿æ€§å›å½’åˆ†æ")
    print("="*80)
    
    # å‡†å¤‡æ•°æ®
    X = combined_df[['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ']]
    y = combined_df['æœ‰æ•ˆæ€»åˆ‡æ•°']
    
    # ç§»é™¤åŒ…å«NaNæˆ–æ— ç©·å¤§çš„è¡Œ
    mask = np.isfinite(X.values).all(axis=1) & np.isfinite(y.values)
    X_clean = X[mask]
    y_clean = y[mask]
    
    if len(X_clean) == 0:
        print("è­¦å‘Šï¼šæ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®è¿›è¡Œå›å½’åˆ†æ")
        return None
    
    # æ‰§è¡Œå¤šå…ƒçº¿æ€§å›å½’
    reg = LinearRegression()
    reg.fit(X_clean, y_clean)
    
    # é¢„æµ‹
    y_pred = reg.predict(X_clean)
    
    # è®¡ç®—RÂ²
    r2 = r2_score(y_clean, y_pred)
    
    # è®¡ç®—è°ƒæ•´åçš„RÂ²
    n = len(y_clean)
    p = X_clean.shape[1]
    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
    
    print(f"å›å½’æ–¹ç¨‹: æœ‰æ•ˆæ€»åˆ‡æ•° = {reg.intercept_:.4f} + {reg.coef_[0]:.4f} Ã— å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡ + {reg.coef_[1]:.4f} Ã— å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ")
    print(f"RÂ² = {r2:.4f}")
    print(f"è°ƒæ•´åçš„RÂ² = {adj_r2:.4f}")
    print(f"æœ‰æ•ˆæ•°æ®ç‚¹: {len(X_clean)}")
    
    # è¿›è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    from scipy.stats import f
    
    # Fç»Ÿè®¡é‡
    mse = np.mean((y_clean - y_pred) ** 2)
    tss = np.sum((y_clean - np.mean(y_clean)) ** 2)
    f_stat = (r2 / p) / ((1 - r2) / (n - p - 1))
    f_p_value = 1 - f.cdf(f_stat, p, n - p - 1)
    
    print(f"Fç»Ÿè®¡é‡: {f_stat:.4f}")
    print(f"Fæ£€éªŒpå€¼: {f_p_value:.6f}")
    
    if f_p_value < 0.05:
        print("âœ… å›å½’æ¨¡å‹åœ¨Î±=0.05æ°´å¹³ä¸‹æ˜¾è‘—")
    else:
        print("âŒ å›å½’æ¨¡å‹åœ¨Î±=0.05æ°´å¹³ä¸‹ä¸æ˜¾è‘—")
    
    return {
        'model': reg,
        'r2': r2,
        'adj_r2': adj_r2,
        'coefficients': reg.coef_,
        'intercept': reg.intercept_,
        'f_stat': f_stat,
        'f_p_value': f_p_value,
        'n_samples': len(X_clean)
    }

def create_visualizations(combined_df, output_dir):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    os.makedirs(output_dir, exist_ok=True)
    
    target_columns = ['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ', 'æœ‰æ•ˆæ€»åˆ‡æ•°']
    
    # 1. ç›¸å…³ç³»æ•°çƒ­åŠ›å›¾
    plt.figure(figsize=(10, 8))
    correlation_matrix = combined_df[target_columns].corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
                square=True, linewidths=0.5, fmt='.4f')
    plt.title('ä¸‰ä¸ªå˜é‡ä¹‹é—´çš„ç›¸å…³ç³»æ•°çƒ­åŠ›å›¾', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'ç›¸å…³ç³»æ•°çƒ­åŠ›å›¾.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. æ•£ç‚¹å›¾çŸ©é˜µ
    fig, axes = plt.subplots(3, 3, figsize=(15, 15))
    
    for i, col1 in enumerate(target_columns):
        for j, col2 in enumerate(target_columns):
            ax = axes[i, j]
            
            if i == j:
                # å¯¹è§’çº¿ä¸Šç»˜åˆ¶ç›´æ–¹å›¾
                ax.hist(combined_df[col1].dropna(), bins=30, alpha=0.7, color='skyblue')
                ax.set_title(f'{col1} åˆ†å¸ƒ', fontsize=10)
            else:
                # éå¯¹è§’çº¿ç»˜åˆ¶æ•£ç‚¹å›¾
                valid_mask = combined_df[[col1, col2]].notna().all(axis=1)
                if valid_mask.sum() > 0:
                    x_data = combined_df.loc[valid_mask, col2]
                    y_data = combined_df.loc[valid_mask, col1]
                    
                    ax.scatter(x_data, y_data, alpha=0.6, s=20)
                    
                    # è®¡ç®—ç›¸å…³ç³»æ•°
                    if len(x_data) > 1:
                        corr_coef = np.corrcoef(x_data, y_data)[0, 1]
                        ax.set_title(f'r = {corr_coef:.4f}', fontsize=10)
                    
                    # æ·»åŠ è¶‹åŠ¿çº¿
                    if len(x_data) > 1:
                        z = np.polyfit(x_data, y_data, 1)
                        p = np.poly1d(z)
                        ax.plot(x_data, p(x_data), "r--", alpha=0.8)
            
            if i == len(target_columns) - 1:
                ax.set_xlabel(col2, fontsize=9)
            if j == 0:
                ax.set_ylabel(col1, fontsize=9)
    
    plt.suptitle('ä¸‰ä¸ªå˜é‡ä¹‹é—´çš„æ•£ç‚¹å›¾çŸ©é˜µ', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'æ•£ç‚¹å›¾çŸ©é˜µ.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. æ—¶é—´åºåˆ—å›¾
    plt.figure(figsize=(15, 10))
    
    for i, col in enumerate(target_columns):
        plt.subplot(3, 1, i+1)
        
        # ä¸ºæ¯ä¸ªæ—¶é—´æ®µçš„æ•°æ®æ·»åŠ ä¸åŒçš„é¢œè‰²
        data_with_index = combined_df[col].dropna().reset_index(drop=True)
        plt.plot(data_with_index.index, data_with_index.values, marker='o', markersize=2, linewidth=1)
        plt.title(f'{col} æ—¶é—´åºåˆ—', fontsize=12)
        plt.ylabel('æ•°å€¼')
        plt.grid(True, alpha=0.3)
        
        if i == len(target_columns) - 1:
            plt.xlabel('æ•°æ®ç‚¹ç´¢å¼•')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'æ—¶é—´åºåˆ—å›¾.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}")

def save_analysis_results(all_correlations, combined_df, regression_result, output_dir):
    """ä¿å­˜åˆ†æç»“æœåˆ°CSVæ–‡ä»¶"""
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. ä¿å­˜å„æ—¶é—´æ®µçš„ç›¸å…³ç³»æ•°
    correlation_results = []
    for result in all_correlations:
        if result is not None:
            period_name = result['period_name']
            corr_matrix = result['correlation_matrix']
            
            correlation_results.append({
                'æ—¶é—´æ®µ': period_name,
                'æ•°æ®ç‚¹æ•°': result['data_count'],
                'å¤–å¾ªç¯_å°åŒ…æœºå…¥åŒ…æ•°_ç›¸å…³ç³»æ•°': corr_matrix.loc['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ'],
                'å¤–å¾ªç¯_æœ‰æ•ˆæ€»åˆ‡æ•°_ç›¸å…³ç³»æ•°': corr_matrix.loc['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'æœ‰æ•ˆæ€»åˆ‡æ•°'],
                'å°åŒ…æœºå…¥åŒ…æ•°_æœ‰æ•ˆæ€»åˆ‡æ•°_ç›¸å…³ç³»æ•°': corr_matrix.loc['å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ', 'æœ‰æ•ˆæ€»åˆ‡æ•°']
            })
    
    correlation_df = pd.DataFrame(correlation_results)
    correlation_path = os.path.join(output_dir, 'å„æ—¶é—´æ®µç›¸å…³ç³»æ•°åˆ†æ.csv')
    correlation_df.to_csv(correlation_path, index=False, encoding='utf-8-sig')
    print(f"å„æ—¶é—´æ®µç›¸å…³ç³»æ•°åˆ†æå·²ä¿å­˜: {correlation_path}")
    
    # 2. ä¿å­˜æ•´ä½“ç›¸å…³ç³»æ•°çŸ©é˜µ
    target_columns = ['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ', 'æœ‰æ•ˆæ€»åˆ‡æ•°']
    overall_corr = combined_df[target_columns].corr()
    overall_corr_path = os.path.join(output_dir, 'æ•´ä½“ç›¸å…³ç³»æ•°çŸ©é˜µ.csv')
    overall_corr.to_csv(overall_corr_path, encoding='utf-8-sig')
    print(f"æ•´ä½“ç›¸å…³ç³»æ•°çŸ©é˜µå·²ä¿å­˜: {overall_corr_path}")
    
    # 3. ä¿å­˜æè¿°æ€§ç»Ÿè®¡
    descriptive_stats = combined_df[target_columns].describe()
    stats_path = os.path.join(output_dir, 'æè¿°æ€§ç»Ÿè®¡.csv')
    descriptive_stats.to_csv(stats_path, encoding='utf-8-sig')
    print(f"æè¿°æ€§ç»Ÿè®¡å·²ä¿å­˜: {stats_path}")
    
    # 4. ä¿å­˜å›å½’åˆ†æç»“æœ
    if regression_result is not None:
        regression_summary = pd.DataFrame({
            'å‚æ•°': ['æˆªè·', 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡ç³»æ•°', 'å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œç³»æ•°'],
            'æ•°å€¼': [regression_result['intercept']] + list(regression_result['coefficients']),
            'è¯´æ˜': ['å›å½’æ–¹ç¨‹çš„æˆªè·é¡¹', 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡çš„å›å½’ç³»æ•°', 'å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œçš„å›å½’ç³»æ•°']
        })
        
        regression_metrics = pd.DataFrame({
            'æŒ‡æ ‡': ['RÂ²', 'è°ƒæ•´åRÂ²', 'Fç»Ÿè®¡é‡', 'Fæ£€éªŒpå€¼', 'æ ·æœ¬é‡'],
            'æ•°å€¼': [regression_result['r2'], regression_result['adj_r2'], 
                    regression_result['f_stat'], regression_result['f_p_value'], 
                    regression_result['n_samples']],
            'è¯´æ˜': ['å†³å®šç³»æ•°', 'è°ƒæ•´åçš„å†³å®šç³»æ•°', 'Fç»Ÿè®¡é‡', 'Fæ£€éªŒçš„på€¼', 'æœ‰æ•ˆæ ·æœ¬é‡']
        })
        
        # åˆ›å»ºç»¼åˆå›å½’åˆ†æç»“æœ
        comprehensive_regression = pd.DataFrame({
            'é¡¹ç›®': ['å›å½’æ–¹ç¨‹', 'æˆªè·', 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡ç³»æ•°', 'å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œç³»æ•°', 
                    'RÂ²', 'è°ƒæ•´åRÂ²', 'Fç»Ÿè®¡é‡', 'Fæ£€éªŒpå€¼', 'æ ·æœ¬é‡', 'æ¨¡å‹æ˜¾è‘—æ€§'],
            'æ•°å€¼': [
                f"æœ‰æ•ˆæ€»åˆ‡æ•° = {regression_result['intercept']:.4f} + {regression_result['coefficients'][0]:.4f} Ã— å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡ + {regression_result['coefficients'][1]:.4f} Ã— å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ",
                f"{regression_result['intercept']:.4f}",
                f"{regression_result['coefficients'][0]:.4f}",
                f"{regression_result['coefficients'][1]:.4f}",
                f"{regression_result['r2']:.4f}",
                f"{regression_result['adj_r2']:.4f}",
                f"{regression_result['f_stat']:.4f}",
                f"{regression_result['f_p_value']:.6f}",
                f"{regression_result['n_samples']}",
                "æ˜¾è‘—" if regression_result['f_p_value'] < 0.05 else "ä¸æ˜¾è‘—"
            ],
            'è¯´æ˜': [
                'å¤šå…ƒçº¿æ€§å›å½’æ–¹ç¨‹',
                'å›å½’æ–¹ç¨‹çš„æˆªè·é¡¹',
                'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡çš„å›å½’ç³»æ•°',
                'å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œçš„å›å½’ç³»æ•°',
                'å†³å®šç³»æ•°ï¼Œè¡¨ç¤ºæ¨¡å‹è§£é‡Šçš„å˜å¼‚æ¯”ä¾‹',
                'è°ƒæ•´åçš„å†³å®šç³»æ•°',
                'Fç»Ÿè®¡é‡ï¼Œç”¨äºæ£€éªŒæ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§',
                'Fæ£€éªŒçš„på€¼ï¼Œ<0.05è¡¨ç¤ºæ¨¡å‹æ˜¾è‘—',
                'å‚ä¸å›å½’åˆ†æçš„æœ‰æ•ˆæ ·æœ¬é‡',
                'åœ¨Î±=0.05æ°´å¹³ä¸‹çš„æ¨¡å‹æ˜¾è‘—æ€§åˆ¤æ–­'
            ]
        })
        
        # ä¿å­˜CSVæ–‡ä»¶
        regression_summary.to_csv(os.path.join(output_dir, 'å›å½’ç³»æ•°.csv'), index=False, encoding='utf-8-sig')
        regression_metrics.to_csv(os.path.join(output_dir, 'å›å½’æŒ‡æ ‡.csv'), index=False, encoding='utf-8-sig')
        comprehensive_regression.to_csv(os.path.join(output_dir, 'ç»¼åˆå›å½’åˆ†æç»“æœ.csv'), index=False, encoding='utf-8-sig')
        
        # åŒæ—¶ä¿å­˜Excelæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        try:
            regression_path = os.path.join(output_dir, 'å›å½’åˆ†æç»“æœ.xlsx')
            with pd.ExcelWriter(regression_path, engine='openpyxl') as writer:
                regression_summary.to_excel(writer, sheet_name='å›å½’ç³»æ•°', index=False)
                regression_metrics.to_excel(writer, sheet_name='å›å½’æŒ‡æ ‡', index=False)
                comprehensive_regression.to_excel(writer, sheet_name='ç»¼åˆç»“æœ', index=False)
            print(f"å›å½’åˆ†æExcelæ–‡ä»¶å·²ä¿å­˜: {regression_path}")
        except ImportError:
            print("æ³¨æ„ï¼šæœªå®‰è£…openpyxlï¼Œè·³è¿‡Excelæ–‡ä»¶ä¿å­˜")
        
        print(f"å›å½’ç³»æ•°CSVå·²ä¿å­˜: {os.path.join(output_dir, 'å›å½’ç³»æ•°.csv')}")
        print(f"å›å½’æŒ‡æ ‡CSVå·²ä¿å­˜: {os.path.join(output_dir, 'å›å½’æŒ‡æ ‡.csv')}")
        print(f"ç»¼åˆå›å½’åˆ†æç»“æœCSVå·²ä¿å­˜: {os.path.join(output_dir, 'ç»¼åˆå›å½’åˆ†æç»“æœ.csv')}")
    
    # 5. ä¿å­˜å®Œæ•´çš„åˆ†ææ•°æ®
    complete_data_path = os.path.join(output_dir, 'å®Œæ•´åˆ†ææ•°æ®.csv')
    combined_df.to_csv(complete_data_path, index=False, encoding='utf-8-sig')
    print(f"å®Œæ•´åˆ†ææ•°æ®å·²ä¿å­˜: {complete_data_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ä¸‰å˜é‡å…³ç³»ç³»æ•°åˆ†æ")
    print("åˆ†æå˜é‡: å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡ vs å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ vs æœ‰æ•ˆæ€»åˆ‡æ•°")
    print("="*80)
    
    # è¯»å–æ•°æ®æ–‡ä»¶
    time_periods_file = "æŠ˜å æœºæ­£å¸¸è¿è¡Œä¸”é«˜å­˜çº¸ç‡æ—¶é—´æ®µ_æœ€ç»ˆç»“æœ.csv"
    summary_file = "å­˜çº¸æ¶æ•°æ®æ±‡æ€».csv"
    
    try:
        # è¯»å–æ—¶é—´æ®µæ•°æ®
        time_periods_df = pd.read_csv(time_periods_file)
        print(f"æˆåŠŸè¯»å–æ—¶é—´æ®µæ–‡ä»¶ï¼Œå…± {len(time_periods_df)} ä¸ªæ—¶é—´æ®µ")
        
        # è¯»å–æ±‡æ€»æ•°æ®
        summary_df = read_csv_with_encoding(summary_file)
        print(f"æˆåŠŸè¯»å–æ±‡æ€»æ–‡ä»¶ï¼Œå…± {len(summary_df)} è¡Œæ•°æ®")
        
        # å­˜å‚¨æ‰€æœ‰åˆ†æç»“æœ
        all_correlations = []
        all_data = []
        
        # å¤„ç†æ¯ä¸ªæ—¶é—´æ®µ
        for idx, row in time_periods_df.iterrows():
            start_time = pd.to_datetime(row['å¼€å§‹æ—¶é—´'])
            end_time = pd.to_datetime(row['ç»“æŸæ—¶é—´'])
            
            print(f"\nå¤„ç†æ—¶é—´æ®µ {idx+1}/{len(time_periods_df)}: {start_time} åˆ° {end_time}")
            
            # å¤„ç†æ•°æ®
            result = process_data_for_time_period(summary_df, start_time, end_time)
            
            if result is not None:
                data_dict, time_index = result
                
                # åˆ†æç›¸å…³æ€§
                period_name = f"{start_time.strftime('%Y%m%d_%H%M%S')}_{end_time.strftime('%Y%m%d_%H%M%S')}"
                correlation_result = analyze_correlation(data_dict, period_name)
                
                if correlation_result is not None:
                    all_correlations.append(correlation_result)
                    
                    # æ”¶é›†æ•°æ®ç”¨äºæ•´ä½“åˆ†æ
                    period_df = correlation_result['data_frame'].copy()
                    period_df['æ—¶é—´æ®µ'] = period_name
                    all_data.append(period_df)
            else:
                print(f"è·³è¿‡æ—¶é—´æ®µ {idx+1}ï¼Œæ— æ•°æ®")
        
        if not all_correlations:
            print("é”™è¯¯ï¼šæ²¡æœ‰æœ‰æ•ˆçš„ç›¸å…³æ€§åˆ†æç»“æœ")
            return
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)
        print(f"\nåˆå¹¶åçš„æ€»æ•°æ®ç‚¹: {len(combined_df)}")
        
        # è®¡ç®—æ•´ä½“ç›¸å…³ç³»æ•°çŸ©é˜µ
        target_columns = ['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ', 'æœ‰æ•ˆæ€»åˆ‡æ•°']
        overall_correlation = combined_df[target_columns].corr()
        
        print("\n" + "="*80)
        print("æ•´ä½“ç›¸å…³ç³»æ•°çŸ©é˜µ")
        print("="*80)
        print(overall_correlation.round(4))
        
        # è¾“å‡ºå…·ä½“çš„ç›¸å…³ç³»æ•°
        print(f"\nğŸ“Š å…³é”®ç›¸å…³ç³»æ•°:")
        print(f"å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡ vs å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ: {overall_correlation.loc['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ']:.4f}")
        print(f"å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡ vs æœ‰æ•ˆæ€»åˆ‡æ•°: {overall_correlation.loc['å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'æœ‰æ•ˆæ€»åˆ‡æ•°']:.4f}")
        print(f"å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ vs æœ‰æ•ˆæ€»åˆ‡æ•°: {overall_correlation.loc['å°åŒ…æœºå…¥åŒ…æ•°æ€»å’Œ', 'æœ‰æ•ˆæ€»åˆ‡æ•°']:.4f}")
        
        # è¿›è¡Œå›å½’åˆ†æ
        regression_result = perform_regression_analysis(combined_df)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = "å…³ç³»ç³»æ•°åˆ†æç»“æœ"
        
        # ä¿å­˜åˆ†æç»“æœ
        save_analysis_results(all_correlations, combined_df, regression_result, output_dir)
        
        # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        visualization_dir = os.path.join(output_dir, "å¯è§†åŒ–å›¾è¡¨")
        create_visualizations(combined_df, visualization_dir)
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 