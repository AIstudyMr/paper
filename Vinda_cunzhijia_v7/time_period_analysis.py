import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

def moving_average_smooth(data, columns_to_smooth, window_size=10, min_periods=None):
    """
    ç§»åŠ¨å¹³å‡å¹³æ»‘å¤„ç† - å‡å°‘çŸ­æœŸæ³¢åŠ¨
    
    å‚æ•°:
    - data: DataFrame, éœ€è¦å¹³æ»‘çš„æ•°æ®
    - columns_to_smooth: list, éœ€è¦å¹³æ»‘çš„åˆ—ååˆ—è¡¨
    - window_size: int, ç§»åŠ¨çª—å£å¤§å°ï¼Œé»˜è®¤5
    - min_periods: int, è®¡ç®—æ‰€éœ€çš„æœ€å°è§‚æµ‹æ•°ï¼Œé»˜è®¤ä¸ºNoneï¼ˆä½¿ç”¨window_sizeï¼‰
    
    è¿”å›:
    - DataFrame, å¹³æ»‘åçš„æ•°æ®å‰¯æœ¬
    
    ä½¿ç”¨ç¤ºä¾‹:
    smoothed_data = moving_average_smooth(summary_data, 
                                        ['æŠ˜å æœºå®é™…é€Ÿåº¦', 'è£åˆ‡æœºå®é™…é€Ÿåº¦'], 
                                        window_size=10)
    """
    
    print(f"æ‰§è¡Œç§»åŠ¨å¹³å‡å¹³æ»‘å¤„ç†...")
    print(f"çª—å£å¤§å°: {window_size}")
    print(f"å¤„ç†åˆ—æ•°: {len(columns_to_smooth)}")
    
    # åˆ›å»ºæ•°æ®å‰¯æœ¬
    smoothed_data = data.copy()
    
    # å¯¹æŒ‡å®šåˆ—è¿›è¡Œç§»åŠ¨å¹³å‡å¹³æ»‘
    for col in columns_to_smooth:
        if col in smoothed_data.columns:
            # ä¿å­˜åŸå§‹åˆ—ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
            smoothed_data[f'{col}_åŸå§‹'] = smoothed_data[col].copy()
            
            # æ‰§è¡Œç§»åŠ¨å¹³å‡
            smoothed_data[col] = smoothed_data[col].rolling(
                window=window_size, 
                min_periods=min_periods if min_periods else max(1, window_size//2),
                center=True  # å±…ä¸­çª—å£ï¼Œå‡å°‘å»¶è¿Ÿ
            ).mean()
            
            # å¤„ç†è¾¹ç•Œå€¼ï¼ˆå‰åå‡ ä¸ªç‚¹ç”¨åŸå§‹å€¼å¡«å……ï¼‰
            mask = smoothed_data[col].isna()
            smoothed_data.loc[mask, col] = smoothed_data.loc[mask, f'{col}_åŸå§‹']
            
            print(f"  âœ… å·²å¹³æ»‘åˆ—: {col}")
        else:
            print(f"  âš ï¸  åˆ—ä¸å­˜åœ¨: {col}")
    
    print(f"ç§»åŠ¨å¹³å‡å¹³æ»‘å®Œæˆ")
    return smoothed_data


def exponential_smooth(data, columns_to_smooth, alpha=0.3, adjust_outliers=True, outlier_threshold=3):
    """
    æŒ‡æ•°å¹³æ»‘å¤„ç† - ä¿ç•™è¶‹åŠ¿ä¿¡æ¯
    
    å‚æ•°:
    - data: DataFrame, éœ€è¦å¹³æ»‘çš„æ•°æ®
    - columns_to_smooth: list, éœ€è¦å¹³æ»‘çš„åˆ—ååˆ—è¡¨
    - alpha: float, å¹³æ»‘å‚æ•°(0-1)ï¼Œè¶Šå¤§è¶Šæ¥è¿‘åŸå§‹æ•°æ®ï¼Œé»˜è®¤0.3
    - adjust_outliers: bool, æ˜¯å¦å…ˆå¤„ç†å¼‚å¸¸å€¼ï¼Œé»˜è®¤True
    - outlier_threshold: float, å¼‚å¸¸å€¼æ£€æµ‹é˜ˆå€¼ï¼ˆæ ‡å‡†å·®å€æ•°ï¼‰ï¼Œé»˜è®¤3
    
    è¿”å›:
    - DataFrame, å¹³æ»‘åçš„æ•°æ®å‰¯æœ¬
    
    ä½¿ç”¨ç¤ºä¾‹:
    smoothed_data = exponential_smooth(summary_data, 
                                     ['æŠ˜å æœºå®é™…é€Ÿåº¦', 'è£åˆ‡æœºå®é™…é€Ÿåº¦'], 
                                     alpha=0.2)
    """
    
    print(f"æ‰§è¡ŒæŒ‡æ•°å¹³æ»‘å¤„ç†...")
    print(f"å¹³æ»‘å‚æ•°alpha: {alpha}")
    print(f"å¤„ç†åˆ—æ•°: {len(columns_to_smooth)}")
    print(f"å¼‚å¸¸å€¼å¤„ç†: {'å¼€å¯' if adjust_outliers else 'å…³é—­'}")
    
    # åˆ›å»ºæ•°æ®å‰¯æœ¬
    smoothed_data = data.copy()
    
    # å¯¹æŒ‡å®šåˆ—è¿›è¡ŒæŒ‡æ•°å¹³æ»‘
    for col in columns_to_smooth:
        if col in smoothed_data.columns:
            # ä¿å­˜åŸå§‹åˆ—ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
            smoothed_data[f'{col}_åŸå§‹'] = smoothed_data[col].copy()
            
            # è·å–åŸå§‹æ•°æ®
            original_series = smoothed_data[col].copy()
            
            # å¼‚å¸¸å€¼å¤„ç†ï¼ˆå¯é€‰ï¼‰
            if adjust_outliers:
                # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                mean_val = original_series.mean()
                std_val = original_series.std()
                
                # è¯†åˆ«å¼‚å¸¸å€¼
                outlier_mask = np.abs((original_series - mean_val) / std_val) > outlier_threshold
                outlier_count = outlier_mask.sum()
                
                if outlier_count > 0:
                    print(f"  å‘ç°å¼‚å¸¸å€¼: {outlier_count}ä¸ª (åˆ—: {col})")
                    
                    # ç”¨ä¸­ä½æ•°æ›¿æ¢å¼‚å¸¸å€¼
                    median_val = original_series.median()
                    original_series.loc[outlier_mask] = median_val
            
            # æ‰§è¡ŒæŒ‡æ•°å¹³æ»‘
            smoothed_series = original_series.ewm(alpha=alpha, adjust=False).mean()
            
            # æ›´æ–°æ•°æ®
            smoothed_data[col] = smoothed_series
            
            # è®¡ç®—å¹³æ»‘æ•ˆæœç»Ÿè®¡
            original_std = smoothed_data[f'{col}_åŸå§‹'].std()
            smoothed_std = smoothed_series.std()
            noise_reduction = (1 - smoothed_std/original_std) * 100 if original_std > 0 else 0
            
            print(f"  âœ… å·²å¹³æ»‘åˆ—: {col} (å™ªå£°å‡å°‘: {noise_reduction:.1f}%)")
        else:
            print(f"  âš ï¸  åˆ—ä¸å­˜åœ¨: {col}")
    
    print(f"æŒ‡æ•°å¹³æ»‘å®Œæˆ")
    return smoothed_data


def compare_smoothing_methods(data, columns_to_compare, window_size=10, alpha=0.3):
    """
    å¯¹æ¯”ä¸¤ç§å¹³æ»‘æ–¹æ³•çš„æ•ˆæœ
    
    å‚æ•°:
    - data: DataFrame, åŸå§‹æ•°æ®
    - columns_to_compare: list, éœ€è¦å¯¹æ¯”çš„åˆ—ååˆ—è¡¨
    - window_size: int, ç§»åŠ¨å¹³å‡çª—å£å¤§å°
    - alpha: float, æŒ‡æ•°å¹³æ»‘å‚æ•°
    
    è¿”å›:
    - dict, åŒ…å«å¯¹æ¯”ç»“æœçš„å­—å…¸
    """
    
    print(f"\n{'='*50}")
    print(f"å¹³æ»‘æ–¹æ³•æ•ˆæœå¯¹æ¯”")
    print(f"{'='*50}")
    
    # æ‰§è¡Œä¸¤ç§å¹³æ»‘
    ma_data = moving_average_smooth(data, columns_to_compare, window_size)
    exp_data = exponential_smooth(data, columns_to_compare, alpha)
    
    comparison_results = {}
    
    for col in columns_to_compare:
        if col in data.columns:
            original = data[col].dropna()
            ma_smoothed = ma_data[col].dropna()
            exp_smoothed = exp_data[col].dropna()
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            original_std = original.std()
            ma_std = ma_smoothed.std()
            exp_std = exp_smoothed.std()
            
            ma_noise_reduction = (1 - ma_std/original_std) * 100 if original_std > 0 else 0
            exp_noise_reduction = (1 - exp_std/original_std) * 100 if original_std > 0 else 0
            
            # è®¡ç®—ä¸åŸå§‹æ•°æ®çš„ç›¸å…³æ€§ï¼ˆä¿ç•™è¶‹åŠ¿èƒ½åŠ›ï¼‰
            ma_correlation = np.corrcoef(original[:len(ma_smoothed)], ma_smoothed)[0,1]
            exp_correlation = np.corrcoef(original[:len(exp_smoothed)], exp_smoothed)[0,1]
            
            comparison_results[col] = {
                'åŸå§‹æ ‡å‡†å·®': original_std,
                'ç§»åŠ¨å¹³å‡æ ‡å‡†å·®': ma_std,
                'æŒ‡æ•°å¹³æ»‘æ ‡å‡†å·®': exp_std,
                'ç§»åŠ¨å¹³å‡å™ªå£°å‡å°‘(%)': ma_noise_reduction,
                'æŒ‡æ•°å¹³æ»‘å™ªå£°å‡å°‘(%)': exp_noise_reduction,
                'ç§»åŠ¨å¹³å‡ç›¸å…³æ€§': ma_correlation,
                'æŒ‡æ•°å¹³æ»‘ç›¸å…³æ€§': exp_correlation
            }
            
            print(f"\nåˆ—: {col}")
            print(f"  ç§»åŠ¨å¹³å‡: å™ªå£°å‡å°‘ {ma_noise_reduction:.1f}%, ç›¸å…³æ€§ {ma_correlation:.4f}")
            print(f"  æŒ‡æ•°å¹³æ»‘: å™ªå£°å‡å°‘ {exp_noise_reduction:.1f}%, ç›¸å…³æ€§ {exp_correlation:.4f}")
            
            # ç»™å‡ºæ¨è
            if ma_noise_reduction > exp_noise_reduction and ma_correlation > 0.9:
                print(f"  æ¨è: ç§»åŠ¨å¹³å‡ (æ›´å¥½çš„å™ªå£°å‡å°‘)")
            elif exp_correlation > ma_correlation and exp_noise_reduction > ma_noise_reduction * 0.8:
                print(f"  æ¨è: æŒ‡æ•°å¹³æ»‘ (æ›´å¥½çš„è¶‹åŠ¿ä¿æŒ)")
            else:
                print(f"  æ¨è: æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©")
    
    return comparison_results

def analyze_by_time_periods():
    """æŒ‰ç…§è°ƒæ•´å.csvçš„æ—¶é—´æ®µè¿›è¡Œå¹¶è¡Œæµç¨‹åˆ†æ"""
    
    # è¯»å–æ—¶é—´æ®µæ•°æ®
    try:
        time_periods = pd.read_csv('æŠ˜å æœºæ­£å¸¸è¿è¡Œä¸”é«˜å­˜çº¸ç‡æ—¶é—´æ®µ_æœ€ç»ˆç»“æœ_å­˜çº¸ç‡1.csv')
        print(f"åŠ è½½æ—¶é—´æ®µæ•°æ®ï¼š{len(time_periods)}ä¸ªæ—¶é—´æ®µ")
        
        # è½¬æ¢æ—¶é—´æ ¼å¼
        time_periods['å¼€å§‹æ—¶é—´'] = pd.to_datetime(time_periods['å¼€å§‹æ—¶é—´'])
        time_periods['ç»“æŸæ—¶é—´'] = pd.to_datetime(time_periods['ç»“æŸæ—¶é—´'])
        
    except Exception as e:
        print(f"è¯»å–æ—¶é—´æ®µæ•°æ®å‡ºé”™ï¼š{e}")
        return
    
    # è¯»å–æ±‡æ€»æ•°æ®
    try:
        summary_data = pd.read_csv('å­˜çº¸æ¶æ•°æ®æ±‡æ€».csv', encoding='utf-8-sig')
        print(f"åŠ è½½æ±‡æ€»æ•°æ®ï¼š{len(summary_data)}æ¡è®°å½•")
        
        # è½¬æ¢æ—¶é—´æ ¼å¼
        summary_data['æ—¶é—´'] = pd.to_datetime(summary_data['æ—¶é—´'])
        summary_data = summary_data.sort_values('æ—¶é—´')
        
        # æ•°æ®å¹³æ»‘å¤„ç† - äºŒé€‰ä¸€ä½¿ç”¨
        # æ–¹æ³•1ï¼šç§»åŠ¨å¹³å‡å¹³æ»‘ï¼ˆå‡å°‘çŸ­æœŸæ³¢åŠ¨ï¼‰
        summary_data = moving_average_smooth(summary_data, 
                                           ['æŠ˜å æœºå…¥åŒ…æ•°', 'æŠ˜å æœºå‡ºåŒ…æ•°','å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡','è¿›ç¬¬ä¸€è£åˆ‡é€šé“çº¸æ¡è®¡æ•°',
                                            'è¿›ç¬¬äºŒè£åˆ‡é€šé“çº¸æ¡è®¡æ•°','è¿›ç¬¬ä¸‰è£åˆ‡é€šé“çº¸æ¡è®¡æ•°','è¿›ç¬¬å››è£åˆ‡é€šé“çº¸æ¡è®¡æ•°',
                                            '1#æœ‰æ•ˆåˆ‡æ•°','2#æœ‰æ•ˆåˆ‡æ•°','3#æœ‰æ•ˆåˆ‡æ•°','4#æœ‰æ•ˆåˆ‡æ•°','1#å°åŒ…æœºå…¥åŒ…æ•°',
                                            '2#å°åŒ…æœºå…¥åŒ…æ•°','3#å°åŒ…æœºå…¥åŒ…æ•°','4#å°åŒ…æœºå…¥åŒ…æ•°'], 
                                           window_size=10)
        
        # æ–¹æ³•2ï¼šæŒ‡æ•°å¹³æ»‘ï¼ˆä¿ç•™è¶‹åŠ¿ä¿¡æ¯ï¼‰
        
        # summary_data = exponential_smooth(summary_data, 
        #                                 ['æŠ˜å æœºå…¥åŒ…æ•°', 'æŠ˜å æœºå‡ºåŒ…æ•°','å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡','è¿›ç¬¬ä¸€è£åˆ‡é€šé“çº¸æ¡è®¡æ•°',
        #                                     'è¿›ç¬¬äºŒè£åˆ‡é€šé“çº¸æ¡è®¡æ•°','è¿›ç¬¬ä¸‰è£åˆ‡é€šé“çº¸æ¡è®¡æ•°','è¿›ç¬¬å››è£åˆ‡é€šé“çº¸æ¡è®¡æ•°',
        #                                     '1#æœ‰æ•ˆåˆ‡æ•°','2#æœ‰æ•ˆåˆ‡æ•°','3#æœ‰æ•ˆåˆ‡æ•°','4#æœ‰æ•ˆåˆ‡æ•°','1#å°åŒ…æœºå…¥åŒ…æ•°',
        #                                     '2#å°åŒ…æœºå…¥åŒ…æ•°','3#å°åŒ…æœºå…¥åŒ…æ•°','4#å°åŒ…æœºå…¥åŒ…æ•°'], 
        #                                 alpha=0.3)
        
    except Exception as e:
        print(f"è¯»å–æ±‡æ€»æ•°æ®å‡ºé”™ï¼š{e}")
        return
    
    # å®šä¹‰å¹¶è¡Œæµç¨‹ç»“æ„
    process_structure = define_process_structure()
    
    # å¯¹æ¯ä¸ªæ—¶é—´æ®µè¿›è¡Œåˆ†æ
    period_results = []
    all_period_data = []
    
    print(f"\n=== å¼€å§‹åˆ†æ {len(time_periods)} ä¸ªæ—¶é—´æ®µ ===")
    
    for index, period in time_periods.iterrows():
        period_id = f"æ—¶é—´æ®µ{index+1:02d}"
        start_time = period['å¼€å§‹æ—¶é—´']
        end_time = period['ç»“æŸæ—¶é—´']
        duration = period['æŒç»­æ—¶é—´']
        
        print(f"\n{'='*50}")
        print(f"åˆ†æ {period_id}: {start_time} ~ {end_time}")
        print(f"æŒç»­æ—¶é—´: {duration}")
        
        # ç­›é€‰è¯¥æ—¶é—´æ®µçš„æ•°æ®
        period_mask = (summary_data['æ—¶é—´'] >= start_time) & (summary_data['æ—¶é—´'] <= end_time)
        period_data = summary_data[period_mask].copy()
        
        if len(period_data) < 10:  # æ•°æ®é‡å¤ªå°‘
            print(f"{period_id} æ•°æ®é‡ä¸è¶³ ({len(period_data)}æ¡)ï¼Œè·³è¿‡åˆ†æ")
            continue
        
        print(f"è¯¥æ—¶é—´æ®µæ•°æ®é‡: {len(period_data)}æ¡")
        
        # åˆ†æè¯¥æ—¶é—´æ®µçš„æµç¨‹å»¶æ—¶
        period_result = analyze_single_period(period_data, period_id, start_time, end_time, process_structure)
        
        if period_result:
            period_results.append(period_result)
            all_period_data.extend(period_result['è¯¦ç»†æ•°æ®'])
    
    if period_results:
        # æ±‡æ€»åˆ†ææ‰€æœ‰æ—¶é—´æ®µ
        comprehensive_period_analysis(period_results, all_period_data)
    else:
        print("æ²¡æœ‰æˆåŠŸåˆ†æçš„æ—¶é—´æ®µ")

def define_process_structure():
    """å®šä¹‰å¹¶è¡Œæµç¨‹ç»“æ„"""
    
    # å…±åŒå‰ç½®æµç¨‹
    common_sequence = [
        ('1_æŠ˜å æœºå…¥åŒ…æ•°', 'æŠ˜å æœºå…¥åŒ…æ•°'),
        ('2_æŠ˜å æœºå‡ºåŒ…æ•°', 'æŠ˜å æœºå‡ºåŒ…æ•°'),
        ('3_å¤–å¾ªç¯è¿›å†…å¾ªç¯', 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡')
    ]
    
    # å››æ¡å¹¶è¡Œç”Ÿäº§çº¿
    parallel_lines = {
        'ç”Ÿäº§çº¿1': [
            ('4_ç¬¬ä¸€è£åˆ‡é€šé“', 'è¿›ç¬¬ä¸€è£åˆ‡é€šé“çº¸æ¡è®¡æ•°'),
            ('5_1å·æœ‰æ•ˆåˆ‡æ•°', '1#æœ‰æ•ˆåˆ‡æ•°'),
            ('6_1å·å°åŒ…æœºå…¥åŒ…æ•°', '1#å°åŒ…æœºå…¥åŒ…æ•°'),
        ],
        'ç”Ÿäº§çº¿2': [
            ('4_ç¬¬äºŒè£åˆ‡é€šé“', 'è¿›ç¬¬äºŒè£åˆ‡é€šé“çº¸æ¡è®¡æ•°'),
            ('5_2å·æœ‰æ•ˆåˆ‡æ•°', '2#æœ‰æ•ˆåˆ‡æ•°'),
            ('6_2å·å°åŒ…æœºå…¥åŒ…æ•°', '2#å°åŒ…æœºå…¥åŒ…æ•°'),
        ],
        'ç”Ÿäº§çº¿3': [
            ('4_ç¬¬ä¸‰è£åˆ‡é€šé“', 'è¿›ç¬¬ä¸‰è£åˆ‡é€šé“çº¸æ¡è®¡æ•°'),
            ('5_3å·æœ‰æ•ˆåˆ‡æ•°', '3#æœ‰æ•ˆåˆ‡æ•°'),
            ('6_3å·å°åŒ…æœºå…¥åŒ…æ•°', '3#å°åŒ…æœºå…¥åŒ…æ•°'),
        ],
        'ç”Ÿäº§çº¿4': [
            ('4_ç¬¬å››è£åˆ‡é€šé“', 'è¿›ç¬¬å››è£åˆ‡é€šé“çº¸æ¡è®¡æ•°'),
            ('5_4å·æœ‰æ•ˆåˆ‡æ•°', '4#æœ‰æ•ˆåˆ‡æ•°'),
            ('6_4å·å°åŒ…æœºå…¥åŒ…æ•°', '4#å°åŒ…æœºå…¥åŒ…æ•°'),
        ]
    }
    
    return {
        'common_sequence': common_sequence,
        'parallel_lines': parallel_lines
    }

def analyze_single_period(data, period_id, start_time, end_time, process_structure):
    """åˆ†æå•ä¸ªæ—¶é—´æ®µçš„æµç¨‹å»¶æ—¶"""
    
    common_sequence = process_structure['common_sequence']
    parallel_lines = process_structure['parallel_lines']
    
    period_result = {
        'æ—¶é—´æ®µID': period_id,
        'å¼€å§‹æ—¶é—´': start_time,
        'ç»“æŸæ—¶é—´': end_time,
        'æ•°æ®é‡': len(data),
        'å…±åŒæµç¨‹': None,
        'ç”Ÿäº§çº¿ç»“æœ': {},
        'è¯¦ç»†æ•°æ®': []
    }
    
    # åˆ†æå…±åŒå‰ç½®æµç¨‹
    common_results = analyze_sequence(data, common_sequence, f"{period_id}_å…±åŒæµç¨‹")
    if common_results:
        period_result['å…±åŒæµç¨‹'] = common_results
        for result in common_results:
            result['æ—¶é—´æ®µID'] = period_id
            result['å¼€å§‹æ—¶é—´'] = start_time
            result['ç»“æŸæ—¶é—´'] = end_time
        period_result['è¯¦ç»†æ•°æ®'].extend(common_results)
    
    # åˆ†æå„æ¡å¹¶è¡Œç”Ÿäº§çº¿
    for line_name, line_sequence in parallel_lines.items():
        # å°†å¤–å¾ªç¯è¿›å†…å¾ªç¯ä½œä¸ºèµ·ç‚¹è¿æ¥åˆ°å„ç”Ÿäº§çº¿
        full_sequence = [common_sequence[-1]] + line_sequence
        line_results = analyze_sequence(data, full_sequence, f"{period_id}_{line_name}")
        
        if line_results:
            period_result['ç”Ÿäº§çº¿ç»“æœ'][line_name] = line_results
            for result in line_results:
                result['æ—¶é—´æ®µID'] = period_id
                result['å¼€å§‹æ—¶é—´'] = start_time
                result['ç»“æŸæ—¶é—´'] = end_time
            period_result['è¯¦ç»†æ•°æ®'].extend(line_results)
    
    # è®¡ç®—è¯¥æ—¶é—´æ®µçš„ç”Ÿäº§çº¿æ€§èƒ½æŒ‡æ ‡
    line_performance = {}
    for line_name, line_results in period_result['ç”Ÿäº§çº¿ç»“æœ'].items():
        if line_results:
            total_avg_delay = sum([r['å¹³å‡å»¶æ—¶(ç§’)'] for r in line_results])
            total_median_delay = sum([r['ä¸­ä½å»¶æ—¶(ç§’)'] for r in line_results])
            avg_stability = np.mean([r['å¹³å‡å»¶æ—¶(ç§’)']/r['æ ‡å‡†å·®(ç§’)'] if r['æ ‡å‡†å·®(ç§’)'] > 0 else 0 for r in line_results])
            
            line_performance[line_name] = {
                'ç¯èŠ‚æ•°': len(line_results),
                'æ€»å¹³å‡å»¶æ—¶(ç§’)': total_avg_delay,
                'æ€»ä¸­ä½å»¶æ—¶(ç§’)': total_median_delay,
                'å¹³å‡å•ç¯èŠ‚å»¶æ—¶(ç§’)': total_avg_delay / len(line_results),
                'å¹³å‡ç¨³å®šæ€§': avg_stability,
                'æœ€å¤§å•ç¯èŠ‚å»¶æ—¶(ç§’)': max([r['å¹³å‡å»¶æ—¶(ç§’)'] for r in line_results])
            }
    
    period_result['ç”Ÿäº§çº¿æ€§èƒ½'] = line_performance
    
    # æ˜¾ç¤ºè¯¥æ—¶é—´æ®µçš„å…³é”®æŒ‡æ ‡
    if line_performance:
        print(f"\n{period_id} ç”Ÿäº§çº¿æ€§èƒ½:")
        for line_name, performance in line_performance.items():
            print(f"  {line_name}: æ€»å»¶æ—¶ {performance['æ€»å¹³å‡å»¶æ—¶(ç§’)']:.1f}ç§’")
        
        # æ‰¾å‡ºæœ€ä¼˜å’Œæœ€å·®ç”Ÿäº§çº¿
        best_line = min(line_performance.items(), key=lambda x: x[1]['æ€»å¹³å‡å»¶æ—¶(ç§’)'])
        worst_line = max(line_performance.items(), key=lambda x: x[1]['æ€»å¹³å‡å»¶æ—¶(ç§’)'])
        
        print(f"  æœ€ä¼˜: {best_line[0]} ({best_line[1]['æ€»å¹³å‡å»¶æ—¶(ç§’)']:.1f}ç§’)")
        print(f"  æœ€å·®: {worst_line[0]} ({worst_line[1]['æ€»å¹³å‡å»¶æ—¶(ç§’)']:.1f}ç§’)")
        print(f"  å·®å¼‚: {worst_line[1]['æ€»å¹³å‡å»¶æ—¶(ç§’)'] - best_line[1]['æ€»å¹³å‡å»¶æ—¶(ç§’)']:.1f}ç§’")
    
    return period_result

def analyze_sequence(data, sequence, sequence_name):
    """åˆ†æå•ä¸ªåºåˆ—çš„ä¼ è¾“å»¶æ—¶"""
    
    # æ£€æŸ¥æ•°æ®ä¸­å­˜åœ¨çš„åˆ—
    available_points = []
    
    for point_name, column_name in sequence:
        if column_name in data.columns:
            available_points.append((point_name, column_name))
    
    if len(available_points) < 2:
        return None
    
    # è®¡ç®—ç›¸é‚»ç‚¹ä½ä¹‹é—´çš„å»¶æ—¶
    delays_results = []
    
    for i in range(len(available_points) - 1):
        current_point = available_points[i]
        next_point = available_points[i + 1]
        
        current_name, current_col = current_point
        next_name, next_col = next_point
        
        # æ‰¾å‡ºæ•°æ®å˜åŒ–ç‚¹
        current_changes = find_change_points(data, current_col)
        next_changes = find_change_points(data, next_col)
        
        if len(current_changes) > 0 and len(next_changes) > 0:
            # è®¡ç®—ä¼ è¾“å»¶æ—¶
            delays = calculate_nearest_delays(current_changes, next_changes)
            
            if len(delays) > 0:
                delay_stats = {
                    'ç”Ÿäº§çº¿': sequence_name,
                    'èµ·å§‹ç‚¹ä½': current_name,
                    'ç›®æ ‡ç‚¹ä½': next_name,
                    'èµ·å§‹åˆ—å': current_col,
                    'ç›®æ ‡åˆ—å': next_col,
                    'å¹³å‡å»¶æ—¶(ç§’)': np.mean(delays),
                    'ä¸­ä½å»¶æ—¶(ç§’)': np.median(delays),
                    'æœ€å°å»¶æ—¶(ç§’)': np.min(delays),
                    'æœ€å¤§å»¶æ—¶(ç§’)': np.max(delays),
                    'æ ‡å‡†å·®(ç§’)': np.std(delays),
                    'æ ·æœ¬æ•°': len(delays)
                }
                
                delays_results.append(delay_stats)
    
    return delays_results

def comprehensive_period_analysis(period_results, all_period_data):
    """å¯¹æ‰€æœ‰æ—¶é—´æ®µè¿›è¡Œç»¼åˆåˆ†æ"""
    
    print(f"\n{'='*60}")
    print(f"=== æ—¶é—´æ®µç»¼åˆåˆ†æ (å…±{len(period_results)}ä¸ªæ—¶é—´æ®µ) ===")
    print(f"{'='*60}")
    
    # ä¿å­˜æ‰€æœ‰è¯¦ç»†æ•°æ®
    if all_period_data:
        all_df = pd.DataFrame(all_period_data)
        all_df.to_csv('æ—¶é—´æ®µå¹¶è¡Œæµç¨‹åˆ†æ_è¯¦ç»†æ•°æ®_åˆ†æ®µ_1.csv', index=False, encoding='utf-8-sig', float_format='%.2f')
        print(f"æ‰€æœ‰æ—¶é—´æ®µè¯¦ç»†æ•°æ®å·²ä¿å­˜ï¼šæ—¶é—´æ®µå¹¶è¡Œæµç¨‹åˆ†æ_è¯¦ç»†æ•°æ®_åˆ†æ®µ_1.csv")
    
    # åˆ›å»ºæ—¶é—´æ®µæ€§èƒ½æ±‡æ€»
    period_summary = []
    
    for period_result in period_results:
        period_id = period_result['æ—¶é—´æ®µID']
        start_time = period_result['å¼€å§‹æ—¶é—´']
        line_performance = period_result['ç”Ÿäº§çº¿æ€§èƒ½']
        
        if line_performance:
            summary_row = {
                'æ—¶é—´æ®µID': period_id,
                'å¼€å§‹æ—¶é—´': start_time,
                'æ•°æ®é‡': period_result['æ•°æ®é‡']
            }
            
            # æ·»åŠ å„ç”Ÿäº§çº¿çš„æ€»å»¶æ—¶
            for line_name, performance in line_performance.items():
                summary_row[f'{line_name}_æ€»å»¶æ—¶'] = performance['æ€»å¹³å‡å»¶æ—¶(ç§’)']
                summary_row[f'{line_name}_ç¨³å®šæ€§'] = performance['å¹³å‡ç¨³å®šæ€§']
            
            # è®¡ç®—è¯¥æ—¶é—´æ®µçš„æ•´ä½“æŒ‡æ ‡
            delays = [perf['æ€»å¹³å‡å»¶æ—¶(ç§’)'] for perf in line_performance.values()]
            summary_row['å¹³å‡å»¶æ—¶'] = np.mean(delays)
            summary_row['æœ€å¤§å»¶æ—¶'] = np.max(delays)
            summary_row['æœ€å°å»¶æ—¶'] = np.min(delays)
            summary_row['å»¶æ—¶å·®å¼‚'] = np.max(delays) - np.min(delays)
            
            period_summary.append(summary_row)
    
    if period_summary:
        summary_df = pd.DataFrame(period_summary)
        summary_df.to_csv('æ—¶é—´æ®µæ€§èƒ½æ±‡æ€»_åˆ†æ®µ_1.csv', index=False, encoding='utf-8-sig', float_format='%.2f')
        print(f"æ—¶é—´æ®µæ€§èƒ½æ±‡æ€»å·²ä¿å­˜ï¼šæ—¶é—´æ®µæ€§èƒ½æ±‡æ€»_åˆ†æ®µ_1.csv")
        
        # åˆ†ææ—¶é—´æ®µæ€§èƒ½è¶‹åŠ¿
        analyze_period_trends(summary_df)
        
        # è¯†åˆ«æœ€ä¼˜å’Œæœ€å·®æ—¶é—´æ®µ
        identify_best_worst_periods(summary_df, period_results)
        
        # åˆ›å»ºå¯è§†åŒ–
        create_period_visualization(summary_df, period_results)
        
        # åˆ›å»ºæµç¨‹æ±‡æ€»ç»Ÿè®¡
        create_process_summary(all_period_data)
        
        # åˆ›å»ºå»¶æ—¶æ‹Ÿåˆæ–¹ç¨‹åˆ†æ
        create_delay_fitting_equations(all_period_data)

def analyze_period_trends(summary_df):
    """åˆ†ææ—¶é—´æ®µæ€§èƒ½è¶‹åŠ¿"""
    
    print(f"\n=== æ—¶é—´æ®µæ€§èƒ½è¶‹åŠ¿åˆ†æ ===")
    
    # æ•´ä½“æ€§èƒ½ç»Ÿè®¡
    print(f"å¹³å‡å»¶æ—¶ç»Ÿè®¡:")
    print(f"  æ•´ä½“å¹³å‡: {summary_df['å¹³å‡å»¶æ—¶'].mean():.2f}ç§’")
    print(f"  æœ€å¥½æ—¶æ®µ: {summary_df['å¹³å‡å»¶æ—¶'].min():.2f}ç§’")
    print(f"  æœ€å·®æ—¶æ®µ: {summary_df['å¹³å‡å»¶æ—¶'].max():.2f}ç§’")
    print(f"  æ ‡å‡†å·®: {summary_df['å¹³å‡å»¶æ—¶'].std():.2f}ç§’")
    
    # ç”Ÿäº§çº¿ç¨³å®šæ€§åˆ†æ
    line_columns = [col for col in summary_df.columns if '_æ€»å»¶æ—¶' in col]
    
    print(f"\nå„ç”Ÿäº§çº¿åœ¨ä¸åŒæ—¶é—´æ®µçš„è¡¨ç°:")
    for col in line_columns:
        line_name = col.replace('_æ€»å»¶æ—¶', '')
        line_data = summary_df[col].dropna()
        if len(line_data) > 0:
            print(f"  {line_name}:")
            print(f"    å¹³å‡å»¶æ—¶: {line_data.mean():.2f}ç§’")
            print(f"    å˜å¼‚ç³»æ•°: {(line_data.std()/line_data.mean()*100):.1f}%")
            print(f"    æœ€ä½³è¡¨ç°: {line_data.min():.2f}ç§’")
            print(f"    æœ€å·®è¡¨ç°: {line_data.max():.2f}ç§’")

def identify_best_worst_periods(summary_df, period_results):
    """è¯†åˆ«æœ€ä¼˜å’Œæœ€å·®æ—¶é—´æ®µ"""
    
    print(f"\n=== æœ€ä¼˜/æœ€å·®æ—¶é—´æ®µè¯†åˆ« ===")
    
    # æŒ‰å¹³å‡å»¶æ—¶æ’åº
    best_period_idx = summary_df['å¹³å‡å»¶æ—¶'].idxmin()
    worst_period_idx = summary_df['å¹³å‡å»¶æ—¶'].idxmax()
    
    best_period = summary_df.iloc[best_period_idx]
    worst_period = summary_df.iloc[worst_period_idx]
    
    print(f"æœ€ä¼˜æ—¶é—´æ®µ: {best_period['æ—¶é—´æ®µID']}")
    print(f"  å¼€å§‹æ—¶é—´: {best_period['å¼€å§‹æ—¶é—´']}")
    print(f"  å¹³å‡å»¶æ—¶: {best_period['å¹³å‡å»¶æ—¶']:.2f}ç§’")
    print(f"  å»¶æ—¶å·®å¼‚: {best_period['å»¶æ—¶å·®å¼‚']:.2f}ç§’")
    
    print(f"\næœ€å·®æ—¶é—´æ®µ: {worst_period['æ—¶é—´æ®µID']}")
    print(f"  å¼€å§‹æ—¶é—´: {worst_period['å¼€å§‹æ—¶é—´']}")
    print(f"  å¹³å‡å»¶æ—¶: {worst_period['å¹³å‡å»¶æ—¶']:.2f}ç§’")
    print(f"  å»¶æ—¶å·®å¼‚: {worst_period['å»¶æ—¶å·®å¼‚']:.2f}ç§’")
    
    # åˆ†ææ€§èƒ½å·®å¼‚åŸå› 
    performance_gap = worst_period['å¹³å‡å»¶æ—¶'] - best_period['å¹³å‡å»¶æ—¶']
    print(f"\næ€§èƒ½å·®å¼‚: {performance_gap:.2f}ç§’ ({performance_gap/best_period['å¹³å‡å»¶æ—¶']*100:.1f}%)")
    
    # æ‰¾å‡ºä¸»è¦å·®å¼‚æ¥æº
    line_columns = [col for col in summary_df.columns if '_æ€»å»¶æ—¶' in col]
    print(f"\nä¸»è¦å·®å¼‚æ¥æº:")
    for col in line_columns:
        line_name = col.replace('_æ€»å»¶æ—¶', '')
        if not pd.isna(best_period[col]) and not pd.isna(worst_period[col]):
            diff = worst_period[col] - best_period[col]
            print(f"  {line_name}: +{diff:.2f}ç§’")

def create_period_visualization(summary_df, period_results):
    """åˆ›å»ºæ—¶é—´æ®µåˆ†æå¯è§†åŒ–"""
    
    plt.figure(figsize=(20, 15))
    
    # å­å›¾1: æ—¶é—´æ®µå¹³å‡å»¶æ—¶è¶‹åŠ¿
    plt.subplot(3, 3, 1)
    plt.plot(range(len(summary_df)), summary_df['å¹³å‡å»¶æ—¶'], 'bo-', alpha=0.7)
    plt.title('å„æ—¶é—´æ®µå¹³å‡å»¶æ—¶è¶‹åŠ¿')
    plt.xlabel('æ—¶é—´æ®µåºå·')
    plt.ylabel('å¹³å‡å»¶æ—¶(ç§’)')
    plt.grid(True, alpha=0.3)
    
    # å­å›¾2: å„ç”Ÿäº§çº¿å»¶æ—¶å¯¹æ¯”ç®±çº¿å›¾
    plt.subplot(3, 3, 2)
    line_columns = [col for col in summary_df.columns if '_æ€»å»¶æ—¶' in col]
    line_data = []
    line_labels = []
    for col in line_columns:
        data = summary_df[col].dropna()
        if len(data) > 0:
            line_data.append(data)
            line_labels.append(col.replace('_æ€»å»¶æ—¶', ''))
    
    if line_data:
        plt.boxplot(line_data, labels=line_labels)
        plt.title('å„ç”Ÿäº§çº¿å»¶æ—¶åˆ†å¸ƒ')
        plt.ylabel('æ€»å»¶æ—¶(ç§’)')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
    
    # å­å›¾3: å»¶æ—¶å·®å¼‚è¶‹åŠ¿
    plt.subplot(3, 3, 3)
    plt.plot(range(len(summary_df)), summary_df['å»¶æ—¶å·®å¼‚'], 'ro-', alpha=0.7)
    plt.title('å„æ—¶é—´æ®µç”Ÿäº§çº¿å·®å¼‚')
    plt.xlabel('æ—¶é—´æ®µåºå·')
    plt.ylabel('æœ€å¤§-æœ€å°å»¶æ—¶(ç§’)')
    plt.grid(True, alpha=0.3)
    
    # å­å›¾4-6: å„ç”Ÿäº§çº¿éšæ—¶é—´å˜åŒ–
    for i, col in enumerate(line_columns[:3]):
        plt.subplot(3, 3, 4+i)
        line_name = col.replace('_æ€»å»¶æ—¶', '')
        plt.plot(range(len(summary_df)), summary_df[col], 'o-', alpha=0.7)
        plt.title(f'{line_name}å»¶æ—¶å˜åŒ–')
        plt.xlabel('æ—¶é—´æ®µåºå·')
        plt.ylabel('æ€»å»¶æ—¶(ç§’)')
        plt.grid(True, alpha=0.3)
    
    # å­å›¾7: æ•°æ®é‡åˆ†å¸ƒ
    plt.subplot(3, 3, 7)
    plt.bar(range(len(summary_df)), summary_df['æ•°æ®é‡'], alpha=0.7)
    plt.title('å„æ—¶é—´æ®µæ•°æ®é‡')
    plt.xlabel('æ—¶é—´æ®µåºå·')
    plt.ylabel('æ•°æ®æ¡æ•°')
    plt.grid(True, alpha=0.3)
    
    # å­å›¾8: æ€§èƒ½çƒ­åŠ›å›¾
    plt.subplot(3, 3, 8)
    if line_columns:
        heatmap_data = summary_df[line_columns].T
        plt.imshow(heatmap_data, cmap='YlOrRd', aspect='auto')
        plt.colorbar(label='å»¶æ—¶(ç§’)')
        plt.yticks(range(len(line_columns)), [col.replace('_æ€»å»¶æ—¶', '') for col in line_columns])
        plt.xlabel('æ—¶é—´æ®µåºå·')
        plt.title('ç”Ÿäº§çº¿æ€§èƒ½çƒ­åŠ›å›¾')
    
    # å­å›¾9: ç¨³å®šæ€§åˆ†æ
    plt.subplot(3, 3, 9)
    stability_columns = [col for col in summary_df.columns if '_ç¨³å®šæ€§' in col]
    if stability_columns:
        for col in stability_columns:
            line_name = col.replace('_ç¨³å®šæ€§', '')
            plt.plot(range(len(summary_df)), summary_df[col], 'o-', alpha=0.7, label=line_name)
        plt.title('å„ç”Ÿäº§çº¿ç¨³å®šæ€§å˜åŒ–')
        plt.xlabel('æ—¶é—´æ®µåºå·')
        plt.ylabel('ç¨³å®šæ€§æŒ‡æ•°')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('æ—¶é—´æ®µå¹¶è¡Œæµç¨‹åˆ†æ_åˆ†æ®µ_1.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"\næ—¶é—´æ®µåˆ†æå¯è§†åŒ–å·²ä¿å­˜ï¼šæ—¶é—´æ®µå¹¶è¡Œæµç¨‹åˆ†æ_åˆ†æ®µ_1.png")

def find_change_points(data, column):
    """æ‰¾å‡ºæ•°æ®å˜åŒ–ç‚¹çš„æ—¶é—´"""
    if column not in data.columns:
        return np.array([])
    
    changes = data[column].diff().fillna(0)
    change_mask = (changes != 0) & (~changes.isna())
    change_times = data.loc[change_mask, 'æ—¶é—´'].values
    return change_times

def calculate_nearest_delays(source_times, target_times):
    """è®¡ç®—æœ€è¿‘é‚»å»¶æ—¶"""
    delays = []
    
    for source_time in source_times:
        # æ‰¾å‡ºåœ¨æºæ—¶é—´ä¹‹åæœ€è¿‘çš„ç›®æ ‡æ—¶é—´
        future_targets = target_times[target_times > source_time]
        if len(future_targets) > 0:
            nearest_target = future_targets[0]
            delay_seconds = (pd.to_datetime(nearest_target) - pd.to_datetime(source_time)).total_seconds()
            if 0 <= delay_seconds <= 1800:  # é™åˆ¶åœ¨30åˆ†é’Ÿå†…çš„åˆç†å»¶æ—¶
                delays.append(delay_seconds)
    
    return delays

def create_process_summary(all_period_data):
    """åˆ›å»ºæµç¨‹æ±‡æ€»ç»Ÿè®¡"""
    
    print(f"\n=== æµç¨‹æ±‡æ€»ç»Ÿè®¡åˆ†æ ===")
    
    if not all_period_data:
        print("æ²¡æœ‰æ•°æ®è¿›è¡Œæµç¨‹æ±‡æ€»åˆ†æ")
        return
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(all_period_data)
    
    # å®šä¹‰æµç¨‹åˆ†ç±»å’Œèµ·å§‹ä½ç½®
    process_categories = {
        'å…±åŒå‰ç½®æµç¨‹': {
            '1_æŠ˜å æœºå…¥åŒ…æ•°': {'èµ·å§‹ä½ç½®': 'æŠ˜å æœºå…¥åŒ…æ•°', 'æµç¨‹é˜¶æ®µ': 'å‰ç½®-æŠ˜å '},
            '2_æŠ˜å æœºå‡ºåŒ…æ•°': {'èµ·å§‹ä½ç½®': 'æŠ˜å æœºå‡ºåŒ…æ•°', 'æµç¨‹é˜¶æ®µ': 'å‰ç½®-æŠ˜å '},
        },
        'å¤–å¾ªç¯åˆ†æµè¿æ¥': {
            '3_å¤–å¾ªç¯â†’ç¬¬ä¸€è£åˆ‡é€šé“': {'èµ·å§‹ä½ç½®': 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'æµç¨‹é˜¶æ®µ': 'åˆ†æµ-è¿æ¥'},
            '3_å¤–å¾ªç¯â†’ç¬¬äºŒè£åˆ‡é€šé“': {'èµ·å§‹ä½ç½®': 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'æµç¨‹é˜¶æ®µ': 'åˆ†æµ-è¿æ¥'},
            '3_å¤–å¾ªç¯â†’ç¬¬ä¸‰è£åˆ‡é€šé“': {'èµ·å§‹ä½ç½®': 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'æµç¨‹é˜¶æ®µ': 'åˆ†æµ-è¿æ¥'},
            '3_å¤–å¾ªç¯â†’ç¬¬å››è£åˆ‡é€šé“': {'èµ·å§‹ä½ç½®': 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡', 'æµç¨‹é˜¶æ®µ': 'åˆ†æµ-è¿æ¥'}
        },
        'ç”Ÿäº§çº¿1æµç¨‹': {
            '4_ç¬¬ä¸€è£åˆ‡é€šé“': {'èµ·å§‹ä½ç½®': 'è¿›ç¬¬ä¸€è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'æµç¨‹é˜¶æ®µ': 'ç”Ÿäº§çº¿1-è£åˆ‡'},
            '5_1å·æœ‰æ•ˆåˆ‡æ•°': {'èµ·å§‹ä½ç½®': '1#æœ‰æ•ˆåˆ‡æ•°', 'æµç¨‹é˜¶æ®µ': 'ç”Ÿäº§çº¿1-è£åˆ‡'},
            '6_1å·å°åŒ…æœºå…¥åŒ…æ•°': {'èµ·å§‹ä½ç½®': '1#å°åŒ…æœºå…¥åŒ…æ•°', 'æµç¨‹é˜¶æ®µ': 'ç”Ÿäº§çº¿1-åŒ…è£…'},
        },
        'ç”Ÿäº§çº¿2æµç¨‹': {
            '4_ç¬¬äºŒè£åˆ‡é€šé“': {'èµ·å§‹ä½ç½®': 'è¿›ç¬¬äºŒè£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'æµç¨‹é˜¶æ®µ': 'ç”Ÿäº§çº¿2-è£åˆ‡'},
            '5_2å·æœ‰æ•ˆåˆ‡æ•°': {'èµ·å§‹ä½ç½®': '2#æœ‰æ•ˆåˆ‡æ•°', 'æµç¨‹é˜¶æ®µ': 'ç”Ÿäº§çº¿2-è£åˆ‡'},
            '6_2å·å°åŒ…æœºå…¥åŒ…æ•°': {'èµ·å§‹ä½ç½®': '2#å°åŒ…æœºå…¥åŒ…æ•°', 'æµç¨‹é˜¶æ®µ': 'ç”Ÿäº§çº¿2-åŒ…è£…'},
        },
        'ç”Ÿäº§çº¿3æµç¨‹': {
            '4_ç¬¬ä¸‰è£åˆ‡é€šé“': {'èµ·å§‹ä½ç½®': 'è¿›ç¬¬ä¸‰è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'æµç¨‹é˜¶æ®µ': 'ç”Ÿäº§çº¿3-è£åˆ‡'},
            '5_3å·æœ‰æ•ˆåˆ‡æ•°': {'èµ·å§‹ä½ç½®': '3#æœ‰æ•ˆåˆ‡æ•°', 'æµç¨‹é˜¶æ®µ': 'ç”Ÿäº§çº¿3-è£åˆ‡'},
            '6_3å·å°åŒ…æœºå…¥åŒ…æ•°': {'èµ·å§‹ä½ç½®': '3#å°åŒ…æœºå…¥åŒ…æ•°', 'æµç¨‹é˜¶æ®µ': 'ç”Ÿäº§çº¿3-åŒ…è£…'},

        },
        'ç”Ÿäº§çº¿4æµç¨‹': {
            '4_ç¬¬å››è£åˆ‡é€šé“': {'èµ·å§‹ä½ç½®': 'è¿›ç¬¬å››è£åˆ‡é€šé“çº¸æ¡è®¡æ•°', 'æµç¨‹é˜¶æ®µ': 'ç”Ÿäº§çº¿4-è£åˆ‡'},
            '5_4å·æœ‰æ•ˆåˆ‡æ•°': {'èµ·å§‹ä½ç½®': '4#æœ‰æ•ˆåˆ‡æ•°', 'æµç¨‹é˜¶æ®µ': 'ç”Ÿäº§çº¿4-è£åˆ‡'},
            '6_4å·å°åŒ…æœºå…¥åŒ…æ•°': {'èµ·å§‹ä½ç½®': '4#å°åŒ…æœºå…¥åŒ…æ•°', 'æµç¨‹é˜¶æ®µ': 'ç”Ÿäº§çº¿4-åŒ…è£…'},

        }
    }
    
    # åˆ›å»ºæ±‡æ€»ç»Ÿè®¡
    summary_data = []
    
    for category_name, processes in process_categories.items():
        print(f"\n--- {category_name} ---")
        
        for process_key, process_info in processes.items():
            # ç‰¹æ®Šå¤„ç†å¤–å¾ªç¯åˆ†æµè¿æ¥
            if category_name == 'å¤–å¾ªç¯åˆ†æµè¿æ¥':
                # æ ¹æ®process_keyç¡®å®šç›®æ ‡åˆ—å
                target_mapping = {
                    '3_å¤–å¾ªç¯â†’ç¬¬ä¸€è£åˆ‡é€šé“': 'è¿›ç¬¬ä¸€è£åˆ‡é€šé“çº¸æ¡è®¡æ•°',
                    '3_å¤–å¾ªç¯â†’ç¬¬äºŒè£åˆ‡é€šé“': 'è¿›ç¬¬äºŒè£åˆ‡é€šé“çº¸æ¡è®¡æ•°',
                    '3_å¤–å¾ªç¯â†’ç¬¬ä¸‰è£åˆ‡é€šé“': 'è¿›ç¬¬ä¸‰è£åˆ‡é€šé“çº¸æ¡è®¡æ•°',
                    '3_å¤–å¾ªç¯â†’ç¬¬å››è£åˆ‡é€šé“': 'è¿›ç¬¬å››è£åˆ‡é€šé“çº¸æ¡è®¡æ•°'
                }
                target_col = target_mapping.get(process_key, '')
                
                # ç­›é€‰ä»å¤–å¾ªç¯è¿›å†…å¾ªç¯åˆ°å¯¹åº”è£åˆ‡é€šé“çš„æ•°æ®
                process_data = df[
                    (df['èµ·å§‹åˆ—å'] == 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡') & 
                    (df['ç›®æ ‡åˆ—å'] == target_col)
                ]
            else:
                # ç­›é€‰è¯¥æµç¨‹çš„æ•°æ®
                process_data = df[df['èµ·å§‹ç‚¹ä½'] == process_key]
            
            if len(process_data) > 0:
                # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                avg_delay = process_data['å¹³å‡å»¶æ—¶(ç§’)'].mean()
                median_delay = process_data['ä¸­ä½å»¶æ—¶(ç§’)'].mean()
                min_delay = process_data['æœ€å°å»¶æ—¶(ç§’)'].min()
                max_delay = process_data['æœ€å¤§å»¶æ—¶(ç§’)'].max()
                std_delay = process_data['æ ‡å‡†å·®(ç§’)'].mean()
                total_samples = process_data['æ ·æœ¬æ•°'].sum()
                time_periods_count = process_data['æ—¶é—´æ®µID'].nunique()
                
                # æ—¶é—´èŒƒå›´
                time_range = f"[{min_delay:.2f},{max_delay:.2f}]"
                
                # ç¡®å®šç›®æ ‡ä½ç½®
                if category_name == 'å¤–å¾ªç¯åˆ†æµè¿æ¥':
                    target_pos = target_mapping.get(process_key, '')
                else:
                    target_pos = process_data['ç›®æ ‡åˆ—å'].iloc[0] if len(process_data) > 0 else ''
                
                summary_row = {
                    'æµç¨‹ç±»åˆ«': category_name,
                    'æµç¨‹ç¯èŠ‚': process_key,
                    # 'æµç¨‹é˜¶æ®µ': process_info['æµç¨‹é˜¶æ®µ'],
                    'èµ·å§‹ä½ç½®': process_info['èµ·å§‹ä½ç½®'],
                    'ç›®æ ‡ä½ç½®': target_pos,
                    'å¹³å‡å»¶æ—¶(ç§’)': avg_delay,
                    'ä¸­ä½å»¶æ—¶(ç§’)': median_delay,
                    'æ—¶é—´èŒƒå›´': time_range,
                    'æœ€å°å»¶æ—¶(ç§’)': min_delay,
                    'æœ€å¤§å»¶æ—¶(ç§’)': max_delay,
                    'æ ‡å‡†å·®(ç§’)': std_delay,
                    'æ¶‰åŠæ—¶é—´æ®µæ•°': time_periods_count,
                    'æ€»æ ·æœ¬æ•°': total_samples,
                    'å˜å¼‚ç³»æ•°(%)': (std_delay / avg_delay * 100) if avg_delay > 0 else 0
                }
                
                summary_data.append(summary_row)
                
                print(f"  {process_key}: å¹³å‡{avg_delay:.2f}ç§’ (èŒƒå›´: {time_range})")
            else:
                print(f"  {process_key}: æ— æ•°æ®")
    
    # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
    if summary_data:
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_csv('æµç¨‹æ±‡æ€»ç»Ÿè®¡_åˆ†æ®µ_1.csv', index=False, encoding='utf-8-sig', float_format='%.2f')
        print(f"\næµç¨‹æ±‡æ€»ç»Ÿè®¡å·²ä¿å­˜ï¼šæµç¨‹æ±‡æ€»ç»Ÿè®¡_åˆ†æ®µ_1.csv")
        
        # æŒ‰æµç¨‹ç±»åˆ«åˆ†ç»„ç»Ÿè®¡
        category_summary = []
        for category in summary_df['æµç¨‹ç±»åˆ«'].unique():
            category_data = summary_df[summary_df['æµç¨‹ç±»åˆ«'] == category]
            
            category_row = {
                'æµç¨‹ç±»åˆ«': category,
                'ç¯èŠ‚æ•°é‡': len(category_data),
                'æ€»å¹³å‡å»¶æ—¶(ç§’)': category_data['å¹³å‡å»¶æ—¶(ç§’)'].sum(),
                'å¹³å‡å•ç¯èŠ‚å»¶æ—¶(ç§’)': category_data['å¹³å‡å»¶æ—¶(ç§’)'].mean(),
                'æœ€å¿«ç¯èŠ‚å»¶æ—¶(ç§’)': category_data['å¹³å‡å»¶æ—¶(ç§’)'].min(),
                'æœ€æ…¢ç¯èŠ‚å»¶æ—¶(ç§’)': category_data['å¹³å‡å»¶æ—¶(ç§’)'].max(),
                'ç±»åˆ«å†…å·®å¼‚(ç§’)': category_data['å¹³å‡å»¶æ—¶(ç§’)'].max() - category_data['å¹³å‡å»¶æ—¶(ç§’)'].min(),
                'æ€»æ ·æœ¬æ•°': category_data['æ€»æ ·æœ¬æ•°'].sum(),
                'å¹³å‡å˜å¼‚ç³»æ•°(%)': category_data['å˜å¼‚ç³»æ•°(%)'].mean()
            }
            category_summary.append(category_row)
        
        # ä¿å­˜æµç¨‹ç±»åˆ«æ±‡æ€»
        if category_summary:
            category_df = pd.DataFrame(category_summary)
            category_df.to_csv('æµç¨‹ç±»åˆ«æ±‡æ€»_åˆ†æ®µ_1.csv', index=False, encoding='utf-8-sig', float_format='%.2f')
            print(f"æµç¨‹ç±»åˆ«æ±‡æ€»å·²ä¿å­˜ï¼šæµç¨‹ç±»åˆ«æ±‡æ€»_åˆ†æ®µ_1.csv")
            
            # æ˜¾ç¤ºç±»åˆ«æ±‡æ€»ç»“æœ
            print(f"\n=== æµç¨‹ç±»åˆ«æ±‡æ€» ===")
            for _, row in category_df.iterrows():
                print(f"{row['æµç¨‹ç±»åˆ«']}:")
                print(f"  ç¯èŠ‚æ•°é‡: {row['ç¯èŠ‚æ•°é‡']}")
                print(f"  æ€»å»¶æ—¶: {row['æ€»å¹³å‡å»¶æ—¶(ç§’)']:.2f}ç§’")
                print(f"  å¹³å‡å•ç¯èŠ‚: {row['å¹³å‡å•ç¯èŠ‚å»¶æ—¶(ç§’)']:.2f}ç§’")
                print(f"  ç±»åˆ«å†…å·®å¼‚: {row['ç±»åˆ«å†…å·®å¼‚(ç§’)']:.2f}ç§’")
                print(f"  å˜å¼‚ç³»æ•°: {row['å¹³å‡å˜å¼‚ç³»æ•°(%)']:.1f}%")
        
        # åˆ›å»ºæµç¨‹å¯¹æ¯”å¯è§†åŒ–
        create_process_comparison_chart(summary_df, category_df)
        
        # åˆ›å»ºå®Œæ•´æµç¨‹è·¯å¾„æ—¶é—´æ€»ç»“
        create_complete_flow_summary(summary_df, category_df)
    else:
        print("æ²¡æœ‰ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡æ•°æ®")

def create_process_comparison_chart(summary_df, category_df):
    """åˆ›å»ºæµç¨‹å¯¹æ¯”å›¾è¡¨"""
    
    plt.figure(figsize=(20, 12))
    
    # å­å›¾1: å„æµç¨‹ç¯èŠ‚å»¶æ—¶å¯¹æ¯”
    plt.subplot(2, 3, 1)
    categories = summary_df['æµç¨‹ç±»åˆ«'].unique()
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
    
    for i, category in enumerate(categories):
        cat_data = summary_df[summary_df['æµç¨‹ç±»åˆ«'] == category]
        plt.bar(range(len(cat_data)), cat_data['å¹³å‡å»¶æ—¶(ç§’)'], 
                alpha=0.7, label=category, color=colors[i % len(colors)])
    
    plt.title('å„æµç¨‹ç¯èŠ‚å¹³å‡å»¶æ—¶å¯¹æ¯”')
    plt.ylabel('å¹³å‡å»¶æ—¶(ç§’)')
    plt.xlabel('æµç¨‹ç¯èŠ‚')
    plt.legend()
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    # å­å›¾2: æµç¨‹ç±»åˆ«æ€»å»¶æ—¶å¯¹æ¯”
    plt.subplot(2, 3, 2)
    plt.bar(category_df['æµç¨‹ç±»åˆ«'], category_df['æ€»å¹³å‡å»¶æ—¶(ç§’)'], 
            alpha=0.7, color=colors[:len(category_df)])
    plt.title('å„æµç¨‹ç±»åˆ«æ€»å»¶æ—¶å¯¹æ¯”')
    plt.ylabel('æ€»å»¶æ—¶(ç§’)')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    # å­å›¾3: å˜å¼‚ç³»æ•°å¯¹æ¯”
    plt.subplot(2, 3, 3)
    plt.bar(category_df['æµç¨‹ç±»åˆ«'], category_df['å¹³å‡å˜å¼‚ç³»æ•°(%)'], 
            alpha=0.7, color=colors[:len(category_df)])
    plt.title('å„æµç¨‹ç±»åˆ«ç¨³å®šæ€§å¯¹æ¯”')
    plt.ylabel('å¹³å‡å˜å¼‚ç³»æ•°(%)')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    # å­å›¾4: ç¯èŠ‚æ•°é‡å¯¹æ¯”
    plt.subplot(2, 3, 4)
    plt.bar(category_df['æµç¨‹ç±»åˆ«'], category_df['ç¯èŠ‚æ•°é‡'], 
            alpha=0.7, color=colors[:len(category_df)])
    plt.title('å„æµç¨‹ç±»åˆ«ç¯èŠ‚æ•°é‡')
    plt.ylabel('ç¯èŠ‚æ•°é‡')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    # å­å›¾5: ç±»åˆ«å†…å·®å¼‚å¯¹æ¯”
    plt.subplot(2, 3, 5)
    plt.bar(category_df['æµç¨‹ç±»åˆ«'], category_df['ç±»åˆ«å†…å·®å¼‚(ç§’)'], 
            alpha=0.7, color=colors[:len(category_df)])
    plt.title('å„æµç¨‹ç±»åˆ«å†…éƒ¨å·®å¼‚')
    plt.ylabel('ç±»åˆ«å†…å·®å¼‚(ç§’)')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    # å­å›¾6: æ ·æœ¬æ•°é‡å¯¹æ¯”
    plt.subplot(2, 3, 6)
    plt.bar(category_df['æµç¨‹ç±»åˆ«'], category_df['æ€»æ ·æœ¬æ•°'], 
            alpha=0.7, color=colors[:len(category_df)])
    plt.title('å„æµç¨‹ç±»åˆ«æ ·æœ¬æ•°é‡')
    plt.ylabel('æ€»æ ·æœ¬æ•°')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('æµç¨‹æ±‡æ€»å¯¹æ¯”_åˆ†æ®µ_1.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"æµç¨‹æ±‡æ€»å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜ï¼šæµç¨‹æ±‡æ€»å¯¹æ¯”_åˆ†æ®µ_1.png")

def create_complete_flow_summary(summary_df, category_df):
    """åˆ›å»ºå®Œæ•´æµç¨‹è·¯å¾„æ—¶é—´æ€»ç»“"""
    
    print(f"\n{'='*60}")
    print(f"=== å®Œæ•´æµç¨‹è·¯å¾„æ—¶é—´æ€»ç»“ ===")
    print(f"{'='*60}")
    
    # å®‰å…¨è·å–å„ç±»åˆ«çš„æ€»å»¶æ—¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼0
    def safe_get_category_time(category_name):
        category_data = category_df[category_df['æµç¨‹ç±»åˆ«'] == category_name]['æ€»å¹³å‡å»¶æ—¶(ç§’)']
        if len(category_data) > 0:
            return category_data.iloc[0]
        else:
            print(f"  âš ï¸  æœªæ‰¾åˆ° {category_name} çš„æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼0")
            return 0.0
    
    common_time = safe_get_category_time('å…±åŒå‰ç½®æµç¨‹')
    distribution_time = safe_get_category_time('å¤–å¾ªç¯åˆ†æµè¿æ¥')
    
    line1_time = safe_get_category_time('ç”Ÿäº§çº¿1æµç¨‹')
    line2_time = safe_get_category_time('ç”Ÿäº§çº¿2æµç¨‹')
    line3_time = safe_get_category_time('ç”Ÿäº§çº¿3æµç¨‹')
    line4_time = safe_get_category_time('ç”Ÿäº§çº¿4æµç¨‹')
    
    # å®‰å…¨è·å–å„åˆ†æµè¿æ¥çš„å»¶æ—¶
    flow_dist_df = summary_df[summary_df['æµç¨‹ç±»åˆ«'] == 'å¤–å¾ªç¯åˆ†æµè¿æ¥']
    
    def safe_get_distribution_time(flow_name):
        flow_data = flow_dist_df[flow_dist_df['æµç¨‹ç¯èŠ‚'] == flow_name]['å¹³å‡å»¶æ—¶(ç§’)']
        if len(flow_data) > 0:
            return flow_data.iloc[0]
        else:
            print(f"  âš ï¸  æœªæ‰¾åˆ° {flow_name} çš„æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼0")
            return 0.0
    
    dist_to_line1 = safe_get_distribution_time('3_å¤–å¾ªç¯â†’ç¬¬ä¸€è£åˆ‡é€šé“')
    dist_to_line2 = safe_get_distribution_time('3_å¤–å¾ªç¯â†’ç¬¬äºŒè£åˆ‡é€šé“')
    dist_to_line3 = safe_get_distribution_time('3_å¤–å¾ªç¯â†’ç¬¬ä¸‰è£åˆ‡é€šé“')
    dist_to_line4 = safe_get_distribution_time('3_å¤–å¾ªç¯â†’ç¬¬å››è£åˆ‡é€šé“')
    
    # è®¡ç®—å®Œæ•´æµç¨‹è·¯å¾„æ—¶é—´
    complete_flows = {
        'ç”Ÿäº§çº¿1å®Œæ•´æµç¨‹': {
            'å…±åŒå‰ç½®æµç¨‹': common_time,
            'å¤–å¾ªç¯â†’ç¬¬ä¸€è£åˆ‡é€šé“': dist_to_line1,
            'ç”Ÿäº§çº¿1æµç¨‹': line1_time,
            'æ€»æ—¶é—´': common_time + dist_to_line1 + line1_time
        },
        'ç”Ÿäº§çº¿2å®Œæ•´æµç¨‹': {
            'å…±åŒå‰ç½®æµç¨‹': common_time,
            'å¤–å¾ªç¯â†’ç¬¬äºŒè£åˆ‡é€šé“': dist_to_line2,
            'ç”Ÿäº§çº¿2æµç¨‹': line2_time,
            'æ€»æ—¶é—´': common_time + dist_to_line2 + line2_time
        },
        'ç”Ÿäº§çº¿3å®Œæ•´æµç¨‹': {
            'å…±åŒå‰ç½®æµç¨‹': common_time,
            'å¤–å¾ªç¯â†’ç¬¬ä¸‰è£åˆ‡é€šé“': dist_to_line3,
            'ç”Ÿäº§çº¿3æµç¨‹': line3_time,
            'æ€»æ—¶é—´': common_time + dist_to_line3 + line3_time
        },
        'ç”Ÿäº§çº¿4å®Œæ•´æµç¨‹': {
            'å…±åŒå‰ç½®æµç¨‹': common_time,
            'å¤–å¾ªç¯â†’ç¬¬å››è£åˆ‡é€šé“': dist_to_line4,
            'ç”Ÿäº§çº¿4æµç¨‹': line4_time,
            'æ€»æ—¶é—´': common_time + dist_to_line4 + line4_time
        }
    }
    
    # æ˜¾ç¤ºå®Œæ•´æµç¨‹è·¯å¾„åˆ†æ
    print(f"\nå®Œæ•´æµç¨‹è·¯å¾„åˆ†æ:")
    print(f"{'='*50}")
    
    for flow_name, flow_data in complete_flows.items():
        print(f"\nğŸ“ {flow_name}:")
        print(f"  å…±åŒå‰ç½®æµç¨‹: {flow_data['å…±åŒå‰ç½®æµç¨‹']:.2f}ç§’")
        
        # æ‰¾åˆ°åˆ†æµè¿æ¥çš„é”®å
        dist_keys = [k for k in flow_data.keys() if 'å¤–å¾ªç¯â†’' in k]
        if dist_keys:
            dist_key = dist_keys[0]
            print(f"  {dist_key}: {flow_data[dist_key]:.2f}ç§’")
        else:
            print(f"  å¤–å¾ªç¯åˆ†æµè¿æ¥: 0.00ç§’")
            dist_key = None
        
        # æ‰¾åˆ°ç”Ÿäº§çº¿æµç¨‹çš„é”®å
        line_keys = [k for k in flow_data.keys() if 'ç”Ÿäº§çº¿' in k and 'æµç¨‹' in k]
        if line_keys:
            line_key = line_keys[0]
            print(f"  {line_key}: {flow_data[line_key]:.2f}ç§’")
        else:
            print(f"  ç”Ÿäº§çº¿æµç¨‹: 0.00ç§’")
            line_key = None
        
        print(f"  â¤ æ€»å»¶æ—¶: {flow_data['æ€»æ—¶é—´']:.2f}ç§’")
        
        # è®¡ç®—å„é˜¶æ®µå æ¯”ï¼ˆé¿å…é™¤0é”™è¯¯ï¼‰
        if flow_data['æ€»æ—¶é—´'] > 0:
            common_pct = (flow_data['å…±åŒå‰ç½®æµç¨‹'] / flow_data['æ€»æ—¶é—´']) * 100
            dist_pct = (flow_data[dist_key] / flow_data['æ€»æ—¶é—´']) * 100 if dist_key else 0
            line_pct = (flow_data[line_key] / flow_data['æ€»æ—¶é—´']) * 100 if line_key else 0
            
            print(f"    - å‰ç½®æµç¨‹å æ¯”: {common_pct:.1f}%")
            print(f"    - åˆ†æµè¿æ¥å æ¯”: {dist_pct:.1f}%")
            print(f"    - ç”Ÿäº§çº¿æµç¨‹å æ¯”: {line_pct:.1f}%")
        else:
            print(f"    - æ€»æ—¶é—´ä¸º0ï¼Œæ— æ³•è®¡ç®—å æ¯”")
    
    # ç”Ÿäº§çº¿æ€§èƒ½å¯¹æ¯”
    print(f"\n{'='*50}")
    print(f"ç”Ÿäº§çº¿æ€§èƒ½æ’å:")
    print(f"{'='*50}")
    
    # æŒ‰æ€»æ—¶é—´æ’åº
    sorted_flows = sorted(complete_flows.items(), key=lambda x: x[1]['æ€»æ—¶é—´'])
    
    if len(sorted_flows) > 0:
        for i, (flow_name, flow_data) in enumerate(sorted_flows, 1):
            status = "ğŸŸ¢ æœ€ä¼˜" if i == 1 else "ğŸ”´ æœ€å·®" if i == len(sorted_flows) else f"ğŸŸ¡ ç¬¬{i}å"
            print(f"{status} {flow_name}: {flow_data['æ€»æ—¶é—´']:.2f}ç§’")
        
        # æ€§èƒ½å·®å¼‚åˆ†æ
        best_time = sorted_flows[0][1]['æ€»æ—¶é—´']
        worst_time = sorted_flows[-1][1]['æ€»æ—¶é—´']
        performance_gap = worst_time - best_time
        
        print(f"\n{'='*50}")
        print(f"æ€§èƒ½å·®å¼‚åˆ†æ:")
        print(f"{'='*50}")
        print(f"æœ€ä¼˜ç”Ÿäº§çº¿: {sorted_flows[0][0]} ({best_time:.2f}ç§’)")
        print(f"æœ€å·®ç”Ÿäº§çº¿: {sorted_flows[-1][0]} ({worst_time:.2f}ç§’)")
        print(f"æ€§èƒ½å·®å¼‚: {performance_gap:.2f}ç§’")
        
        if best_time > 0:
            print(f"ç›¸å¯¹å·®å¼‚: {(performance_gap/best_time)*100:.1f}%")
        if worst_time > 0:
            improvement_potential = (performance_gap / worst_time) * 100
            print(f"ä¼˜åŒ–æ½œåŠ›: {improvement_potential:.1f}%")
    else:
        print("æ²¡æœ‰ç”Ÿäº§çº¿æ•°æ®å¯ä¾›å¯¹æ¯”")
    
    # ç“¶é¢ˆç¯èŠ‚è¯†åˆ«
    print(f"\n{'='*50}")
    print(f"ç“¶é¢ˆç¯èŠ‚è¯†åˆ«:")
    print(f"{'='*50}")
    
    # æ‰¾å‡ºå„é˜¶æ®µçš„æœ€å¤§å»¶æ—¶
    max_common = common_time
    max_dist = max(dist_to_line1, dist_to_line2, dist_to_line3, dist_to_line4)
    max_line = max(line1_time, line2_time, line3_time, line4_time)
    
    bottleneck_stage = max(
        ("å…±åŒå‰ç½®æµç¨‹", max_common),
        ("å¤–å¾ªç¯åˆ†æµè¿æ¥", max_dist),
        ("ç”Ÿäº§çº¿æµç¨‹", max_line)
    )
    
    print(f"æœ€å¤§ç“¶é¢ˆé˜¶æ®µ: {bottleneck_stage[0]} ({bottleneck_stage[1]:.2f}ç§’)")
    
    if bottleneck_stage[0] == "å¤–å¾ªç¯åˆ†æµè¿æ¥":
        # æ‰¾å‡ºå“ªä¸ªåˆ†æµè¿æ¥æ˜¯ç“¶é¢ˆ
        dist_times = {
            "ç¬¬ä¸€è£åˆ‡é€šé“": dist_to_line1,
            "ç¬¬äºŒè£åˆ‡é€šé“": dist_to_line2,
            "ç¬¬ä¸‰è£åˆ‡é€šé“": dist_to_line3,
            "ç¬¬å››è£åˆ‡é€šé“": dist_to_line4
        }
        worst_dist = max(dist_times.items(), key=lambda x: x[1])
        print(f"åˆ†æµç“¶é¢ˆ: å¤–å¾ªç¯â†’{worst_dist[0]} ({worst_dist[1]:.2f}ç§’)")
    
    elif bottleneck_stage[0] == "ç”Ÿäº§çº¿æµç¨‹":
        # æ‰¾å‡ºå“ªä¸ªç”Ÿäº§çº¿æ˜¯ç“¶é¢ˆ
        line_times = {
            "ç”Ÿäº§çº¿1": line1_time,
            "ç”Ÿäº§çº¿2": line2_time,
            "ç”Ÿäº§çº¿3": line3_time,
            "ç”Ÿäº§çº¿4": line4_time
        }
        worst_line = max(line_times.items(), key=lambda x: x[1])
        print(f"ç”Ÿäº§çº¿ç“¶é¢ˆ: {worst_line[0]} ({worst_line[1]:.2f}ç§’)")
    
    # ä¿å­˜å®Œæ•´æµç¨‹è·¯å¾„æ±‡æ€»
    complete_flow_data = []
    for flow_name, flow_data in complete_flows.items():
        # å®‰å…¨è·å–åˆ†æµè¿æ¥å’Œç”Ÿäº§çº¿æµç¨‹çš„å€¼
        dist_values = [v for k, v in flow_data.items() if 'å¤–å¾ªç¯â†’' in k]
        line_values = [v for k, v in flow_data.items() if 'ç”Ÿäº§çº¿' in k and 'æµç¨‹' in k]
        
        dist_time = dist_values[0] if dist_values else 0.0
        line_time = line_values[0] if line_values else 0.0
        
        row = {
            'å®Œæ•´æµç¨‹è·¯å¾„': flow_name,
            'å…±åŒå‰ç½®æµç¨‹(ç§’)': flow_data['å…±åŒå‰ç½®æµç¨‹'],
            'å¤–å¾ªç¯åˆ†æµè¿æ¥(ç§’)': dist_time,
            'ç”Ÿäº§çº¿æµç¨‹(ç§’)': line_time,
            'æ€»å»¶æ—¶(ç§’)': flow_data['æ€»æ—¶é—´']
        }
        
        # å®‰å…¨è®¡ç®—å æ¯”ï¼ˆé¿å…é™¤0é”™è¯¯ï¼‰
        if flow_data['æ€»æ—¶é—´'] > 0:
            row['å‰ç½®æµç¨‹å æ¯”(%)'] = (flow_data['å…±åŒå‰ç½®æµç¨‹'] / flow_data['æ€»æ—¶é—´']) * 100
            row['åˆ†æµè¿æ¥å æ¯”(%)'] = (dist_time / flow_data['æ€»æ—¶é—´']) * 100
            row['ç”Ÿäº§çº¿æµç¨‹å æ¯”(%)'] = (line_time / flow_data['æ€»æ—¶é—´']) * 100
        else:
            row['å‰ç½®æµç¨‹å æ¯”(%)'] = 0.0
            row['åˆ†æµè¿æ¥å æ¯”(%)'] = 0.0
            row['ç”Ÿäº§çº¿æµç¨‹å æ¯”(%)'] = 0.0
        
        complete_flow_data.append(row)
    
    # ä¿å­˜åˆ°CSV
    if complete_flow_data:
        complete_flow_df = pd.DataFrame(complete_flow_data)
        complete_flow_df = complete_flow_df.sort_values('æ€»å»¶æ—¶(ç§’)')
        complete_flow_df.to_csv('å®Œæ•´æµç¨‹è·¯å¾„æ±‡æ€»_åˆ†æ®µ_1.csv', index=False, encoding='utf-8-sig', float_format='%.2f')
        print(f"\nå®Œæ•´æµç¨‹è·¯å¾„æ±‡æ€»å·²ä¿å­˜ï¼šå®Œæ•´æµç¨‹è·¯å¾„æ±‡æ€»_åˆ†æ®µ_1.csv")

def create_delay_fitting_equations(all_period_data):
    """æ ¹æ®æ—¶é—´æ®µå»¶æ—¶æ•°æ®æ‹Ÿåˆæ–¹ç¨‹å¼"""
    
    print(f"\n{'='*60}")
    print(f"=== å»¶æ—¶æ—¶é—´æ‹Ÿåˆæ–¹ç¨‹åˆ†æ ===")
    print(f"{'='*60}")
    
    if not all_period_data:
        print("æ²¡æœ‰æ•°æ®è¿›è¡Œæ‹Ÿåˆåˆ†æ")
        return
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(all_period_data)
    
    # è¯»å–æ±‡æ€»æ•°æ®ä»¥è·å–é€Ÿåº¦ä¿¡æ¯
    try:
        summary_data = pd.read_csv('å­˜çº¸æ¶æ•°æ®æ±‡æ€».csv', encoding='utf-8-sig')
        summary_data['æ—¶é—´'] = pd.to_datetime(summary_data['æ—¶é—´'])
        print(f"åŠ è½½æ±‡æ€»æ•°æ®ç”¨äºé€Ÿåº¦ä¿¡æ¯ï¼š{len(summary_data)}æ¡è®°å½•")
    except Exception as e:
        print(f"è¯»å–æ±‡æ€»æ•°æ®å‡ºé”™ï¼š{e}")
        return
    
    # å®šä¹‰æµç¨‹ç¯èŠ‚ä¸å…¶å¯¹åº”çš„é€Ÿåº¦åˆ—çš„æ˜ å°„
    speed_column_mapping = {
        '1_æŠ˜å æœºé€Ÿåº¦': 'æŠ˜å æœºå®é™…é€Ÿåº¦',
        '2_æŠ˜å æœºå…¥åŒ…æ•°': 'æŠ˜å æœºå®é™…é€Ÿåº¦',  # ä½¿ç”¨æŠ˜å æœºé€Ÿåº¦ä½œä¸ºå‚è€ƒ
        '3_æŠ˜å æœºå‡ºåŒ…æ•°': 'æŠ˜å æœºå®é™…é€Ÿåº¦',
        '4_å­˜çº¸ç‡': 'æŠ˜å æœºå®é™…é€Ÿåº¦',
        '5_å¤–å¾ªç¯â†’ç¬¬ä¸€è£åˆ‡é€šé“': 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡',  # ä½¿ç”¨æ•°é‡ä½œä¸ºé€Ÿåº¦å‚è€ƒ
        '5_å¤–å¾ªç¯â†’ç¬¬äºŒè£åˆ‡é€šé“': 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡',
        '5_å¤–å¾ªç¯â†’ç¬¬ä¸‰è£åˆ‡é€šé“': 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡',
        '5_å¤–å¾ªç¯â†’ç¬¬å››è£åˆ‡é€šé“': 'å¤–å¾ªç¯è¿›å†…å¾ªç¯çº¸æ¡æ•°é‡',
        '6_ç¬¬ä¸€è£åˆ‡é€šé“': 'è£åˆ‡æœºå®é™…é€Ÿåº¦',
        '7_è£åˆ‡é€Ÿåº¦': 'è£åˆ‡æœºå®é™…é€Ÿåº¦',
        '8_1å·æœ‰æ•ˆåˆ‡æ•°': 'è£åˆ‡æœºå®é™…é€Ÿåº¦',
        '9_1å·å°åŒ…æœºå…¥åŒ…æ•°': '1#å°åŒ…æœºå®é™…é€Ÿåº¦',
        '8_2å·æœ‰æ•ˆåˆ‡æ•°': 'è£åˆ‡æœºå®é™…é€Ÿåº¦',
        '9_2å·å°åŒ…æœºå…¥åŒ…æ•°': '2#å°åŒ…æœºå®é™…é€Ÿåº¦',
        '8_3å·æœ‰æ•ˆåˆ‡æ•°': 'è£åˆ‡æœºå®é™…é€Ÿåº¦',
        '9_3å·å°åŒ…æœºå…¥åŒ…æ•°': '3#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦',
        '8_4å·æœ‰æ•ˆåˆ‡æ•°': 'è£åˆ‡æœºå®é™…é€Ÿåº¦',
        '9_4å·å°åŒ…æœºå…¥åŒ…æ•°': '4#å°åŒ…æœºä¸»æœºå®é™…é€Ÿåº¦'
    }
    
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æµç¨‹ç¯èŠ‚
    process_steps = df['èµ·å§‹ç‚¹ä½'].unique()
    print(f"è¯†åˆ«åˆ° {len(process_steps)} ä¸ªæµç¨‹ç¯èŠ‚éœ€è¦æ‹Ÿåˆ")
    
    # å­˜å‚¨æ‹Ÿåˆç»“æœ
    fitting_results = []
    
    for step in process_steps:
        print(f"\n--- åˆ†ææµç¨‹ç¯èŠ‚: {step} ---")
        
        # ç­›é€‰è¯¥æµç¨‹ç¯èŠ‚çš„æ•°æ®
        step_data = df[df['èµ·å§‹ç‚¹ä½'] == step].copy()
        
        if len(step_data) < 5:  # æ•°æ®é‡å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆæ‹Ÿåˆ
            print(f"  æ•°æ®é‡ä¸è¶³ ({len(step_data)}æ¡)ï¼Œè·³è¿‡æ‹Ÿåˆ")
            continue
        
        # è·å–å¯¹åº”çš„é€Ÿåº¦åˆ—
        speed_column = speed_column_mapping.get(step, None)
        if not speed_column:
            print(f"  æœªæ‰¾åˆ°å¯¹åº”çš„é€Ÿåº¦åˆ—ï¼Œè·³è¿‡æ‹Ÿåˆ")
            continue
        
        if speed_column not in summary_data.columns:
            print(f"  é€Ÿåº¦åˆ— {speed_column} ä¸å­˜åœ¨ï¼Œè·³è¿‡æ‹Ÿåˆ")
            continue
        
        # å‡†å¤‡æ‹Ÿåˆæ•°æ®
        fitting_data = prepare_fitting_data(step_data, summary_data, speed_column, step)
        
        if fitting_data is None or len(fitting_data) < 5:
            print(f"  å‡†å¤‡æ‹Ÿåˆæ•°æ®å¤±è´¥æˆ–æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡æ‹Ÿåˆ")
            continue
        
        # æ‰§è¡Œå¤šç§æ‹Ÿåˆæ–¹æ³•
        fit_result = perform_multiple_fitting(fitting_data, step, speed_column)
        
        if fit_result:
            fitting_results.append(fit_result)
            print(f"  âœ… æ‹Ÿåˆå®Œæˆ")
        else:
            print(f"  âŒ æ‹Ÿåˆå¤±è´¥")
    
    # ä¿å­˜æ‹Ÿåˆç»“æœ
    if fitting_results:
        save_fitting_results(fitting_results)
        create_fitting_visualization(fitting_results)
        analyze_fitting_patterns(fitting_results)
    else:
        print("æ²¡æœ‰æˆåŠŸçš„æ‹Ÿåˆç»“æœ")

def prepare_fitting_data(step_data, summary_data, speed_column, step_name):
    """å‡†å¤‡æ‹Ÿåˆæ•°æ®"""
    
    fitting_records = []
    
    for _, row in step_data.iterrows():
        time_period_id = row['æ—¶é—´æ®µID']
        start_time = row['å¼€å§‹æ—¶é—´']
        end_time = row['ç»“æŸæ—¶é—´']
        delay_time = row['å¹³å‡å»¶æ—¶(ç§’)']
        
        # ç­›é€‰è¯¥æ—¶é—´æ®µçš„æ•°æ®
        period_mask = (summary_data['æ—¶é—´'] >= start_time) & (summary_data['æ—¶é—´'] <= end_time)
        period_summary = summary_data[period_mask]
        
        if len(period_summary) == 0:
            continue
        
        # è®¡ç®—è¯¥æ—¶é—´æ®µçš„å¹³å‡é€Ÿåº¦
        speed_values = period_summary[speed_column].dropna()
        if len(speed_values) == 0:
            continue
        
        avg_speed = speed_values.mean()
        median_speed = speed_values.median()
        max_speed = speed_values.max()
        min_speed = speed_values.min()
        std_speed = speed_values.std()
        
        # è¿‡æ»¤å¼‚å¸¸å€¼
        if avg_speed <= 0 or delay_time <= 0 or avg_speed > 10000 or delay_time > 3600:
            continue
        
        fitting_records.append({
            'æ—¶é—´æ®µID': time_period_id,
            'æµç¨‹ç¯èŠ‚': step_name,
            'é€Ÿåº¦åˆ—': speed_column,
            'å¹³å‡é€Ÿåº¦': avg_speed,
            'ä¸­ä½é€Ÿåº¦': median_speed,
            'æœ€å¤§é€Ÿåº¦': max_speed,
            'æœ€å°é€Ÿåº¦': min_speed,
            'é€Ÿåº¦æ ‡å‡†å·®': std_speed,
            'å»¶æ—¶æ—¶é—´': delay_time,
            'å¼€å§‹æ—¶é—´': start_time,
            'ç»“æŸæ—¶é—´': end_time
        })
    
    if len(fitting_records) > 0:
        return pd.DataFrame(fitting_records)
    else:
        return None

def perform_multiple_fitting(data, step_name, speed_column):
    """æ‰§è¡Œå¤šç§æ‹Ÿåˆæ–¹æ³•"""
    
    X = data['å¹³å‡é€Ÿåº¦'].values.reshape(-1, 1)
    y = data['å»¶æ—¶æ—¶é—´'].values
    
    fitting_methods = {}
    
    try:
        # 1. çº¿æ€§æ‹Ÿåˆ: y = ax + b
        linear_reg = LinearRegression()
        linear_reg.fit(X, y)
        y_pred_linear = linear_reg.predict(X)
        r2_linear = r2_score(y, y_pred_linear)
        mse_linear = mean_squared_error(y, y_pred_linear)
        
        fitting_methods['çº¿æ€§æ‹Ÿåˆ'] = {
            'æ–¹ç¨‹å¼': f"y = {linear_reg.coef_[0]:.4f}*x + {linear_reg.intercept_:.4f}",
            'ç³»æ•°a': linear_reg.coef_[0],
            'ç³»æ•°b': linear_reg.intercept_,
            'RÂ²': r2_linear,
            'MSE': mse_linear,
            'é¢„æµ‹å€¼': y_pred_linear
        }
        
        # 2. äºŒæ¬¡å¤šé¡¹å¼æ‹Ÿåˆ: y = axÂ² + bx + c
        poly_features = PolynomialFeatures(degree=2)
        X_poly = poly_features.fit_transform(X)
        poly_reg = LinearRegression()
        poly_reg.fit(X_poly, y)
        y_pred_poly = poly_reg.predict(X_poly)
        r2_poly = r2_score(y, y_pred_poly)
        mse_poly = mean_squared_error(y, y_pred_poly)
        
        fitting_methods['äºŒæ¬¡å¤šé¡¹å¼æ‹Ÿåˆ'] = {
            'æ–¹ç¨‹å¼': f"y = {poly_reg.coef_[2]:.6f}*xÂ² + {poly_reg.coef_[1]:.4f}*x + {poly_reg.intercept_:.4f}",
            'ç³»æ•°a': poly_reg.coef_[2],
            'ç³»æ•°b': poly_reg.coef_[1],
            'ç³»æ•°c': poly_reg.intercept_,
            'RÂ²': r2_poly,
            'MSE': mse_poly,
            'é¢„æµ‹å€¼': y_pred_poly
        }
        
        # 3. åæ¯”ä¾‹æ‹Ÿåˆ: y = a/x + b (å½“x > 0æ—¶)
        if np.all(X > 0):
            X_inv = 1 / X
            inv_reg = LinearRegression()
            inv_reg.fit(X_inv, y)
            y_pred_inv = inv_reg.predict(X_inv)
            r2_inv = r2_score(y, y_pred_inv)
            mse_inv = mean_squared_error(y, y_pred_inv)
            
            fitting_methods['åæ¯”ä¾‹æ‹Ÿåˆ'] = {
                'æ–¹ç¨‹å¼': f"y = {inv_reg.coef_[0]:.4f}/x + {inv_reg.intercept_:.4f}",
                'ç³»æ•°a': inv_reg.coef_[0],
                'ç³»æ•°b': inv_reg.intercept_,
                'RÂ²': r2_inv,
                'MSE': mse_inv,
                'é¢„æµ‹å€¼': y_pred_inv
            }
        
        # 4. æŒ‡æ•°æ‹Ÿåˆ: y = a*e^(bx) (å½“y > 0æ—¶)
        if np.all(y > 0):
            try:
                log_y = np.log(y)
                exp_reg = LinearRegression()
                exp_reg.fit(X, log_y)
                log_y_pred = exp_reg.predict(X)
                y_pred_exp = np.exp(log_y_pred)
                r2_exp = r2_score(y, y_pred_exp)
                mse_exp = mean_squared_error(y, y_pred_exp)
                
                fitting_methods['æŒ‡æ•°æ‹Ÿåˆ'] = {
                    'æ–¹ç¨‹å¼': f"y = {np.exp(exp_reg.intercept_):.4f}*exp({exp_reg.coef_[0]:.6f}*x)",
                    'ç³»æ•°a': np.exp(exp_reg.intercept_),
                    'ç³»æ•°b': exp_reg.coef_[0],
                    'RÂ²': r2_exp,
                    'MSE': mse_exp,
                    'é¢„æµ‹å€¼': y_pred_exp
                }
            except:
                pass
        
        # 5. å¹‚å‡½æ•°æ‹Ÿåˆ: y = a*x^b (å½“x > 0, y > 0æ—¶)
        if np.all(X > 0) and np.all(y > 0):
            try:
                log_X = np.log(X)
                log_y = np.log(y)
                power_reg = LinearRegression()
                power_reg.fit(log_X, log_y)
                log_y_pred = power_reg.predict(log_X)
                y_pred_power = np.exp(log_y_pred)
                r2_power = r2_score(y, y_pred_power)
                mse_power = mean_squared_error(y, y_pred_power)
                
                fitting_methods['å¹‚å‡½æ•°æ‹Ÿåˆ'] = {
                    'æ–¹ç¨‹å¼': f"y = {np.exp(power_reg.intercept_):.4f}*x^{power_reg.coef_[0]:.4f}",
                    'ç³»æ•°a': np.exp(power_reg.intercept_),
                    'ç³»æ•°b': power_reg.coef_[0],
                    'RÂ²': r2_power,
                    'MSE': mse_power,
                    'é¢„æµ‹å€¼': y_pred_power
                }
            except:
                pass
        
        # é€‰æ‹©æœ€ä½³æ‹Ÿåˆæ–¹æ³•ï¼ˆåŸºäºRÂ²ï¼‰
        best_method = max(fitting_methods.items(), key=lambda x: x[1]['RÂ²'])
        
        # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
        correlation, p_value = stats.pearsonr(X.flatten(), y)
        
        result = {
            'æµç¨‹ç¯èŠ‚': step_name,
            'é€Ÿåº¦åˆ—': speed_column,
            'æ•°æ®ç‚¹æ•°': len(data),
            'é€Ÿåº¦èŒƒå›´': f"[{X.min():.2f}, {X.max():.2f}]",
            'å»¶æ—¶èŒƒå›´': f"[{y.min():.2f}, {y.max():.2f}]",
            'çš®å°”é€Šç›¸å…³ç³»æ•°': correlation,
            'På€¼': p_value,
            'æœ€ä½³æ‹Ÿåˆæ–¹æ³•': best_method[0],
            'æœ€ä½³æ–¹ç¨‹å¼': best_method[1]['æ–¹ç¨‹å¼'],
            'æœ€ä½³RÂ²': best_method[1]['RÂ²'],
            'æœ€ä½³MSE': best_method[1]['MSE'],
            'æ‰€æœ‰æ‹Ÿåˆæ–¹æ³•': fitting_methods,
            'åŸå§‹æ•°æ®': data
        }
        
        # æ˜¾ç¤ºæ‹Ÿåˆç»“æœ
        print(f"    æ•°æ®ç‚¹æ•°: {len(data)}")
        print(f"    é€Ÿåº¦èŒƒå›´: [{X.min():.2f}, {X.max():.2f}]")
        print(f"    å»¶æ—¶èŒƒå›´: [{y.min():.2f}, {y.max():.2f}]")
        print(f"    ç›¸å…³ç³»æ•°: {correlation:.4f} (På€¼: {p_value:.4f})")
        print(f"    æœ€ä½³æ‹Ÿåˆ: {best_method[0]}")
        print(f"    æœ€ä½³æ–¹ç¨‹: {best_method[1]['æ–¹ç¨‹å¼']}")
        print(f"    RÂ²: {best_method[1]['RÂ²']:.4f}")
        
        return result
        
    except Exception as e:
        print(f"    æ‹Ÿåˆè¿‡ç¨‹å‡ºé”™: {e}")
        return None

def save_fitting_results(fitting_results):
    """ä¿å­˜æ‹Ÿåˆç»“æœ"""
    
    # åˆ›å»ºè¯¦ç»†æ‹Ÿåˆç»“æœè¡¨
    detailed_results = []
    summary_results = []
    
    for result in fitting_results:
        # æ±‡æ€»ç»“æœ
        summary_row = {
            'æµç¨‹ç¯èŠ‚': result['æµç¨‹ç¯èŠ‚'],
            'é€Ÿåº¦åˆ—': result['é€Ÿåº¦åˆ—'],
            'æ•°æ®ç‚¹æ•°': result['æ•°æ®ç‚¹æ•°'],
            'é€Ÿåº¦èŒƒå›´': result['é€Ÿåº¦èŒƒå›´'],
            'å»¶æ—¶èŒƒå›´': result['å»¶æ—¶èŒƒå›´'],
            'çš®å°”é€Šç›¸å…³ç³»æ•°': result['çš®å°”é€Šç›¸å…³ç³»æ•°'],
            'På€¼': result['På€¼'],
            'æœ€ä½³æ‹Ÿåˆæ–¹æ³•': result['æœ€ä½³æ‹Ÿåˆæ–¹æ³•'],
            'æœ€ä½³æ–¹ç¨‹å¼': result['æœ€ä½³æ–¹ç¨‹å¼'],
            'æœ€ä½³RÂ²': result['æœ€ä½³RÂ²'],
            'æœ€ä½³MSE': result['æœ€ä½³MSE']
        }
        summary_results.append(summary_row)
        
        # è¯¦ç»†ç»“æœï¼ˆæ‰€æœ‰æ‹Ÿåˆæ–¹æ³•ï¼‰
        for method_name, method_data in result['æ‰€æœ‰æ‹Ÿåˆæ–¹æ³•'].items():
            detailed_row = {
                'æµç¨‹ç¯èŠ‚': result['æµç¨‹ç¯èŠ‚'],
                'é€Ÿåº¦åˆ—': result['é€Ÿåº¦åˆ—'],
                'æ‹Ÿåˆæ–¹æ³•': method_name,
                'æ–¹ç¨‹å¼': method_data['æ–¹ç¨‹å¼'],
                'RÂ²': method_data['RÂ²'],
                'MSE': method_data['MSE'],
                'æ•°æ®ç‚¹æ•°': result['æ•°æ®ç‚¹æ•°'],
                'ç›¸å…³ç³»æ•°': result['çš®å°”é€Šç›¸å…³ç³»æ•°']
            }
            
            # æ·»åŠ ç³»æ•°ä¿¡æ¯
            if 'ç³»æ•°a' in method_data:
                detailed_row['ç³»æ•°a'] = method_data['ç³»æ•°a']
            if 'ç³»æ•°b' in method_data:
                detailed_row['ç³»æ•°b'] = method_data['ç³»æ•°b']
            if 'ç³»æ•°c' in method_data:
                detailed_row['ç³»æ•°c'] = method_data['ç³»æ•°c']
            
            detailed_results.append(detailed_row)
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    if summary_results:
        summary_df = pd.DataFrame(summary_results)
        summary_df = summary_df.sort_values('æœ€ä½³RÂ²', ascending=False)
        summary_df.to_csv('å»¶æ—¶æ‹Ÿåˆæ–¹ç¨‹æ±‡æ€»_åˆ†æ®µ_1.csv', index=False, encoding='utf-8-sig', float_format='%.6f')
        print(f"\nå»¶æ—¶æ‹Ÿåˆæ–¹ç¨‹æ±‡æ€»å·²ä¿å­˜ï¼šå»¶æ—¶æ‹Ÿåˆæ–¹ç¨‹æ±‡æ€»_åˆ†æ®µ_1.csv")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    if detailed_results:
        detailed_df = pd.DataFrame(detailed_results)
        detailed_df = detailed_df.sort_values(['æµç¨‹ç¯èŠ‚', 'RÂ²'], ascending=[True, False])
        detailed_df.to_csv('å»¶æ—¶æ‹Ÿåˆæ–¹ç¨‹è¯¦ç»†_åˆ†æ®µ_1.csv', index=False, encoding='utf-8-sig', float_format='%.6f')
        print(f"å»¶æ—¶æ‹Ÿåˆæ–¹ç¨‹è¯¦ç»†å·²ä¿å­˜ï¼šå»¶æ—¶æ‹Ÿåˆæ–¹ç¨‹è¯¦ç»†_åˆ†æ®µ_1.csv")

def create_fitting_visualization(fitting_results):
    """åˆ›å»ºæ‹Ÿåˆå¯è§†åŒ–"""
    
    n_results = len(fitting_results)
    if n_results == 0:
        return
    
    # è®¡ç®—å­å›¾å¸ƒå±€
    cols = min(4, n_results)
    rows = (n_results + cols - 1) // cols
    
    plt.figure(figsize=(5*cols, 4*rows))
    
    for i, result in enumerate(fitting_results[:16]):  # æœ€å¤šæ˜¾ç¤º16ä¸ª
        plt.subplot(rows, cols, i+1)
        
        # è·å–åŸå§‹æ•°æ®
        data = result['åŸå§‹æ•°æ®']
        X = data['å¹³å‡é€Ÿåº¦'].values
        y = data['å»¶æ—¶æ—¶é—´'].values
        
        # ç»˜åˆ¶æ•£ç‚¹å›¾
        plt.scatter(X, y, alpha=0.6, s=30, label='å®é™…æ•°æ®')
        
        # ç»˜åˆ¶æœ€ä½³æ‹Ÿåˆæ›²çº¿
        best_method = result['æœ€ä½³æ‹Ÿåˆæ–¹æ³•']
        best_fit_data = result['æ‰€æœ‰æ‹Ÿåˆæ–¹æ³•'][best_method]
        
        # ç”Ÿæˆæ‹Ÿåˆæ›²çº¿çš„xå€¼
        x_range = np.linspace(X.min(), X.max(), 100)
        
        try:
            if best_method == 'çº¿æ€§æ‹Ÿåˆ':
                y_fit = best_fit_data['ç³»æ•°a'] * x_range + best_fit_data['ç³»æ•°b']
            elif best_method == 'äºŒæ¬¡å¤šé¡¹å¼æ‹Ÿåˆ':
                y_fit = best_fit_data['ç³»æ•°a'] * x_range**2 + best_fit_data['ç³»æ•°b'] * x_range + best_fit_data['ç³»æ•°c']
            elif best_method == 'åæ¯”ä¾‹æ‹Ÿåˆ':
                y_fit = best_fit_data['ç³»æ•°a'] / x_range + best_fit_data['ç³»æ•°b']
            elif best_method == 'æŒ‡æ•°æ‹Ÿåˆ':
                y_fit = best_fit_data['ç³»æ•°a'] * np.exp(best_fit_data['ç³»æ•°b'] * x_range)
            elif best_method == 'å¹‚å‡½æ•°æ‹Ÿåˆ':
                y_fit = best_fit_data['ç³»æ•°a'] * (x_range ** best_fit_data['ç³»æ•°b'])
            else:
                y_fit = None
            
            if y_fit is not None:
                plt.plot(x_range, y_fit, 'r-', alpha=0.8, label=f'{best_method}')
        except:
            pass
        
        plt.title(f"{result['æµç¨‹ç¯èŠ‚'][:15]}...\nRÂ²={result['æœ€ä½³RÂ²']:.3f}", fontsize=10)
        plt.xlabel('å¹³å‡é€Ÿåº¦')
        plt.ylabel('å»¶æ—¶æ—¶é—´(ç§’)')
        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=8)
    
    plt.tight_layout()
    plt.savefig('å»¶æ—¶æ‹Ÿåˆæ–¹ç¨‹å¯è§†åŒ–_åˆ†æ®µ_1.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"å»¶æ—¶æ‹Ÿåˆæ–¹ç¨‹å¯è§†åŒ–å·²ä¿å­˜ï¼šå»¶æ—¶æ‹Ÿåˆæ–¹ç¨‹å¯è§†åŒ–_åˆ†æ®µ_1.png")

def analyze_fitting_patterns(fitting_results):
    """åˆ†ææ‹Ÿåˆæ¨¡å¼"""
    
    print(f"\n{'='*50}")
    print(f"æ‹Ÿåˆæ¨¡å¼åˆ†æ:")
    print(f"{'='*50}")
    
    # ç»Ÿè®¡æ‹Ÿåˆæ–¹æ³•åˆ†å¸ƒ
    method_counts = {}
    r2_stats = []
    correlation_stats = []
    
    for result in fitting_results:
        method = result['æœ€ä½³æ‹Ÿåˆæ–¹æ³•']
        r2 = result['æœ€ä½³RÂ²']
        corr = result['çš®å°”é€Šç›¸å…³ç³»æ•°']
        
        method_counts[method] = method_counts.get(method, 0) + 1
        r2_stats.append(r2)
        correlation_stats.append(abs(corr))
    
    print(f"\næœ€ä½³æ‹Ÿåˆæ–¹æ³•åˆ†å¸ƒ:")
    for method, count in sorted(method_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(fitting_results)) * 100
        print(f"  {method}: {count}ä¸ª ({percentage:.1f}%)")
    
    print(f"\næ‹Ÿåˆè´¨é‡ç»Ÿè®¡:")
    print(f"  å¹³å‡RÂ²: {np.mean(r2_stats):.4f}")
    print(f"  RÂ²æ ‡å‡†å·®: {np.std(r2_stats):.4f}")
    print(f"  æœ€é«˜RÂ²: {np.max(r2_stats):.4f}")
    print(f"  æœ€ä½RÂ²: {np.min(r2_stats):.4f}")
    
    print(f"\nç›¸å…³æ€§ç»Ÿè®¡:")
    print(f"  å¹³å‡ç›¸å…³ç³»æ•°ç»å¯¹å€¼: {np.mean(correlation_stats):.4f}")
    print(f"  å¼ºç›¸å…³(|r|>0.7): {sum(1 for c in correlation_stats if c > 0.7)}ä¸ª")
    print(f"  ä¸­ç­‰ç›¸å…³(0.3<|r|â‰¤0.7): {sum(1 for c in correlation_stats if 0.3 < c <= 0.7)}ä¸ª")
    print(f"  å¼±ç›¸å…³(|r|â‰¤0.3): {sum(1 for c in correlation_stats if c <= 0.3)}ä¸ª")
    
    # æ‰¾å‡ºæ‹Ÿåˆæ•ˆæœæœ€å¥½çš„æµç¨‹ç¯èŠ‚
    best_fits = sorted(fitting_results, key=lambda x: x['æœ€ä½³RÂ²'], reverse=True)[:5]
    print(f"\næ‹Ÿåˆæ•ˆæœæœ€ä½³çš„5ä¸ªæµç¨‹ç¯èŠ‚:")
    for i, result in enumerate(best_fits, 1):
        print(f"  {i}. {result['æµç¨‹ç¯èŠ‚']}")
        print(f"     æ–¹ç¨‹å¼: {result['æœ€ä½³æ–¹ç¨‹å¼']}")
        print(f"     RÂ²: {result['æœ€ä½³RÂ²']:.4f}")
    
    # æ‰¾å‡ºç›¸å…³æ€§æœ€å¼ºçš„æµç¨‹ç¯èŠ‚
    strongest_corr = sorted(fitting_results, key=lambda x: abs(x['çš®å°”é€Šç›¸å…³ç³»æ•°']), reverse=True)[:5]
    print(f"\nç›¸å…³æ€§æœ€å¼ºçš„5ä¸ªæµç¨‹ç¯èŠ‚:")
    for i, result in enumerate(strongest_corr, 1):
        corr = result['çš®å°”é€Šç›¸å…³ç³»æ•°']
        corr_type = "æ­£ç›¸å…³" if corr > 0 else "è´Ÿç›¸å…³"
        print(f"  {i}. {result['æµç¨‹ç¯èŠ‚']}")
        print(f"     ç›¸å…³ç³»æ•°: {corr:.4f} ({corr_type})")
        print(f"     æ–¹ç¨‹å¼: {result['æœ€ä½³æ–¹ç¨‹å¼']}")

if __name__ == "__main__":
    analyze_by_time_periods() 